[{"title":"epoll实现探究","url":"/2020/01/09/epoll原理/","content":"\n### IO多路复用\n在以前，传统的网络编程是多线程模型，一个线程单独处理一个请求。\n\n然而，线程是很昂贵的资源：\n\n1. 线程的创建和销毁成本很高，linux的线程实际上是特殊的进程；因此通常会使用线程池来减少线程创建和销毁的开销\n2. 线程本身占用较大的内存，如果并发比较高，那么光是线程本身占用的内存就很大了\n3. 线程上下文切换的成本也比较高，如果频繁切换，可能cpu用于处理线程切换的时间会大于业务执行时间\n4. 容易导致系统负载升高\n\n因此，当面对海量连接的时候，传统的多线程模式就无能为力了。\n\n而且：\n1. 处理请求的很大比重时间，线程都是在等待网络io，线程很多时候都是在等待io\n2. 在推送服务场景下，有海量连接，但是大多数时候只有少数连接是活跃的\n\n这时候，我们就想，能不能有一种机制，可不可以让多个连接复用一个线程？就像cpu的分时复用一样。\n\n答案肯定是可以的，这就是IO多路复用。要能够实现IO多路复用，需要：\n1. 非阻塞IO。传统的阻塞IO，当需要等待IO时，会阻塞线程，这还复用个屁？\n2. 需要有一个poller，能够轮询连接的io事件。比如当需要往一个连接写入内容，这个连接当前缓冲区是满的，无法写入，我们希望当它的缓存区空闲的时候能够收到通知，从而完成写入\n\n对于非阻塞io，我们可以在`accept`连接的时候指定`SOCK_NONBLOCK`标志，或者在一些低版本的linux内核上，通过`fcntl`系统调用来设置。\n\n而poller，linux提供了[select](http://man7.org/linux/man-pages/man2/select.2.html)、[poll](http://man7.org/linux/man-pages/man2/poll.2.html)和[epoll](http://man7.org/linux/man-pages/man7/epoll.7.html)三种系统接口。本篇文章的主角是epoll。\n\n首先来看select接口：\n```c\nint select(int nfds, fd_set *readfds, fd_set *writefds,\n                  fd_set *exceptfds, struct timeval *timeout);\n```\n> An fd_set is a fixed size buffer. The Linux kernel allows fd_set of arbitrary size, determining the length of the sets to be checked from the value of nfds. However, the glibc implementation make the fd_set a fixed-size type is fixed in size, with size defined as 1024.\n>\n>Three independent sets of file descriptors are watched.  The file descriptors listed in readfds will be watched to see if characters become available for reading (more precisely, to see if a read will not block; in particular, a file descriptor is also ready on end-of-file).  The file descriptors in writefds will be watched to see if space is available for write (though a large write may still block). The file descriptors in exceptfds will be watched for exceptional conditions.\n> \n> The timeout argument specifies the interval that select() should block waiting for a file descriptor to become ready. If both fields of the timeval structure are zero, then select() returns immediately.  (This is useful for polling.)  If timeout is NULL (no timeout), select() can block indefinitely.\n\nselect具有如下几个缺陷：\n1. 如果一个fd需要同时监听readab和writable事件，那么需要同时加入到readfds和writefds列表中\n2. 每次调用select，都需要将要监听的fd列表从用户空间传到内核空间，内核每次都需要COPY FROM USER，而往往fd列表是固定的内容\n3. glibc的实现中，限制了fd_set的大小固定为1024，因此当需要监听的fd列表超过了这个大小，就无能为力了\n\npoll接口相对select接口有所改进：\n```c\nstruct pollfd {\n    int   fd;         /* file descriptor */\n    short events;     /* requested events */\n    short revents;    /* returned events */\n};\n\n// fds is a pollfd array\nint poll(struct pollfd *fds, nfds_t nfds, int timeout);\n```\n\n首先，poll使用结构体`pollfd`来描述需要监听的文件，并且可以通过设置`pollfd#events`来描述感兴趣的事件，同时当返回时，可以通过`pollfd#revents`来获取就绪的事件类型。这样就不需要像select接口分别指定三种类型的文件描述符集合了，而且事件类型也更加丰富。而且poll接口对于fds数组的大小也没有限制。\n\n但是，每次调用poll，依然需要传入感兴趣的fd列表。\n\n也正是因为poll的不足，才有了后来epoll的出现。\n\n接下来简单看一下epoll的接口：\n```c\n// 创建一个epoll实例\nint epoll_create(int size);\n\n// 往一个epoll实例中添加/移除/修改对fd的事件监听 \nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);\n\n// 等待就绪事件\nint epoll_wait(int epfd, struct epoll_event *events,\n                      int maxevents, int timeout);\n```\n\n光从接口就可以看到epoll的强大。select和poll才只有一个接口，而epoll自己就足足有3个啊！\n\nepoll本身也是一个file，我们需要先调用`epoll_create`创建它，才能使用，该方法会在内核空间创建一个`epoll`实例，然后返回对应的`fd`。\n既然`epoll`是一个file，那么当不再使用的时候，需要被close掉，就像我们close一个打开的文件一样。\n\n\n当我们通过epoll_create创建一个epoll实例，然后通过epoll_ctl来添加，移除或者修改对文件的就绪事件的监听。比如，当一个连接到达时，我们将其加入epoll，并监听其EPOLLIN/EPOLLOUT/EPOLLRDHUP事件。然后可以多次调用epoll_wait方法来轮询是否有就绪事件，而不用重新加入；当接收到EPOLLRDHUP事件时，移除对该文件的监听，并将其关闭。\n\n\nepoll也可以被别的epoll轮询！！！\n\n接下来我们基于linux kernel 5 的代码看一下epoll的实现。\n\n### epoll实现\n\n##### epoll的创建\n我们首先来看一下epoll的创建过程。\n\n在`fs/eventpoll.c`文件下，可以看到`epoll_create`系统调用的定义，\n```c\nSYSCALL_DEFINE1(epoll_create, int, size)\n{\n\tif (size <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_epoll_create(0);\n}\n```\n`SYSCALL_DEFINEN`是一系列用于定义系统调用的宏，`N`表示系统调用的参数个数。因为`epoll_create`只有一个参数，因此使用的是`SYSCALL_DEFINE1`。\n\n我们可以看到，如果传入的`size`小于或者等于`0`，则会返回`EINVAL`，表示参数错误。在早期的版本中，`epoll`是使用哈希表来管理注册的文件列表，需要传入一个`size`参数；但是现在的版本已经换做红黑树了，该参数已经没有具体的意义了，但是为了兼容性还是保留了下来，使用时传入一个任意大于0的数就好了。\n\n接下来，我们看一下`do_epoll_create`的实现，该方法才是真正干活的主：\n```c\nstatic int do_epoll_create(int flags)\n{\n\tint error, fd;\n\tstruct eventpoll *ep = NULL; // eventpoll是epoll实例\n\tstruct file *file; // 前面说过epoll实例本身也是一个file\n\n   // flags只支持设置EPOLL_CLEXEC\n\tif (flags & ~EPOLL_CLOEXEC)\n\t\treturn -EINVAL;\n    \n    // 分配一个epoll实例\n\terror = ep_alloc(&ep);\n\tif (error < 0)\n\t\treturn error;\n\t\n    // 分配一个未使用的文件描述符fd\n\tfd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));\n\tif (fd < 0) {\n\t\terror = fd;\n\t\tgoto out_free_ep;\n\t}\n    \n    // 获取一个具有匿名inode的file，并设置file->f_op为eventpoll_fops\n    // 同时设置file->private_date为ep\n\tfile = anon_inode_getfile(\"[eventpoll]\", &eventpoll_fops, ep,\n\t\t\t\t O_RDWR | (flags & O_CLOEXEC));\n\tif (IS_ERR(file)) {\n\t\terror = PTR_ERR(file);\n\t\tgoto out_free_fd;\n\t}\n    // 管理epoll实例和file\n\tep->file = file;\n    // 设置fd对应的file\n\tfd_install(fd, file);\n\treturn fd;\n\nout_free_fd:\n    // 创建失败，释放fd\n\tput_unused_fd(fd);\nout_free_ep:\n    // 创建失败，释放内存\n\tep_free(ep);\n\treturn error;\n}\n```\n`do_epoll_create`接收一个`flags`参数，如果我们使用`epoll_create`来创建，默认是没有设置任何`flags`的。但是内核后面又添加了`epoll_create1`系统调用，使用该系统调用是可以设置`flags`的，具体可以查看`man`手册。\n\n上面代码已经加了具体的注释了。在linux中正所谓一切皆文件，可以看到当我们创建epoll实例的时候，同时会创建一个`file`。其实`linux`的文件系统正是面向对象的思想，我们可以把`file`看成是一个统一的接口，具体的实现只需要把对应的方法实现注册到`file->f_op`中就好了，而且`file->private_data`保存了实际实现的指针。那些支持接口的语言，背后的实现无外乎也就是这样。\n\n##### epoll实例\n既然是创建epoll实例，那我们是不是应该看一下这个epoll实例到底是什么？\n```c\nstruct eventpoll {\n\t// 资源保护锁\n\tstruct mutex mtx;\n\n\t// 阻塞在epoll_wait的等待队列\n\twait_queue_head_t wq; \n\n\t// epoll实例本身实现了file->f_op->poll方法，对应的poll等待队列\n    // 比如epoll实例本身也可以被添加到其他epoll实例中\n\twait_queue_head_t poll_wait;\n\n\t// 就绪队列，保存已经就绪的事件\n\tstruct list_head rdllist;\n\n    // 用于保护rdllist和ovflist的锁\n\trwlock_t lock;\n\n\t// 用于存储添加到epoll实例的fd的红黑树\n\tstruct rb_root_cached rbr; \n\n\t// 当正在转移就绪队列中的事件到用户空间时，这段时期就绪的事件会被暂时加入该队列，等待转移结束再添加到rdllist。\n\tstruct epitem *ovflist; \n\n\tstruct wakeup_source *ws;\n\n\t// 创建epoll实例的用户\n\tstruct user_struct *user;\n\n    // epoll本身也是一个文件\n\tstruct file *file;\n\n\t/* used to optimize loop detection check */\n\tint visited;\n\tstruct list_head visited_list_link;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\t/* used to track busy poll napi_id */\n\tunsigned int napi_id;\n#endif\n};\n```\n\nepoll实例使用红黑树来管理注册的需要监听的fd，关于红黑树的介绍可以参考[该篇](https://mcll.top/2019/12/06/%E7%BA%A2%E9%BB%91%E6%A0%91/)，这里就不介绍了。\n\n每个被添加到epoll实例的fd对应一个`epitem`：\n```c\nstruct epitem {\n\tunion {\n\t\t// 红黑树节点，linux内核中红黑树被广泛使用，因此有必要实现成通用结构，通过rb_node连接到rb-tree\n        // 当需要访问具体结构体时，通过简单的指针运算以及类型转换就可以了\n\t\tstruct rb_node rbn;\n\t\t/* Used to free the struct epitem */\n\t\tstruct rcu_head rcu;\n\t};\n\n\t// 用于添加到epoll实例的就绪队列的链表节点\n\tstruct list_head rdllink; \n\n    // 在将就绪列表中的事件转移到用户空间期间，新的就绪事件会先加入到epoll的ovflist；为什么不复用rdllink字段呢？因为同一个fd，可能还在就绪队列中，但是又有了新的就绪事件了，这时候它既在就绪队列中，也在ovflist中\n\tstruct epitem *next; \n\n\t// 该epitem关联的fd和file信息，用于红黑树中的比较；我们知道，红黑树是二叉搜索树，在查找的时候，是需要比较大小的\n\tstruct epoll_filefd ffd; \n\n\t// Number of active wait queue attached to poll operations \n\tint nwait;\n\n\t// List containing poll wait queues\n\tstruct list_head pwqlist;\n\n\t// 该epitem注册的epoll实例\n\tstruct eventpoll *ep; \n\n\t// used to link this item to the \"struct file\" items list\n\tstruct list_head fllink;\n\n\t// wakeup_source used when EPOLLWAKEUP is set \n\tstruct wakeup_source __rcu *ws;\n\n\t// 该结构体保存epitem对应的fd和感兴趣的事件列表\n\tstruct epoll_event event; \n};\n```\n\n当一个`fd`被添加到epoll实例时，epoll实例会调用对应file的poll方法，poll方法有两个作用：\n\n- 设置callback，当文件有新的就绪事件产生时，调用该callback\n- 返回文件当前的就绪事件\n\n而当我们从epoll实例移除一个fd时，需要从file中移除注册的callback。\n\n注册callback，实际上就是添加到file的poll wait列表；而这个时候，也会把file的poll wait列表加入到epitem的pwdlist这个列表中。当需要移除注册的callback时，遍历epitem的pwdlist就好了。\n\n接下来看一下 `epoll_filefd`以及在搜索红黑树时，是如果通过该结构体比较大小的：\n```c\nstruct epoll_filefd {\n\tstruct file *file; // 对应file\n\tint fd;            // 对应的fd\n} __packed;            // _packed设置内存对齐为1\n\n/* Compare RB tree keys */\nstatic inline int ep_cmp_ffd(struct epoll_filefd *p1,\n\t\t\t     struct epoll_filefd *p2)\n{\n\treturn (p1->file > p2->file ? +1:\n\t        (p1->file < p2->file ? -1 : p1->fd - p2->fd));\n}\n```\n可以看到，比较的逻辑是：\n\n1. 首先比较file的地址\n2. 如果属于同一个file，则比较fd；比如使用dup系统调用，就可以产生两个fd指向同一个file\n\n##### 添加监听文件\n\n\n\n##### 更新监听文件\n\n##### 移除监听文件\n\n\n##### 等待就绪事件\n\n\n\n##### 惊群效应\n\n\n##### Flags\n\n\n##### Patterns","tags":["linux - epoll"]},{"title":"计算机的原码、反码和补码","url":"/2020/01/08/数值表示/","content":"\n# 原码 反码 补码\n\n### 机器数和真值\n\n##### 机器数\n\n 一个数在计算机中的二进制表示形式, 叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1 。\n\n比如，十进制中的数` +3` ，计算机字长为8位，转换成二进制就是`00000011`。如果是 `-3` ，就是 `10000011` 。\n\n那么，这里的 `00000011` 和 `10000011` 就是机器数。\n\n##### 真值\n\n因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 `10000011`，其最高位`1`代表负，其真正数值是 `-3` 而不是形式值`131`（`10000011`转换成十进制等于`131`）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。\n\n因此`10000011`的真值为`-3`。\n\n\n\n### 原码 反码 补码\n\n##### 原码\n\n原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:\n\n> [-1]<sub>原</sub> = 1000 0001\n>\n> [+1]<sub>原</sub> = 0000 0001\n\n第一位是符号位。因为第一位是符号位, 所以8位二进制数的取值范围就是：\n\n>  [1111 1111 , 0111 1111] \n\n即\n\n> [-127, 127]\n\n\n\n##### 反码\n\n反码的表示方法是：\n\n- 正数的反码是其本身\n\n- 负数的反码是在其原码的基础上，符号位不变，其余各个位取反：\n\n  > [+1] = [00000001]<sub>原</sub> = [00000001]<sub>反</sub>\n  >\n  > [-1] = [10000001]<sub>原</sub> = [11111110]<sub>反</sub>\n\n可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值。通常要将其转换成原码再计算。\n\n\n\n ##### 补码\n\n补码的表示方法是：\n\n- 正数的补码就是其本身\n\n- 负数的补码是在其原码的基础上，符号位不变，其余各位取反，最后加1（即在反码的基础上加1）\n\n  >[+1] = [00000001]<sub>原</sub> = [00000001]<sub>反</sub> = [00000001]<sub>补</sub>\n  >\n  >[-1] = [10000001]<sub>原</sub> = [11111110]<sub>反</sub> = [11111111]<sub>补</sub>\n\n\n\n##### 为什么需要反码和补码\n\n 首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减。 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单 。 计算机辨别\"符号位\"显然会让计算机的基础电路设计变得十分复杂 。 于是人们想出了将符号位也参与运算的方法 。\n\n我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: `1 - 1 = 1 + (-1) = 0` , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了 。\n\n对于计算`1-1=0`，首先来看原码：\n\n> 1 - 1 = 1 + (-1) = [00000001]<sub>原</sub> + [10000001]<sub>原</sub> = [10000010]<sub>原</sub> = -2\n\n如果用原码来进行计算，将符号位也参与计算，显然计算结果是错误的。\n\n因此引入了反码：\n\n> 1 - 1 = 1 + (-1) = [00000001]<sub>反</sub> + [111111110]<sub>反</sub>  = [11111111]<sub>反</sub> = -0\n\n发现使用反码计算，结果的真值是正确的。但是也存在问题，再看下面一个例子：\n\n> 0 + 0 = [00000000]<sub>反</sub> + [00000000]<sub>反</sub> = +0\n\n对于结果`0`,可能会得到`-0`和`+0`两个不同的结果。\n\n而补码的出现，解决了这个问题：\n\n> 1 - 1 = [00000001]<sub>补</sub> + [11111111]<sub>补</sub> = [00000000]<sub>补</sub> = 0\n>\n> 0 + 0 = [00000000]<sub>补</sub> + [00000000]<sub>补</sub> = 0\n\n可以看到，使用补码进行计算，将会只有一个`0`，而且可以用`10000000`来表示`-128`:\n\n> -1 + -127 = [11111111]<sub>补</sub> + [10000001]<sub>补</sub> = [10000000]<sub>补</sub>\n\n因此，对于一个8位的带符号数，如果使用补码来进行表示，则取值范围位`[-128, 127]`\n\n而对于原码和反码，因为都存在两个`0`：\n\n> +0 = [00000000]<sub>原</sub> = [00000000]<sub>反</sub>\n>\n> -0 = [10000000]<sub>原</sub> = [11111111]<sub>反</sub>\n\n有效取值范围都只有`[-127, 127]`\n\n使用补码，不仅解决了减法的问题，还修复了存在两个`0`的问题，使得负数的有效值可以多一位。\n\n因此，计算机都是使用补码来表示正数，对于`n`位的整数，其有效范围为：\n\n> [-2<sup>n-1</sup>, 2<sup>n-1</sup>-1]\n\n\n\n##### 补码的本质\n\n补码本质是用来构成一个环，以实现一个同余运算。 \n\n假设8位的整数为例，我们计算的结果最终都是`2^8`的余数，这与符号位没有关系，也就是需要`mod 2^8`，也就是当计算结果超过8位时，高位会被舍弃，就是溢出。\n\n比如：\n\n> 1 + 255 = 256  = 0\n>\n> 1 - 1 = 0 \n\n这其实是**同余**的概念：两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余，记作`a ≡ b (mod m)`\n\n我们来看 255和-1：\n\n> 255 mod 256 = 255 \n>\n> -1 mod 256 = 255\n\n也就是 ` -1 ≡ 255 mod 256`\n\n在一个环中，00000000<sub>b</sub>和11111111<sub>b</sub>连接，当00000000<sub>b</sub>向后走一步，就回到11111111<sub>b</sub>了。\n\n\n\n同余加法：\n\n> a ≡ b mod m\n>\n> c ≡ d mod m \n>\n> a+c ≡ b+d mod m\n\n所以有：\n\n> 1 ≡ 1 mod 256\n>\n> -1 ≡ 255 mod 256\n>\n> 1-1 ≡ 1+255 mod 256\n\n所以 1-1 = 1+255 = 0\n\n对于255的二进制表示：\n\n> 255 = 11111111<sub>b</sub>\n\n因为 `-1≡255 mod 256`，在8位的带符号数中，我们可以用11111111<sub>b</sub>来表示`-1`，而这不正是`-1`的补码吗。\n\n\n### 应用\n前面讲到，计算机中使用补码存储整数，而我们可以把补码看成是一个环\n因此：\n```go\n  a := uint8(2)\n  b := uint8(255)\n  a - b == 3 // 2倒退3步回到255\n```\n\n利用这个性质，当我们在实现环形队列时：\n```go\ntype RingQueue struct {\n\tbuf        []interface{}\n\thead, tail uint32\n\tcap        uint32\n}\n\nfunc New(cap uint32) *RingQueue {\n\treturn &RingQueue{\n\t\tbuf: make([]interface{}, cap),\n\t\tcap: cap,\n\t}\n}\n\nfunc (r *RingQueue) Push(v interface{}) bool {\n\tif r.tail-r.head < r.cap {\n\t\tr.buf[r.tail%r.cap] = v\n\t\tr.tail++\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (r *RingQueue) Size() uint32 {\n\treturn r.tail - r.head\n}\n\nfunc (r *RingQueue) Cap() uint32 {\n\treturn r.cap\n}\n\nfunc (r *RingQueue) Pop() (interface{}, bool) {\n\tif r.tail-r.head > 0 {\n\t\tv := r.buf[r.head%r.cap]\n\t\tr.head++\n\t\treturn v, true\n\t}\n\treturn nil, false\n}\n```\n\n\n\n### 参考\n- [原码、反码和补码详解](https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/computercode.html)","tags":["计算机原理"]},{"title":"在go中获取连接的文件描述符，你掉坑里了吗","url":"/2020/01/07/go-file-fd/","content":"\n在`go`中，我们有时候想要拿到一条连接对应的`fd`，我们可能会这样写：\n```go\n    ln, _ := net.Listen(\"tcp\", \":8080\")\n    defer ln.Close()\n\n    tcpln := ln.(*net.TCPListener)\n\n    cf, _ := tcpln.File()\n    defer cf.Close()\n \n    fd := cf.FD() // 拿到对应的fd\n```\n可以看到，在上面的代码中，我们分别调用了两次`Close`。这是因为`File`方法，实际上执行的是`dup`系统调用。\n\n![](/img/openfile.png)\n\n在表示进程的`task_struct`中，有一个`files`字段：\n```c\nstruct task_struct {\n    ...\n\n\t/* Open file information: */\n\tstruct files_struct\t\t*files;\n    ...\n}\n```\n`files_struct`用于保存当前进程打开的文件信息。\n\n当前进程打开的文件，会有一个对应的`file`结构体，而在`files_struct`有一个`files *`类型的数组，保存当前进程打开的所有文件的`file`结构体的地址。我们在用户空间中使用到的文件描述符`fd`，实际上是该数组中的下标。当需要访问某个文件时，通过`fd`在与数组起始地址进行指针运算，就可以得到对应的`file`结构体的地址了。\n\n\n该数组的前三个默认是：\n- `0`：标准输入\n- `1`：标准输出\n- `2`：标准错误输出\n\n替换这三个的内容，就可以实现重定向了。\n\n一个打开的文件可以被多个进程同时引用，用于实现共享文件；也可以通过`dup`系统调用来实现同一个进程同时引用同一个文件多次：\n> The dup() system call creates a copy of the file descriptor oldfd, using the lowest-numbered unused file descriptor for the new descriptor. After a successful return, the old and new file descriptors may be used interchangeably. They refer to the same open file description and thus share file offset and file status flags.\n\n当打开一个文件时，会对应有一个`file`结构体来表示，也就是上面说的`file description`，而`file descriptor`对应的就是我们所说的`fd`。\n\n`file`中有一个字段用来表示引用次数，当调用`close`时，会先把引用次数减`1`，只有当不再被引用时才执行真正的`close`。\n\n回到开头的例子中，因为`File`方法内部实际上是执行`dup`系统调用，因此当前进程现在会有两个`fd`指向同一个打开的连接，因此如果要真正关闭该连接，需要分别执行两次`close`方法。\n\n接下来，我们看一下`File`方法的实现：\n```go\nfunc (l *TCPListener) File() (f *os.File, err error) {\n\tif !l.ok() { // 判断文件是否为空\n\t\treturn nil, syscall.EINVAL\n\t}\n\tf, err = l.file() // 真正干活的方法\n\tif err != nil {\n\t\treturn nil, &OpError{Op: \"file\", Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err}\n\t}\n\treturn\n}\n```\n可以看到，实际上调用的是`TCPListener#file`方法，接下来看一下该方法：\n```go\nfunc (ln *TCPListener) file() (*os.File, error) {\n\tf, err := ln.fd.dup()  // 这里出现dup了吧\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn f, nil\n}\n\nfunc (fd *netFD) dup() (f *os.File, err error) {\n\tns, call, err := fd.pfd.Dup() // 继续看一下\n\tif err != nil {\n\t\tif call != \"\" {\n\t\t\terr = os.NewSyscallError(call, err)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\treturn os.NewFile(uintptr(ns), fd.name()), nil\n}\n\nfunc (fd *FD) Dup() (int, string, error) {\n\tif err := fd.incref(); err != nil {\n\t\treturn -1, \"\", err\n\t}\n\tdefer fd.decref()\n\treturn DupCloseOnExec(fd.Sysfd) \n}\n```\n可以看到最终会调用`DupCloseOnExec`方法：\n```go\n// DupCloseOnExec dups fd and marks it close-on-exec.\nfunc DupCloseOnExec(fd int) (int, string, error) {\n\tif atomic.LoadInt32(&tryDupCloexec) == 1 {\n        // 通过fcntl系统调用执行dup，同时设置 close-on-exec flag\n\t\tr0, e1 := fcntl(fd, syscall.F_DUPFD_CLOEXEC, 0) \n\t\tif e1 == nil {\n\t\t\treturn r0, \"\", nil\n\t\t}\n\t\tswitch e1.(syscall.Errno) {\n\t\tcase syscall.EINVAL, syscall.ENOSYS:\n\t\t\t// 老版本的linux内核\n\t\t\tatomic.StoreInt32(&tryDupCloexec, 0)\n\t\tdefault:\n\t\t\treturn -1, \"fcntl\", e1\n\t\t}\n    }\n    // 老版本内核的fcntl不支持F_DUPFD_CLOEXEC命令\n\treturn dupCloseOnExecOld(fd)\n}\n\n// dupCloseOnExecUnixOld is the traditional way to dup an fd and\n// set its O_CLOEXEC bit, using two system calls.\nfunc dupCloseOnExecOld(fd int) (int, string, error) {\n    // 这里要加fork锁！！！\n\tsyscall.ForkLock.RLock()\n\tdefer syscall.ForkLock.RUnlock()\n\tnewfd, err := syscall.Dup(fd)\n\tif err != nil {\n\t\treturn -1, \"dup\", err\n\t}\n\tsyscall.CloseOnExec(newfd)\n\treturn newfd, \"\", nil\n}\n```\n首先，会尝试使用`fcntl`系统调用的`F_DUPFD_CLOEXEC`命令来实现。该方法具有原子性，也就是执行`dup`和设置`close-on-exec`标志位两个操作是原子的。而老版本的内核并不支持该方法，则需要先执行`dup`系统调用，然后通过`fcntl`系统调用设置`close-on-exec`标志位。\n\n在`go`中，打开的文件默认都会设置`close-on-exec`标志位。因为，`fork`的子进程默认会继承父进程打开的文件列表。而设置了`close-on-exec`标志位的文件，在子进程执行`exec`族函数时会先`close`掉。这样就可以防止文件被子进程继承，而子进程又没有关闭，导致文件泄露。如果子进程确实需要继承父进程的文件，则需要手动指定。\n\n回到前面，在低版本内核中，需要分为两步执行，那么可能在执行`dup`系统调用后，设置`close-on-exec`标志前，在另一个协程中执行了`fork`系统调用，这时候这个文件就不会在子进程执行`exec`时被关闭，从而导致泄露。因此在`dupCloseOnExecOld`这个方法中，需要加`syscall.ForkLock`锁。\n\n接下来，我们看一下获取`fd`的`FD`方法：\n```go\nfunc (f *File) Fd() uintptr {\n\tif f == nil {\n\t\treturn ^(uintptr(0))\n\t}\n\n\tif f.nonblock {\n        // 如果是非阻塞模式，则设置成阻塞模式\n\t\tf.pfd.SetBlocking() \n\t}\n\n\treturn uintptr(f.pfd.Sysfd)\n}\n```\n\n在`go`，网络连接默认是非阻塞模式的。\n\n在非阻塞模式中，当`accept/write/read`没有新的请求可以接受/没有空闲缓冲区可写/缓冲区没有内容可读，会立即返回`EAGEIN`。这时候，`runtime`会将其加入到`epoll`中监听，然后将对应的协程挂起，直到等待的事件到来才将其唤醒。\n\n而如果是阻塞模式，当`accept/write/read`没有新的请求可以接受/没有空闲缓冲区可写/缓冲区没有内容可读，会一直阻塞，不仅会阻塞当前协程，还会把系统线程阻塞掉。一不小心就会导致系统线程激增。\n\n因此在对网络连接使用`FD`方法时，需要格外小心。可以使用下面方法替代：\n```go\nrawConn, err := tcpConn.SyscallConn()\nif err == nil {\n\trawConn.Control(func(fd uintptr) {\n\t\t// 这里就可以拿到 fd 了\n\t})\n}\n```\n\n最后看一下下面完整demo：\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"syscall\"\n)\n\nfunc main() {\n\tln, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n    }\n    \n\ttcpln := ln.(*net.TCPListener)\n\n\tnf, err := tcpln.File() // 使用dup系统调用，内核中的file的引用加1，返回的是新的fd\n\tln.Close()              // close掉原先的连接\n\n\tfd := nf.Fd()    // 注意这时候 listener 变成 Blocking\n\tfmt.Println(fd) // 会输出 4, 而不是 3\n\n\tcid, _, err := syscall.Accept(int(fd))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tsyscall.Close(cid) // 关闭请求连接\n\tsyscall.Close(int(fd)) // 关闭listener\n}\n```","tags":["go"]},{"title":"paxos一致性协议","url":"/2019/12/28/paxos/","content":"\n\n### Paxos\n`Paxos`是分布式领域的宗师`Lamport`提出的基于消息传递、失败容忍的分布式一致性算法。\n\n`Google Chubby`的作者`Mike Burrows`说过，**世界上只有一种一致性算法，那就是`Paxos`**，可见`Paxos`在分布式领域的重要性。\n\n接下来我们就基于`<<Paxos Made Simple>>`这篇论文来学习一下`Paxos`算法。\n\n在该论文中，介绍的实际上是`Base Paxos`算法：假设有一组进程可以提议值，那么通过`Base Paxos`，在这些被提议的值中只会有一个值被选中，这组进程就这个值达到一致。\n\n很重要的一点，`Base Paxos`只能选中一个值。\n\n`Base Paxos`需要保证：\n- 如果没有值被提出，就不会有值被选中，不能无中生有\n- 最终只能有一个值被选中，否则就脑裂了\n- 任意一个进程不能学习到未被选中的值\n\n\n##### 系统中的角色\n在`Paxos`算法中，存在三种角色的`agents`：`proposers`，`acceptors`，`learners`。在具体实现中，一个进程可能同时充当所有角色。\n\n- `proposers`：向`acceptors`提议某个值\n- `acceptors`：对`proposers`提出的值进行投票\n- `leaders`：学习最后被选中的值\n\n我们假设：\n- `agents`可以以任意速度运行，可能停止或者重启；因为`agents`可能会重启，因此需要能够持久化一些关键信息，并在重启之后能够恢复。\n- `agents`可以通过发送消息来进行通信。消息可以任意长，可以重复或者丢失，但是不能被篡改。\n\n也就是，这是一个异步的、非拜占庭模型。\n\n##### 算法推导\n为了能够在多个进程中选中一个值，最简单的方法是只有一个`acceptor agent`，只要选中其第一个接收到的提议的值。然而，这样就存在单点故障，无法实现失败容忍了。只要这个`agent`挂了，那么系统就无法工作了。\n\n因此，需要同时存在多个`acceptor`。一个`acceptor`可以接受它接收到的提议值。当一个提议值被大多数`acceptor`接受时，那么这个提议值就被选中。\n\n> A proposer sends a proposed value to a set of acceptors. An acceptor may accepte the proposed value. The value is chosen when a large enough set of acceptors have accepted it.\n\n这里的大多数，也就是`majority`应该是多少呢？假设有N个acceptor，那么应该至少需要 `N/2+1`。这样在N个acceptor中，任意两个`majority`会有一个公共的`acceptor`。如果一个`acceptor`只能接受一个值，那么就可以确保不会有超过一个提议值被选中。\n\n不考虑节点故障或者消息丢失的情况下，我们想要即使只有一个`proposer`发表提案，系统最终都能够选中一个值，因此要求满足条件 P1：\n\n**P1：acceptors必须接受他接收到的第一个提案**\n\n然而，同一时间可能会有多个`proposer`同时发起提议，每个`acceptor`可能接收到不同的值，按照条件 P1，每个`acceptor`都接受他们第一个收到的值，从而导致没有一个提议值被`majority`的`acceptors`接受，进而导致最终没有值被选中。\n\n\n选票瓜分是客端存在的，而为了能够达到最终有一个提案被`majority`接受，因此一个`acceptor`应该能够接受超过一个提案，当然，这肯定是有条件的，不能随便接受。\n\n首先规定，每次`propose`提议一个值的时候，都要**生成一个全局唯一且可比较**的编号。因此，一个`proposer`发表的提案，实际上包含了一个唯一编号和提议值。有了这个唯一编号之后，我们就可以有效的区分不同的提案了，我们可以通过`<number, value>`唯一标识一个提案。\n\n> 一个生成编号的可行方案：\n> \n> In a system with n replicas, assign each replica r a unique id i<sub>r</sub> between 0 and n-1. Replica r picks the smallest sequence number s larger than any it has seen, such that s mod n = i<sub>r</sub>.\n>\n>如果集群成员需要变更，则 `unique id` 可以使用素数替代。\n\n**每次`proposer`在生成一个编号的时候，都要确保新的编号大于它当前所看见的最大编号**。我们可以通过比较提案的编号来判断提案的新旧，更高编号的提案被认为更新。\n\n一个提议值被选中，只有当对应的提案被大多数`acceptor`所接受，也就是对应的提案被选中。\n\n在`paxos`中，**允许多个提案被选中，但是必须保证这些提案都具有相同的提议值，也就是具有相同的`value`**。\n\n这时候，为了保证所有被选中的提案具有相同的值，需要保证条件 P2 成立：\n\n**P2：如果一个包含值 v 的提案被选中，那么所有被选中的具有更高编号的提案也包含值 v**\n\n因为提案的编号是全局可比较的，该条件保证了关键的安全性：只有一个值被选中。\n\n\n为了被选中，一个提案至少需要被一个`acceptor`接受，因此，可以通过满足条件 P2<sup>a</sup> 来满足条件 P2：\n\n**P2<sup>a</sup>：如果一个具有值v的提案被选中，那么任意一个accptor接受的更高编号的提案要包含值v**\n\n条件 P1 仍然需要维护，以确保能够有提案被选中。\n\n可能当一个值被选中时，某个`acceptor`还没有接收过任何提案，比如在此期间该`acceptor`重启了，刚好错过了`proposer`的请求；这时候如果该`acceptor`收到了一个具有更高编号但是包含了一个不同值的提案，这时候根据条件 P1，`acceptor`需要接受这个值，从而违反了条件 P2<sup>a</sup>。为了同时满足 P1和 P2<sup>a</sup>，需要增强条件 P2<sup>a</sup> 为 P2<sup>b</sub>：\n\n**P2<sup>b</sup>：如果一个包含值v的提案被选中，那么任意proposer提出的更高编号的提案需要包含值v**\n\n因为提案被接受之前需要先被提出，因此满足 P2<sup>b</sup> 也就满足 P2<sup>a</sup>，也就满足了P2。\n\n要发现如何满足 P2<sup>b</sup>，我们可以尝试证明在什么条件下该条件成立。\n\n我们假设一个提案`<m, v>`已经被选中了，现在需要证明任何具有大于`m`的编号`n`的提案也具有值`v`。\n\n我们通过在`n`上使用归纳法来简化证明。我们首先引入额外的归纳假设，每个编号在`[m, n-1]`的提案都具有值`v`。\n\n**因为编号m的提案被选中，因此，必定存在集合C，包含acceptors的majority，集合C中的每个acceptor都接受该提案**。\n\n结合该条件以及归纳假设，我们假设m被选中，则意味着：\n\n> 集合`C`中的每个`acceptor`都曾经接受过一个编号在`[m,n-1]`的提案，并且每个被任意`acceptor`接受的编号在`[m,n-1]`的提案都有值`v`。\n>\n> Every acceptor in C has accepted a proposal with number in m ..(n − 1), and every proposal with number in m ..(n − 1) accepted by any acceptor has value v. \n\n`acceptors`中的任意`majority`组成的集合`S`，至少与集合`C`有一个公共的成员，我们可以通过维护条件 P2<sup>c</sup>，来推断出编号`n`的提案具有值`v`这个结论：\n\n**P2<sup>c</sup>：对于一个具有任意编号`n`和值`v`的提案被提议，那么存在集合`S`是`acceptors`中的`majority`，要么满足： a)`S`中没有`acceptor`接受过提案编号小于`n`的提案，即当前还没有编号小于 `n`的提案被选中；或者满足： b)`v`是`S`中的所有`acceptors`接受的所有编号小于`n`的提案中编号最高的那个提案的值，这个提案可能现在已经被选中了，也有可能还没有被选中，但是将来可能被选中。**\n\n由此，我们可以通过维护条件 P2<sup>c</sup>，来满足条件 P2<sup>b</sup>。\n\n为了维护不变式 P2<sup>c</sup>，一个`proposer`在提议编号为`n`的提案之前，必须先了解整个系统中，当前编号小于`n`的提案中具有最高编号的，并且已经被选中或者**将被选中**的提案。了解已经被接受的提案很容易，但是要预测将会被接受的提案很困难。\n\n替代试图去预测未来，`paxos`选择了在提议一个值的时候，使用两阶段提交的方式。\n\n`proposer`在提议一个编号为`n`的提案之前，需要先向`acceptors`发起一个`prepare`请求：请求`acceptor`不要接受编号小于`n`的提案。具体如下：\n\n1. `proposer`选择一个新的提案编号`n`，然后向`acceptor`集合发送`prepare`请求：\n\n   a). 要求承诺不再接受编号小于`n`的的提案\n\n   b). 如果之前曾经接受过编号小于`n`的提案，则返回这些提案（指编号小于`n`的提案）中编号最大的那个提案\n\n2. 如果`proposer`接收到了`acceptors`中的`majority`的响应，则可以提议一个新的编号为`n`的提案，如果这些响应中有包含`acceptor`已经接受的提案，则新的提案的值为这些返回的提案中编号最大的那个提案的值，否则该`proposer`可以自己决定一个值。\n\n`proposer`通过向`acceptor`集合发送请求来提议一个提案，该请求要求这些`acceptor`接受该提案，因此称作`accept`请求。发送`prepare`请求的目标`acceptor`集合不需要与发送`accept`请求的目标集合相同。\n\n​可以看到，`acceptor`可以接受两种请求：`prepare`请求和`accept`请求。`acceptor`可以忽略任何一种请求（本身由于网络的不稳定，请求或者响应消息本身就可能丢失），并不会影响协议的安全性。\n\n因为引入了`prepare`请求，P1 条件需要进行修订：\n\n**P1<sup>a</sup>：一个acceptor在接受一个编号为n的提案时，要求其没有响应过编号大于n的prepare请求**\n\n我们假设所有提案的编号是唯一的，那么现在我们就有一个完整的算法，来满足在多个实例之间确定一个值的算法了。 \n\n这里可以对`prepare`请求进行优化，如果一个`acceptor`在收到一个编号为`n`的`prepare`请求时，已经响应过一个具有更高编号的`prepare`请求，根据承诺这个`acceptor`不会再接受编号为`n`的提案了，因此这个时候可以忽略掉该次`prepare`请求。`acceptor`还可以忽略掉它已经接受的提案的`prepare`请求。\n\n通过这种优化，`acceptor`只需要记住它曾经接受的最高编号的提案以及它已经响应的最高编号的`prepare`请求的编号。为了维护条件 P2<sup>c</sup> ，因此 `acceptor` 必须将这些信息持久化保存，并且再重启之后能够恢复。请注意，`proposer`可以放弃任何一个提案并完全忘记它，只要它不尝试发布另一个具有相同编号的提案。 \n\n最终的算法分为两阶段：\n\n阶段一：\n\n1. `proposer`选择一个编号`n`，然后向`aceptors`中的`majority`发送`prepare`请求\n2. 当`acceptor`接收到该编号为`n`的`prepare`请求，如果编号`n`大于所有它已经响应过的`prepare`请求的编号，那么它响应这次`prepare`请求，承诺不会接受任何编号小于`n`的提案，并在返回中携带它已经接受过的编号最高的提案（如果有的话）。\n\n阶段二：\n\n1. 如果`proposer`的编号为`n`的`prepare`请求得到了`acceptors`中的`majority`的响应，那么它可以发送一个`accetpt`请求给`acceptors`的`majority`，该请求提议的提案编号为`n`，值为所有响应中编号最高的提案的值，如果所有响应都不包含提案，说明还没有提案被选中，并且由于`majority of acceptors`对编号为`n`的`prepare`请求的承诺，这时候不会有编号小于`n`的提案被选中，`proposer`可以自己确定一个要提议的值。\n2. 如果一个acceptor接收到了一个编号为n的accept请求，如果它还没有响应过一个编号大于n的prepare请求，则接受该提案。\n\n`proposer`可以多次提议，也可以在提议中途丢弃某个提案。如果一个提案已经过时（有更高的编号被提议），`acceptor`可以通知`proposer`丢弃该提案，然后重新提议一个编号更高的提案。\n\n##### 活锁问题\n如果有多个`proposer`同时在提议，第一个提交了编号为`n`的提案，第二个提交了编号`n+1`的提案，导致前一个`n`的提案被废弃，然后重新提交了`n+2`的提案，导致`n+1`的提案被废弃。。。这样一直下去，会导致活锁，系统最终无法选中一个值。\n\n`FLP Impossibility`已经证明了:\n> no completely asynchronous consensus protocol can tolerate even a single unannounced process death.\n\n我们可以参考`raft`的选主过程，每次`proposer`在下一次提议新的提案时，先等待一个**随机**的时间间隔，这样就可以有效改善活锁问题了。\n\n##### 学习选中的值\n前面说过，在`paxos`协议中有三中角色的`agent`，这里还有`learner`没有提及。\n\n`learner`需要学习系统中被选中的值。为了学习被选中的值，`learner`需要知道被大多数`acceptor`接受的提案。\n\n一种直观的做法是，可以让`acceptor`每次接受一个提案时，就通知所有的`learner`。假如系统有`m`个`acceptor`，`n`个`learner`，那么正常情况下一个提案会有`m*n`个请求被发送出去。\n\n另一种做法是，因为是非拜占庭模型，消息不会被篡改。可以让所有的`acceptor`只通知一个`learner`，然后当这个`learner`发现一个提案被选中时，就通知其他的`learner`，这只需要`m+n`个请求。然而这就存在单点故障了。\n\n更通常的做法是，`acceptor`通知一部分`learner`，然后这部分`learner`再去通知其余的`learner`。\n\n\n\n### Multi Paxos\n`Base Paxos`只能用于在多个进程中就一个值达到一致性，而这肯定是不符合我们的现实需求的。\n\n通常我们会对数据进行分片和多副本存储。\n\n分片的主要目的是为了解决单机的性能瓶颈，从而实现水平扩展，正所谓性能不够机器来凑。\n\n而多副本存储则是为了保证数据安全，防止单点故障。如果每个分片采用单副本存储，如果某台机器故障了，比如磁盘损坏了，那么上面的数据就丢失了。因为我们需要对每个分片进行多副本存储。\n\n那么，如果保证同一个分片的多个副本之间的数据一致性呢？\n\n在`<<Paxos Made Simple>>`中，作者提出了确定性状态机(deterministric state machine)。\n\n对于一个确定性状态机，从一个确定的状态输入相同的命令（也叫做日志），会进入另一个确定的状态。\n对于一组确定性状态机，只要执行相同序列的命令，最终都会处于一致的状态。\n\n将每个存储服务实现为确定性状态机，只要保证他们执行相同序列的命令，就可以保证多个副本之间的一致性。\n\n现在，为了保证副本之间的一致性，我们只需要保证这些副本执行一致的命令序列就可以了。\n\n我们可以将每个命令看成是一个日志。对于对序列中的每个日志，可以通过一个`BasePaxos`实例来确定。也就是，序列中第`i`个日志，就由第`i`个`BasePaxos`实例来确定。\n\n然而，每个`BasePaxos`实例，都需要发送`prepare`和`accept`请求。这里可以做一个优化，如果一个`proposer`提议了一个日志被选中，那么后续可以继续由该`proposer`提议，并且跳过`prepare`阶段，直接发送`accept`请求，并且保持提案中的编号`n`不变。而为了能够对这些日志进行区分，需要额外引入一个日志序列号。也就是日志和它的序列号组成提案的`value`。\n\n这种优化是安全的，即使同时存在多个`proposer`发起提议，也不过是退化成`BasePaxos`。\n\n也就是，当一个`proposer`提议成功，可以将他看成是`leader`，并且定期向其他`proposer`发送心跳，这些`proposer`在本地维护`leader`的租期定时器，在该期间不允许发起提议，并且接收到的客户端请求转发到当前`leader`。而即使同时存在多个`leader`，也不过是退化成`BasePaxos`，根据前面的推导，该算法是安全的。\n\n与`raft`协议进行对比，`multiPaxos`中提案的编号和日志的序列号，不就对应`raft`中的`term`和日志的`index`吗？只不过`raft`是强`leader`，只允许`leader`同步日志，从而保证日志的`index`是连续的，这更便于日志的查询和复制。而`multiPaxos`的日志序列号则允许空洞存在。\n\n\n根据`CAP`理论，一个分布式系统不能同时满足：\n- C：强一致性，每次读请求都可以获取到最新的写入，通常指的是线性一致性\n- A：可用性，每个请求都可以接收到非错误的返回（不需要保证总是可以读取到最新的写入）\n- P：分区容忍性，即使节点之间通信的任意数量的消息丢失或者延时，系统都可以正常运行。\n\n当设计分布式系统时，三者只能择其二。因为网络分区总是客观存在的，因此`P`是无法舍弃的，只能在`CP`和`AP`之间做选择。\n\n根据`Paxos`和`raft`的原理，通过这些协议实现的分布式存储系统，通常都是`CP`系统。\n\n\n\n\n\n### 参考\n- [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem)\n- [Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)\n- [paxosmulti-paxos-algorithm](http://amberonrails.com/paxosmulti-paxos-algorithm/)","tags":["paxos - consensus algorithm"]},{"title":"k8s informer源码分析","url":"/2019/12/28/k8s-informer/","content":"\n ![](/img/kubernetes-high-level-component-archtecture.jpg)\n\n我们与`k8s`集群的交互，主要是通过向`api-server`提交资源对象。\n\n举个例子，我们要部署一个应用，则只需要提交一个`deployment`类型的资源对象到`api-server`。\n\n`api-server`在验证用户提交的资源对象通过之后，会将该资源对象保存到`etcd`集群中。\n\n同时，`api-server`还需要通知`deployment-manager`有一个新的`deployment`资源对象被新增了，从而能够对它进行处理，比如创建`replicaSet`。\n\n也就是，在`k8s`中，需要有一个机制，能够让资源对象的`controller`能够感知到资源对象。\n\n`Informer`接口就是用于实现该功能。\n\n`Informer`提供了：\n\n- `ListAndWatch`：用于跟`api-server`同步资源对象列表和更新\n- 带索引的缓存功能：将资源对象缓存到本地，可以直接通过缓存查询资源对象内容，减轻`api-server`压力\n- 事件回调功能：提供回调机制，当有资源对象新增、更新或者删除时可以回调自定义接口\n\n`Informer`是`client-go`中一个非常核心的功能，除了在`controller-manager`、`scheduler`等组件中被广泛使用，在各种自定义资源对象的控制器中也离不开它的身影。\n\n![](/img/informer.png)\n\n上图描述了一个资源对象的控制器的大体工作流程，来自于极客时间的《深入剖析Kubernetes》课程：\n\n- 控制器通过`informer`与`api-server`同步资源对象的列表和变更\n- 在事件回调中，将事件加入到`workQueue`中，这里`workQueue`可以协调生产者与消费者之间的速率，而且消费失败的事件可以重新加入队列，一般使用限速队列，当消费失败重新加入队列到下次重新消费之前，限速器会根据重试次数产生一定的延时，因为一般消费失败，马上进行重试很大概率还是会失败。\n- 在控制循环中，不断从`workQueue`取出事件进行处理\n\n\n\n接下来看一下`informer`的实现，相关的代码都在`k8s.io/client-go/tools/cache`包下面\n\n### Informer相关的接口\n\n```go\ntype ResourceEventHandler interface {\n\tOnAdd(obj interface{})\n\tOnUpdate(oldObj, newObj interface{})\n\tOnDelete(obj interface{})\n}\n\ntype SharedInformer interface {\n\t// 添加一个事件handler\n\tAddEventHandler(handler ResourceEventHandler)\n\t// 添加一个事件handler，同时指定resync周期，上一个方法没有指定则使用默认的\n    // 如果resync周期大于0，则会定期将本地缓存中的资源对象加入到DeltaFifo中，重新触发事件\n\tAddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)\n\t// 获取本地缓存\n\tGetStore() Store\n\t\n\tGetController() Controller\n\t// 开始从apiServer同步资源对象\n    // 该方法会阻塞直到stop\n\tRun(stopCh <-chan struct{})\n\t// 是否List已经执行，并且List返回的资源对象已经从DeltaFifo中被pop完\n\tHasSynced() bool\n\t// 获取最新的资源版本号;k8s依赖于etcd，每个资源对象都有自己的全局唯一的资源版本号，并且是全局递增的\n\tLastSyncResourceVersion() string\n}\n\ntype SharedIndexInformer interface {\n\tSharedInformer\n\t// 给本地缓存添加索引\n\tAddIndexers(indexers Indexers) error\n    // 获取带索引的本地缓存\n\tGetIndexer() Indexer\n}\n```\n\n### 探索informer实现\n\n接下来，我们看一下`SharedIndexInformer`接口的实现\n\n##### 创建\n\n```go\ntype sharedIndexInformer struct {\n    // 带索引的本地缓存\n\tindexer    Indexer\n    // 控制器，上面图中的Reflector实际上是controller的\n\tcontroller Controller\n\t// 保存事件handler列表，并且分发事件\n\tprocessor             *sharedProcessor\n    // 用于检测本地缓存中的资源对象是否被更改了\n    // 如果更新了本地缓存，则会导致缓存与apiServer的数据不一致\n    // 如果我们确实要更新资源对象，则应该先使用DeepCopy获取一个副本，然后在副本上进行更新\n\tcacheMutationDetector MutationDetector\n\t// 提供与apiServer交互的ListAndWatch接口\n\tlisterWatcher ListerWatcher\n    // 该informer所关注的资源对象的类型\n\tobjectType    runtime.Object\n    // 定时检查是否需要resync的周期\n\tresyncCheckPeriod time.Duration\n    // 通过AddEventHandler方法添加事件handler的默认resync周期\n\tdefaultEventHandlerResyncPeriod time.Duration\n    \n    clock clock.Clock\n    // 是否已经运行、停止\n\tstarted, stopped bool\n\tstartedLock      sync.Mutex\n    // 保护事件handler列表的锁\n\tblockDeltas sync.Mutex\n}\n```\n\n接着看一下对应的创建：\n\n```go\n// NewSharedInformer creates a new instance for the listwatcher.\nfunc NewSharedInformer(lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration) SharedInformer {\n\treturn NewSharedIndexInformer(lw, objType, resyncPeriod, Indexers{})\n}\n\n// NewSharedIndexInformer creates a new instance for the listwatcher.\n// 参数lw提供listAndWatch的接口\n// 参数indexers则是本地缓存的索引函数\nfunc NewSharedIndexInformer(lw ListerWatcher, objType runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer {\n\trealClock := &clock.RealClock{}\n\tsharedIndexInformer := &sharedIndexInformer{\n        // 初始化processor\n\t\tprocessor:                       &sharedProcessor{clock: realClock},\n        // 创建一个带索引的本地缓存\n\t\tindexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),\n        // listAndWatch接口\n\t\tlisterWatcher:                   lw,\n\t\tobjectType:                      objType,\n\t\tresyncCheckPeriod:               defaultEventHandlerResyncPeriod,\n\t\tdefaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod,\n        // 默认是一个空实现\n\t\tcacheMutationDetector:           NewCacheMutationDetector(fmt.Sprintf(\"%T\", objType)),\n\t\tclock:                           realClock,\n\t}\n\treturn sharedIndexInformer\n}\n\n```\n\n上面的`NewIndex`方法，实际上返回的是一个并发安全的`map`，同时可以根据`indexers`中的索引函数来对资源对象进行索引。\n\n##### 启动\n\n```go\nfunc (s *sharedIndexInformer) Run(stopCh <-chan struct{}) {\n\tdefer utilruntime.HandleCrash()\n\n\tfifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) // indexer会作为fifo的knownObjects\n\n\tcfg := &Config{\n\t\tQueue:            fifo, // 先进先出队列\n\t\tListerWatcher:    s.listerWatcher,\n\t\tObjectType:       s.objectType,\n\t\tFullResyncPeriod: s.resyncCheckPeriod,\n\t\tRetryOnError:     false,\n        // 该方法会定时调用，判断是否有事件handler需要resync\n\t\tShouldResync:     s.processor.shouldResync, // 是否同步\n\t\t// watch到的事件，最终会回调s.HandleDeltas\n        // 该方法中会更新本地缓存，然后再通过s.processor将事件分发给注册的事件handler\n\t\tProcess: s.HandleDeltas, \n\t}\n\n\tfunc() {\n\t\ts.startedLock.Lock()\n\t\tdefer s.startedLock.Unlock()\n\t\t// 创建controller\n\t\ts.controller = New(cfg) \n\t\ts.controller.(*controller).clock = s.clock\n\t\ts.started = true\n\t}()\n\n\t// Separate stop channel because Processor should be stopped strictly after controller\n\tprocessorStopCh := make(chan struct{})\n\tvar wg wait.Group\n\tdefer wg.Wait()              // Wait for Processor to stop\n\tdefer close(processorStopCh) // Tell Processor to stop\n    // 后台运行定时脏缓存检查\n\twg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)\n    // 我们注册的事件handler会被包装成processorListener\n    // 后台运行processorListener的pop和run方法，每个processorListener会开两个子协程\n\twg.StartWithChannel(processorStopCh, s.processor.run)\n\n\tdefer func() {\n\t\ts.startedLock.Lock()\n\t\tdefer s.startedLock.Unlock()\n\t\ts.stopped = true // Don't want any new listeners\n\t}()\n    // run controller\n\ts.controller.Run(stopCh)\n}\n```\n\n可以看到，在`Run`方法中：\n\n- 设置状态为开始运行\n- 创建一个`deltaFifo`和`controller`\n- 后台运行脏缓存检查\n- 后台为每个`processorListener`启动`pop`和`run`工作协程，开始监听事件。一旦有新的事件到来，会先调用`s.HandleDeltas`将其更新到本地缓存，然后通过`channel`通知每个`processorListener`，`processorListener`再去回调我们注册的事件handler。\n- 调用`controller`的`Run`方法，实际跟`api-server`的`listAndWatch`交互是由`controller`来负责的\n\n### 解密controller\n\n接下来我们看一下`controller`的实现：\n\n```go\n// Controller is a generic controller framework.\ntype Controller interface {\n    // 开始运行\n\tRun(stopCh <-chan struct{})\n    // Informer的HasSynced的实现\n\tHasSynced() bool\n    // Informer的LastSyncResourceVersion的实现\n\tLastSyncResourceVersion() string\n}\n\n// Controller is a generic controller framework.\ntype controller struct {\n\tconfig         Config\n\treflector      *Reflector\n\treflectorMutex sync.RWMutex\n\tclock          clock.Clock\n}\n\n// New makes a new Controller from the given Config.\nfunc New(c *Config) Controller {\n\tctlr := &controller{\n\t\tconfig: *c,\n\t\tclock:  &clock.RealClock{},\n\t}\n\treturn ctlr\n}\n```\n\n接下来看一下主要的`Run`方法的逻辑：\n\n```go\nfunc (c *controller) Run(stopCh <-chan struct{}) {\n\tdefer utilruntime.HandleCrash()\n\tgo func() {\n\t\t<-stopCh\n\t\tc.config.Queue.Close()\n\t}()\n    \n    // 创建一个Reflector\n\tr := NewReflector(\n\t\tc.config.ListerWatcher,\n\t\tc.config.ObjectType,\n\t\tc.config.Queue,\n\t\tc.config.FullResyncPeriod,\n\t)\n\tr.ShouldResync = c.config.ShouldResync\n\tr.clock = c.clock\n\n\tc.reflectorMutex.Lock()\n\tc.reflector = r\n\tc.reflectorMutex.Unlock()\n\n\tvar wg wait.Group\n\tdefer wg.Wait()\n\t// 后台开始运行r.Run\n    // 该方法会执行ListAndWatch，并且定时触发resync检查\n\twg.StartWithChannel(stopCh, r.Run)\n\t// 运行c.processLoop，直到stop\n\twait.Until(c.processLoop, time.Second, stopCh)\n}\n```\n\n看一下`processLoop`方法：\n\n```go\nfunc (c *controller) processLoop() {\n\tfor {\n        // 这里的Queue，就是上面在s.Run里面创建的DeltaFifo\n        // Process实际上就是s.HandleDeltas\n        // ListAndWatch产生的事件会添加到DeltaFifo中，该方法不断从队列获取事件并回调Process\n\t\tobj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process))\n\t\tif err != nil {\n\t\t\tif err == ErrFIFOClosed {\n\t\t\t\treturn\n\t\t\t}\n            // 如果需要重试，则重新加入队列中\n\t\t\tif c.config.RetryOnError {\n\t\t\t\t// This is the safe way to re-enqueue.\n\t\t\t\tc.config.Queue.AddIfNotPresent(obj)\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Reflector：ListAndWatch\n\n我们接下来看一下`Reflector`的主要逻辑：\n\n```go\nfunc (r *Reflector) Run(stopCh <-chan struct{}) {\n\tklog.V(3).Infof(\"Starting reflector %v (%s) from %s\", r.expectedTypeName, r.resyncPeriod, r.name)\n\twait.Until(func() {\n        // 终于看到调用ListAndWatch了\n\t\tif err := r.ListAndWatch(stopCh); err != nil {\n\t\t\tutilruntime.HandleError(err)\n\t\t}\n\t}, r.period, stopCh)\n}\n```\n\n接下来看一下`ListAndWatch`方法，该方法有点长，只保留相关代码：\n\n```go\nfunc (r *Reflector) ListAndWatch(stopCh <-chan struct{}) error {\n \tvar resourceVersion string\n\n\toptions := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}\n\t// 执行List\n\tif err := func() error {\n\t\tvar list runtime.Object\n\t\tvar err error\n\t\tlistCh := make(chan struct{}, 1)\n   \n\t\tgo func() {\n\t\t\tpager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) {\n\t\t\t\treturn r.listerWatcher.List(opts)\n\t\t\t}))\n\t\t\tif r.WatchListPageSize != 0 {\n\t\t\t\tpager.PageSize = r.WatchListPageSize\n\t\t\t}\t\t\t\n            // 获取list列表\n\t\t\tlist, err = pager.List(context.Background(), options)\n\t\t\tclose(listCh)\n\t\t}()\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn nil\n\t\tcase <-listCh:\n\t\t}\n \n\t\tlistMetaInterface, err := meta.ListAccessor(list)\n\t\tresourceVersion = listMetaInterface.GetResourceVersion()\n \t\titems, err := meta.ExtractList(list)\n        // 会调用DeltaFifo的Replace接口，替换掉DeltaFifo的内容\n \t\tif err := r.syncWith(items, resourceVersion); err != nil {\n\t\t\treturn fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err)\n\t\t}\n \t\t// 更新已经同步的资源版本\n\t\tr.setLastSyncResourceVersion(resourceVersion)\n \t\treturn nil\n\t}(); err != nil {\n\t\treturn err\n\t}\n\n \n\t// 定时触发resync检查\n\tgo func() {\n\t\tresyncCh, cleanup := r.resyncChan()\n\t\tdefer func() {\n\t\t\tcleanup() // Call the last one written into cleanup\n\t\t}()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-resyncCh:\n\t\t\tcase <-stopCh:\n\t\t\t\treturn\n\t\t\tcase <-cancelCh:\n\t\t\t\treturn\n\t\t\t}\n            // ShouldResync会检查是否需要触发resync\n\t\t\tif r.ShouldResync == nil || r.ShouldResync() {\n                // 如果需要触发resync，则调用DeltaFifo的Resync\n\t\t\t\tif err := r.store.Resync(); err != nil {\n\t\t\t\t\tresyncerrc <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tcleanup()\n\t\t\tresyncCh, cleanup = r.resyncChan()\n\t\t}\n\t}()\n\n\t// 开始Watch\n\tfor {\n\n\t\ttimeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))\n\t\t\n\t\toptions = metav1.ListOptions{\n\t\t\tResourceVersion: resourceVersion,\n\t\t\tTimeoutSeconds: &timeoutSeconds,\n\t\t\tAllowWatchBookmarks: true,\n\t\t}\n\n\t\tw, err := r.listerWatcher.Watch(options)\n\t\tif err != nil {\n\t\t\tif utilnet.IsConnectionRefused(err) {\n\t\t\t\ttime.Sleep(time.Second)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\t// 处理watch到的事件\n\t\tif err := r.watchHandler(w, &resourceVersion, resyncerrc, stopCh); err != nil {\n\t\t\t...\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n```\n\n`watch`的主要逻辑都在`watchHandler`中：\n\n```go\n\nfunc (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh <-chan struct{}) error {\n\tstart := r.clock.Now()\n\teventCount := 0\n\n\tdefer w.Stop()\n\nloop:\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn errorStopRequested\n\t\tcase err := <-errc:\n\t\t\treturn err\n\t\t\t// watch事件\n\t\tcase event, ok := <-w.ResultChan():\n\t\t\tif !ok {\n\t\t\t\tbreak loop\n\t\t\t}\n\t\t \n\t\t\tmeta, err := meta.Accessor(event.Object)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// 获取对应的资源版本\n\t\t\tnewResourceVersion := meta.GetResourceVersion()\n\t\t\tswitch event.Type {\n\t\t\t// 新增事件\n\t\t\tcase watch.Added:\n\t\t\t\t// 调用DeltaFifo的Add接口\n\t\t\t\terr := r.store.Add(event.Object)\n\t\t\t\tif err != nil {\n\t\t\t\t\tutilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err))\n\t\t\t\t}\n\t\t\t\t// 更新事件\n\t\t\tcase watch.Modified:\n\t\t\t\t// 调用DeltaFifo的Update接口\n\t\t\t\terr := r.store.Update(event.Object)\n\t\t\t\tif err != nil {\n\t\t\t\t\tutilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err))\n\t\t\t\t}\n\t\t\t\t// 删除事件\n\t\t\tcase watch.Deleted:\n\t\t\t\terr := r.store.Delete(event.Object)\n\t\t\t\tif err != nil {\n\t\t\t\t\tutilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err))\n\t\t\t\t}\n\t\t\tcase watch.Bookmark:\n\t\t\t\t// A `Bookmark` means watch has synced here, just update the resourceVersion\n\t\t\tdefault:\n\t\t\t\tutilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event))\n\t\t\t}\n\t\t\t// 更新最后同步的资源版本\n\t\t\t*resourceVersion = newResourceVersion\n\t\t\tr.setLastSyncResourceVersion(newResourceVersion)\n\t\t\teventCount++\n\t\t}\n\t}\n \n \treturn nil\n}\n```\n\n\n\n### InformerFactory\n\n`client-go`还提供了一个`SharedInformerFactory`，方便我们创建（多个）`Informer`。\n\n\n\n### 总结\n\n我们可以看到，`Reflector`会通过`ListAndWatch`接口，从`api-server`同步资源对象的变更情况，然后添加到`DeltaFifo`队列中；\n\n`controller`会不断从`DeltaFifo`中取出事件，然后回调`sharedIndexInformer`的`HandleDeltas`方法；\n\n`HandleDeltas`方法中，会更新本地缓存，然后根据事件类型调用注册的事件handler。\n\n还要一些细节，比如`DeltaFifo`，`Indexer`，`processorListener`的实现，或者`Informer`的`HasSynced`接口实现等，比较简单就不在讲了。\n\n\n","tags":["k8s - informer"]},{"title":"红黑树没有想象的那么难","url":"/2019/12/06/红黑树/","content":"\n\n红黑树是特殊的4阶b-tree，也就是2-3-4  tree。\n\n根据 Knuth 的定义，一个 *m* 阶的B树是一个有以下属性的树：\n\n1. 每一个节点最多有 *m* 个子节点\n2. 每一个非叶子节点（除根节点）最少有 ⌈*m*/2⌉ 个（向上取整）子节点，比如5阶B树则最少要有3个子节点\n3. 如果根节点不是叶子节点，那么它至少有两个子节点\n4. 有 *k* 个子节点的非叶子节点拥有 *k* − 1 个键\n5. 所有的叶子节点都在同一层\n\nb-tree插入新的节点时，如果当前节点满了，也就是上溢，则分裂成一个父键和两个子节点，将父键插入到原节点的父节点中；插入之后，如果原节点的父节点也上溢了，则继续分裂，插入\n\nb-tree删除节点时，首先转换成删除前驱节点或者后继节点；如果导致当前键数量不足 ⌈*m*/2⌉-1，也就是下溢，则首先尝试向sibling借一个键；如果sibling不足以借出，则sibling的键个数刚好处于 ⌈*m*/2⌉-1，这时候这将两个子结点和父节点的一个键进行合并，这时候新的节点键个数刚好为m-1个。由于合并时需要一个父节点的键，相当于从父节点删除了这个键，如果导致父节点发生下溢，则同样，首先尝试从父节点的sibling借一个键，如果sibling不足以借出，则合并\n\n###  红黑树的性质\n\n红黑树是二叉搜索树，除了具备二叉搜索树的性质之外，还具有下列性质：\n\n1. 节点是`BLACK`或者`RED`\n2. 根节点总是`BLACK`\n3. `RED`节点的子结点只能是`BLACK`节点，不能有两个`RED`节点相连\n4. 叶子节点总是空的`BLACK`节点；从根节点到叶子节点的所有路径，具有相同数量的`BLACK`节点。因此`rb-tree`树中的最长路径最长只会是最短路径的两倍。其中最长路径为黑红交替，最短路径为全黑。该性质确保红黑树不会退化成链表，保证了搜索效率。\n\n\n\n红黑树是从`2-3-4`树演化过来的，一个黑色节点和一个红色节点组成一个`3-node`，一个黑色节点和两个红色节点组成一个`4-node`：\n\n![](../img/rb_node.png)\n\n红黑树的黑色节点，可以看成是2-3-4树节点中的中间key。将红黑树与2-3-4树联系起来，就很容易理解红黑树的几个性质了。\n\n\n\n### 查询\n\n二叉搜索树常规操作。\n\n\n\n### 插入\n\n为了不破坏性质4，新插入的节点总是红色的。\n\n如果新插入的节点，其父节点是黑色的，那么这时候不会破坏红黑树的性质，插入完成。\n\n如果新插入的节点，其`uncle`节点也是红色的：\n\n![](/img/rb_add_red3.png)\n\n如上图，这时候，可以看成是往一个`4-node` 插入了新值，这时候需要将`4-node`分裂成一个`2-node`和一个`3-node`，对应的操作就是将`gp`设置成`RED`，将`p`和`uncle`设置成`BLACK`。`gp`设置成`RED`，也就是将`gp`插入其父节点所在的`node`中，可能会触发新的分裂，这是一个递归的过程。\n\n而当`uncle`节点是`BLACK`，说明当前节点还没有满，不需要分裂。有时候需要进行`rotate`，调整一下`BLACK`节点的位置：\n\n![](/img/rb_add_red1.png)\n\n如果`uncle`是`BLACK`，说明`uncle`是属于其他`node`，而`new`，`p`，和`gp`组成一个`4-node`。而这时候`p`才是`4-node`的中间，需要对`gp`执行`right-rotate`以及重新着色。\n\n![](/img/rb_add_red2.png)\n\n这种情况，`p`，`new`和`gp`组成一个`4-ndoe`，而`new`才是中间，需要先对`p`执行`left-rotate`再对`gp`执行`right-rotate`，然后重新着色\n\n可以看到，红黑树的插入，只需执行多次重新着色以及最多两次`rotate`，是比较高效。\n\n\n\n### 删除\n\n先按照BST的常规做法，使用待删除节点的前驱或者后继节点替代，转变为删除前驱或者后继节点，记为`x`\n\n1. 如果x是红色的，直接删除，不会破坏红黑树的规则\n\n2. 如果x具有一个红色的child，说明当前节点是`3-node`，使用child节点替代并染成黑色，完成删除\n\n3. 说明当前节点是一个`2-node`，需要向sibling借或者与parent合并\n\n   3.1 如果sibling是红色的，说明parent是一个3-node，这时候先对parent执行rotate，将sibling转变为3-node的中间节点，即黑色的，而当前的parent则变成红色，更新x当前新的sibling。B树删除时，会先考虑向slibing借一个节点，但是实际上是向父节点拿一个节点，然后从sibling拿一个节点补充给父节点。我们这一步rotate，是要设置x真正的sibling。\n\n      ![](/img/RBTreeDelete31.png)\n\n      \n\n   3.2 如果sibling的两个子节点都是黑色的，说明silbing也是一个2-node，这时候无法向sibling借一个节点，因此需要与父节点合并，将sibling变为red；如果parent是红色的，说明x的grantprarent至少是3-node，并不会导致其产生下溢，将其变成黑色的，完成删除。如果x的parent是黑色的，则可以看成当前正在删除x的parent，设置x为x.parent，从3.1重新开始\n\n      ![](/img/RBTreeDelete32.png)\n\n   3.3 如果当前节点是左分支，且sibling的左儿子是红色的，右儿子是黑色的，则sibling是一个3-node，可以借出一个节点，但是需要先调换sibling和sibling.left的颜色，然后对sibling进行right rotate，本质上就是原来sibling是3-node的中间key，现在需要调换sibling.left为中间key，继续下一步；如果当前节点是右分支，与之类似\n\n      ![](/img/RBTreeDelete33.png)\n\n   3.4 如果当前节点是左分支，且sibling的右儿子是红色的，那么sibling是一个3-node或者4-node，这时候可以借出节点，将sibling的颜色设置为parent的颜色，sibling的right设置为黑色，parent设置为黑色，然后对parent执行left rotate，完成删除。本质上是借出sibling，然后sibling.right变成中间节点；如果是右分支，与之类似\n\n      ![](/img/RBTreeDelete34.png)\n\n[go语言实现](https://github.com/ymcvalu/leetcode/tree/master/data_structure/tree/rb_tree)\n\n### 参考\n\n[red black tree]( https://algorithmtutor.com/Data-Structures/Tree/Red-Black-Trees/ )\n","tags":["data structure"]},{"title":"用好切片","url":"/2019/12/03/slice/","content":"`slice`是`go`中比较简单的一个数据结构了（当然，`string`更加简单）。\n\n先看其定义：\n```go\ntype slice struct {\n\tarray unsafe.Pointer  // 对应底层数组\n\tlen   int             // 切片当前长度，也就是具有的元素个数\n\tcap   int             // 底层数组的实际长度\n}\n```\n\n可以看到切片实际上有3个字段，第一个是其底层数组，第二个是当前具有的元素个数，我们可以直接使用下标`[0,len)`来访问他们，而`cap`字段是底层数组的长度。\n我们可以使用`make`来创建一个切片：\n```go\ns1 := make([]User,0,10)\n```\n表示创建一个长度是0，容量是10的User切片，然后我们可以使用内置函数`append`来追加元素：\n```go\n s1 = append(s1, User{})\n```\n使用`append`，如果切片的底层数组如果还有足够的容量，则可以直接扩展`len`字段，否则会发生切片的扩容，具体可以参考[切片扩容](https://mcll.top/2019/02/26/slice%E6%89%A9%E5%AE%B9/)\n因为`slice`本质上是一个结构体，是一个值类型，而`append`需要更改内部状态，因此会创建一个新的切片返回。\n如果发生了扩容，那么新返回的切片的底层数组和原来的底层数组是两个不同的数组，否则会共享同一个底层数组。\n\n如果我们需要append可以返回一个有独立底层数组的切片，则可以：\n```go\n\ts1 := make([]User, 10, 20)\n\ts2 :=append(s1[::len(s1)], User{}) // s1[::len(s1)]会创建一个cap等于len的切片，这样append就会发生扩容\n```\n\n\n在平常的开发中，我们经常使用到切片的一个场景是数据库分页查询，创建一个`slice`，然后把返回的结果集装到这个`slice`中。\n\n那么在这个场景下我们应该怎么用好`slice`呢？\n\n首先，因为我们是分页查询，结果集的大小是已知的，因此我们可以**预分配切片大小**。\n\n然后，是使用`[]User`好还是`[]*User`好呢？\n\n因为`go`的内存管理会将`mspan`按照`sizeClass`进行划分，从`8byte`到`32kb`，并且会在`P`中缓存每个级别的`mspan`（每个P中都有一个mcache，mcache中针对每种sizeClass缓存两个mspan，一个只用于分配不需要gc扫描的对象，这样整个mspan中的page都不需要扫描）。\n\n因此，如果对象的大小不超过`32kb`（实际上我们很少会用到那么大的对象），那么内存分配的效率与分配的大小并没有多大关系。而且本身频繁的内存分配也是一种性能损耗。\n\n因此，**推荐的是使用`[]User`类型**。这样只会分配一次内存。而如果使用`[]*User`类型，需要分配n+1次，底层数组1次加上n次切片元素。\n\n然而，事与愿违的是，我们一般会使用第三方`orm`框架，这些框架中，会循环的通过反射去`new`对象，然后`append`进去。\n\n\n那么切片什么时候等于`nil`呢？\n\n`slice`本身本质上是一个结构体，只有当其三个字段都是零值的时候，才等于`nil`。\n\n在代码中，我们判断一个`slice`是否为空，并不需要关心其是否是`nil`,只要使用`len(s) == 0`就可以了\n```go\nvar s []int // s == nil\n\ns1 = append(s, 1) // 发生扩容\n\ns2 = s[:0] // s2 != nil\n```\n像上面的操作，虽然`s`等于`nil`，但是这些操作都可以正常执行，因此我们在平时的使用中，并不需要刻意关心切片是否等于`nil`。\n\n但是当`json`序列化时，如果切片为`nil`时，对应序列化为`null`，而如果是非`nil`的空切片，则对应为`[]`。\n\n\n当我们声明一个变量时：\n```go\n var s1 []int\n```\n\n因为初始化为零值，则对应的切片结构体三个字段都是零值，因此`s1==nil`\n\n而当我们这样写时：\n```go\n var s1 = []int{}\n var s2 = make([]int, 0, 0)\n```\n这两个切片都是非`nil`的空切片。而又因为切片的`cap`是零，`go`会将其`data`字段初始化为：\n```go\n // base address for all 0-byte allocations\n var zerobase uintptr\n```\n\n`zerobase`是声明在`runtime`包下面的一个全局变量，当需要分配一块零字节的内存时，都会返回该变量的地址：\n```go\nfunc mallocgc(size uintptr, typ *_typ, needzero bool) unsafe.Pointer {\n\tif size == 0 {\n\t\treturn unsafe.Pointer(&zerobase)\n\t}\n\t...\n}\n```","tags":["go"]},{"title":"redis集群搭建","url":"/2019/08/30/redis集群搭建/","content":"\n### 环境准备\n\n准备六台服务器（系统为`centos7`），并且安装make、gcc等工具\n\n每个节点需要开放指定端口，为了方便，关闭防火墙，生产不要关闭\n\n```sh\n$ service iptables stop # centos 6.x\n$ systemctl stop firewalld.service # centos 7.x\n```\n\n### 编译redis源码\n下载`redis`源码（`redis-5.0.3.tar.gz`)，解压进入项目目录后，执行`make MALLOC=libc`进行编译，编译完成后，可以在`src/`目录下看到编译生成的二进制可执行文件。\n\n### 启动redis实例\n\n##### 编辑配置文件\n\n一个节点可以有多个redis实例，这里每台只配置两个实例。\n\n在每个节点上创建目录`~/redis-cluster/7000`和`~/redis-cluster/7001`两个目录，这里`7000`和`7001`表示实例监听端口，如果一个节点需要部署多个实例，就创建多个不同端口号的目录，然后在每个目录下创建`redis.conf`文件：\n\n```ini\n#端口7000，7001\nport 7000\n\n#默认ip为127.0.0.1，需要改为其他节点机器可访问的ip，否则创建集群时无法访问对应的端口，无法创建集群\nbind 0.0.0.0\n\n#redis后台运行\ndaemonize yes\n\n#pidfile文件 7000和7001\npidfile /var/run/redis_7000.pid\n\n#开启集群\ncluster-enabled yes\n\n#集群的配置，配置文件首次启动自动生成   \ncluster-config-file nodes_7000.conf\n\n#请求超时，默认15秒，可自行设置 \ncluster-node-timeout 10100\n\n#aof日志开启，有需要就开启，它会每次写操作都记录一条日志\nappendonly yes\n\n#默认是yes，只要有结点宕机导致16384个槽不能都可以访问到，整个集群就全部停止服务，所以一定要改为no\ncluster-require-full-coverage no\n```\n\n##### 启动\n在每个节点执行：\n\n```sh\n$ for ((i=0;i<2;i++)); do redis-5.0.3/src/redis-server redis-cluster/700$i/redis.conf; done\n```\n\n启动时，会为每个实例生成nodeid，并在当前目录生成`nodes_7000.conf`和`nodes_7001.conf`两个文件。\n\n检查进程：\n\n```sh\n$ ps -ef | grep redis           //redis是否启动成功\n$ netstat -tnlp | grep redis    //监听redis端口\n```\n\n### 创建集群\n官方提供了`redis-trib.rb`来创建集群，就在`redis-5.0.3/src`目录下，该脚本只需要在集群中任意一个节点上执行一次就行。\n\n##### 安装ruby\n\n如果是`redis-4.x.x`版本，需要安装ruby。\n\n```sh\n$ yum -y install ruby ruby-devel rubygems rpm-build\n```\n\n在`centos 7`中，安装的`ruby`版本过低，使用下面方法安装：\n\n```sh\n$ yum install centos-release-scl-rh\n$ yum install rh-ruby23 -y\n$ scl  enable  rh-ruby23 bash\n$ ruby -v\n```\n\n安装`redis`\n\n```sh\n$ gem install redis -v 3.3.5\n```\n\n##### 集群创建\n###### 5.0以下\n如果为`5.0`以下版本创建集群，则执行官方的ruby脚本，这个工具能自动检测服务器分配master和slave，`--replicas`指定每个`master`有几个`slave`\n\n```sh\n$ redis-4.0.12/src/redis-trib.rb create --replicas 3 10.0.1.50:7000 10.0.1.50:7001 10.0.1.19:7000 10.0.1.19:7001 10.0.1.133:7000 10.0.1.133:7001 10.0.1.169:7000 10.0.1.169:7001 10.0.1.210:7000 10.0.1.210:7001 10.0.1.127:7000 10.0.1.127:7001\n```\n\n###### 5.0之后\n通过`redis-cli`客户端工具来创建集群，` --cluster-replicas 3`指定每个`master`有3个`slave`\n```sh\n$ redis-cli --cluster create --replicas 3 10.0.1.50:7000 10.0.1.50:7001 10.0.1.19:7000 10.0.1.19:7001 10.0.1.133:7000 10.0.1.133:7001 10.0.1.169:7000 10.0.1.169:7001 10.0.1.210:7000 10.0.1.210:7001 10.0.1.127:7000 10.0.1.127:7001 --cluster-replicas 3\n```\n输出：\n\n```\n>>> Performing hash slots allocation on 12 nodes...\nMaster[0] -> Slots 0 - 5460\nMaster[1] -> Slots 5461 - 10922\nMaster[2] -> Slots 10923 - 16383\nAdding replica 10.0.1.169:7000 to 10.0.1.50:7000\nAdding replica 10.0.1.210:7000 to 10.0.1.50:7000\nAdding replica 10.0.1.127:7000 to 10.0.1.50:7000\nAdding replica 10.0.1.50:7001 to 10.0.1.19:7000\nAdding replica 10.0.1.133:7001 to 10.0.1.19:7000\nAdding replica 10.0.1.169:7001 to 10.0.1.19:7000\nAdding replica 10.0.1.19:7001 to 10.0.1.133:7000\nAdding replica 10.0.1.210:7001 to 10.0.1.133:7000\nAdding replica 10.0.1.127:7001 to 10.0.1.133:7000\nM: 0c58af35acebf7988ff8eade95dfc9873eefe89c 10.0.1.50:7000\n   slots:[0-5460] (5461 slots) master\nS: da377f156086211e31170674caabb348bdb544e3 10.0.1.50:7001\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nM: 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d 10.0.1.19:7000\n   slots:[5461-10922] (5462 slots) master\nS: 903b50ea7ed1524f491d10b6f0be6e16dbfb8d40 10.0.1.19:7001\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nM: d0e3847c6b89140566a180e6a536c053b64ab901 10.0.1.133:7000\n   slots:[10923-16383] (5461 slots) master\nS: 0f8e1743c272e959cabea082002a2b3e4dc24dd0 10.0.1.133:7001\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nS: bf03f0fe45fc6d1850f3edfeec918193a61f0bba 10.0.1.169:7000\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: 047f288ddf0ec6a92042520aa4b812e531803aed 10.0.1.169:7001\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nS: 3c1a50c8e195f93d569abcb0bc77b2b8dfcef828 10.0.1.210:7000\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: 229ee5af18dab07d5b813acbc9f9eb17b9132dd7 10.0.1.210:7001\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nS: 2b7ad5f50a04b5df6015a06b2be3b07f0384c557 10.0.1.127:7000\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: 20172d062bd9b2a7d9c833ab1e653e25bbb6be03 10.0.1.127:7001\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nCan I set the above configuration? (type 'yes' to accept): \n```\n可以看到分配了三个主节点，并且每个主节点分配了三个从节点，输入`yes`完成创建：\n```\n>>> Nodes configuration updated\n>>> Assign a different config epoch to each node\n>>> Sending CLUSTER MEET messages to join the cluster\nWaiting for the cluster to join\n..........\n>>> Performing Cluster Check (using node 10.0.1.50:7000)\nM: 0c58af35acebf7988ff8eade95dfc9873eefe89c 10.0.1.50:7000\n   slots:[0-5460] (5461 slots) master\n   3 additional replica(s)\nS: 0f8e1743c272e959cabea082002a2b3e4dc24dd0 10.0.1.133:7001\n   slots: (0 slots) slave\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nS: 229ee5af18dab07d5b813acbc9f9eb17b9132dd7 10.0.1.210:7001\n   slots: (0 slots) slave\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nS: 3c1a50c8e195f93d569abcb0bc77b2b8dfcef828 10.0.1.210:7000\n   slots: (0 slots) slave\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: 047f288ddf0ec6a92042520aa4b812e531803aed 10.0.1.169:7001\n   slots: (0 slots) slave\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nS: da377f156086211e31170674caabb348bdb544e3 10.0.1.50:7001\n   slots: (0 slots) slave\n   replicates 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d\nS: 2b7ad5f50a04b5df6015a06b2be3b07f0384c557 10.0.1.127:7000\n   slots: (0 slots) slave\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: bf03f0fe45fc6d1850f3edfeec918193a61f0bba 10.0.1.169:7000\n   slots: (0 slots) slave\n   replicates 0c58af35acebf7988ff8eade95dfc9873eefe89c\nS: 903b50ea7ed1524f491d10b6f0be6e16dbfb8d40 10.0.1.19:7001\n   slots: (0 slots) slave\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nS: 20172d062bd9b2a7d9c833ab1e653e25bbb6be03 10.0.1.127:7001\n   slots: (0 slots) slave\n   replicates d0e3847c6b89140566a180e6a536c053b64ab901\nM: d0e3847c6b89140566a180e6a536c053b64ab901 10.0.1.133:7000\n   slots:[10923-16383] (5461 slots) master\n   3 additional replica(s)\nM: 1e8cc5ef1f15cea04e94f48610a66ebc8a775d8d 10.0.1.19:7000\n   slots:[5461-10922] (5462 slots) master\n   3 additional replica(s)\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n```\n\n### 测试集群\n\n使用客户端工具连接`redis`集群：\n\n```sh\n$ redis-5.0.3/src/redis-cli -c -p 7000\n```\n\n上面的`-c`表示连接集群，当访问的`key`不在当前节点时，可以重定向到目标节点获取数据：\n\n```\n127.0.0.1:7001> get foo\n-> Redirected to slot [12182] located at 10.0.1.210:7001\n\"haha\"\n```\n\n### 查看集群状态\n```sh\n10.0.1.210:7001> cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:12\ncluster_size:3\ncluster_current_epoch:13\ncluster_my_epoch:13\ncluster_stats_messages_ping_sent:4896\ncluster_stats_messages_pong_sent:3255\ncluster_stats_messages_meet_sent:2\ncluster_stats_messages_auth-req_sent:11\ncluster_stats_messages_sent:8164\ncluster_stats_messages_ping_received:3234\ncluster_stats_messages_pong_received:3382\ncluster_stats_messages_meet_received:9\ncluster_stats_messages_fail_received:2\ncluster_stats_messages_auth-ack_received:2\ncluster_stats_messages_received:6629\n```\n\n### redis集群的数据分片\n> Redis Cluster does not use consistent hashing, but a different form of sharding where every key is conceptually part of what we call an **hash slot**.\n>\n> There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.\n>\n> Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where:\n>\n> - Node A contains hash slots from 0 to 5500.\n> - Node B contains hash slots from 5501 to 11000.\n> - Node C contains hash slots from 11001 to 16383.\n\n\nredis集群没有使用hash一致性来存储来对key进行分配，而是使用哈希槽的概念。\n\n在redis集群中有16384个哈希槽，通过使用`CRC16(key)%16384`来确定key存储在哪个哈希槽中。\nredis集群中每个节点可以包含多个哈希槽，比如：\n- 节点A：0~5500\n- 节点B：5501~11000\n- 节点C：11001~16383\n\n##### mult-keys 操作\n`redis`集群对`mult-keys`命令的支持有限：\n\n**Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot**.The user can force multiple keys to be part of the same hash slot by using a concept called *hash tags*.\n\n一个事务、lua脚本或者`mult-keys`命令只能使用位于同一个哈希槽中的`key`，也就是不支持一个`mult-keys`操作不支持跨哈希槽。可以使用`hash tags`来强制让需要在一个命令中执行的数据存储到同一个哈希槽中。\n\n在key中包含`{TAG}`，则会使用`{}`中的`TAG`来计算该key所属的哈希槽，比如`{foo}k1`和`{foo}k2`将会被分配到同一个哈希槽，这样就可以在`lua`脚本中同时使用`{foo}k1`和`{foo}k2`。否则如果在一个`lua`脚本中同时访问不同哈希槽中的key将会报错。\n\n### codis\n和redis集群不同的是，`Codis`采用一层无状态的`proxy`层，将分布式逻辑写在`proxy`上，底层的存储引擎还是`Redis`本身。\n\n`codis`通过`presharding`把数据在概念上分成`1024`个`slot`，然后在`proxy`中将不同`key`的请求转发到不同的机器上，通过`crc32(key)%1024`计算`key`对应的`slot`。\n\n`Codis`支持的`MGET/MSET`无法保证原本单点时的原子语义。 因为`MSET`所参与的`key`可能分不在不同的机器上，如果需要保证原来的语义，也就是要么一起成功，要么一起失败，这样就是一个分布式事务的问题，对于`Redis`来说，并没有`WAL`或者回滚这么一说，所以即使是一个最简单的二阶段提交的策略都很难实现，而且即使实现了，性能也没有保证。所以在`Codis`中使用`MSET/MGET`其实和你本地开个多线程`SET/GET`效果一样，只不过是由`codis`帮忙实现了。\n\n`codis`支持`lua`脚本，但是**仅仅是转发而已，它并不保证你脚本操作的数据是否在正确的节点上。**比如，脚本里涉及操作多个`key`，`Codis`能做的就是将这个脚本分配到参数列表中的第一个`key`的机器上执行。所以这种场景下，你需要自己保证你的脚本所用到的`key`分布在同一个机器上，这里可以采用`hashtag`的方式。","tags":["redis"]},{"title":"gops","url":"/2019/08/29/gops/","content":"[gops](https://github.com/google/gops): A tool to list and diagnose Go processes currently running on your system\n\n`gops`能够列出当前系统中运行的`go`进程，并且能够帮助对指定`go`进程进行诊断，是`gopher`必备居家良品。\n\n### 安装\n```sh\n$ go get -u github.com/google/gops\n```\n\n### 简单使用\n##### 查看命令帮助\n```sh\n$ gops \ngops is a tool to list and diagnose Go processes.\n\nUsage:\n  gops <cmd> <pid|addr> ...\n  gops <pid> # displays process info\n  gops help  # displays this help message\n\nCommands:\n  stack      Prints the stack trace.\n  gc         Runs the garbage collector and blocks until successful.\n  setgc\t     Sets the garbage collection target percentage.\n  memstats   Prints the allocation and garbage collection stats.\n  version    Prints the Go version used to build the program.\n  stats      Prints runtime stats.\n  trace      Runs the runtime tracer for 5 secs and launches \"go tool trace\".\n  pprof-heap Reads the heap profile and launches \"go tool pprof\".\n  pprof-cpu  Reads the CPU profile and launches \"go tool pprof\".\n\nAll commands require the agent running on the Go process.\n\"*\" indicates the process is running the agent.\n```\n\n##### 查看当前系统的go进程\n```sh\n$ gops \n3348  3088  docker-containerd       go1.7.5  /usr/bin/docker-containerd\n3088  1     dockerd                 go1.7.5  /usr/bin/dockerd\n26504 3088  docker-proxy            go1.7.5  /usr/bin/docker-proxy\n26510 3348  docker-containerd-shim  go1.7.5  /usr/bin/docker-containerd-shim\n29311 18550 gops                    go1.11.2 /usr/local/bin/gops\n29135 27413 gops-demo             * go1.11.2 /home/vagrant/go/src/just-for-fun/gops/gops-demo\n```\n上面各列的含义分别是：\n```\npid ppid 二进制文件名 go编译版本 二进制文件路径\n```\n而在go编译版本前面的 `*` 表明当前进程包含了`agent`。\n\n##### 查看某个进程的信息\n```sh\n$ gops 29573\nparent PID:\t27413\nthreads:\t6\nmemory usage:\t0.278%\ncpu usage:\t0.012%\nusername:\troot\ncmd+args:\t./gops-demo\nelapsed time:\t02:49\nlocal/remote:\t:::5050 <-> :::0 (LISTEN)\nlocal/remote:\t:::8989 <-> :::0 (LISTEN)\n```\n\n### 进程诊断\n为了能够使用诊断功能，我们需要在我们的程序代码中启动一个`agent`：\n```go\n// import \"github.com/google/gops/agent\"\n\nfunc main(){\n\terr := agent.Listen(agent.Options{\n\t\tAddr:            \":5050\", // agent监听地址\n\t\tShutdownCleanup: false, // 如果true，会监听SIGINT信号，自动关闭agent并退出进程\n\t})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer agent.Close() // 手动关闭agent\n\n  .....\n\n}\n```\n\n在启动`agent`的时候，我们需要指定监听的端口。\n\n启动了`agent`之后，我们在`gops`中就可以看到多了一个`*`：\n```sh\n$ gops\n29135 27413 gops-demo   * go1.11.2  /home/...\n```\n\n开启了`agent`之后，我们就可以使用`gops`强大的诊断功能了\n```sh\n$ gops <cmd> <pid|agent-addr>\n```\n\nCommands:\n- `stack`：查看进程栈信息\n- `gc`：手动触发一次`gc`，并阻塞等到`gc`结束\n- `setgc`：相当于设置`GOGC`值\n- `memstats`：查看进程的内存统计信息\n- `version`：查看构建二进制程序的go版本\n- `stats`：查看runtimes的统计信息，主要是goroutine数量和thread数量\n- `trace`：运行5s的 runtime tracer，并启动一个`http server`用于查看trace信息\n- `pprof-heap`：读取heap的profile并启动`go tool pprof`\n- `pprof-cpu`：读取cpu之后30s的profile并启动`go tool pprof`\n\n\n##### 查看内存统计\n```sh\n$ gops memstats 29573\nalloc: 196.65KB (201368 bytes) # 当前分配的堆对象字节数，同下面的`heap-alloc`\ntotal-alloc: 3.64MB (3819256 bytes) # 分配的堆对象字节数的累加统计\nsys: 68.69MB (72022264 bytes) # 从操作系统分配的内存的字节数，是下面所有`Xsys`的累加\nlookups: 0 # runtime执行的指针查找次数统计，主要用于runtime内部的debug\nmallocs: 2225 # 堆对象分配次数统计\nfrees: 1507 # 堆对象释放次数统计\n# 堆内存统计\nheap-alloc: 196.65KB (201368 bytes) # 当前分配的堆对象的字节数\nheap-sys: 63.56MB (66650112 bytes) # 从操作系统分配的堆内存的大小\nheap-idle: 62.80MB (65847296 bytes) # 空闲的堆内存大小（bytes in idle spans）\nheap-in-use: 784.00KB (802816 bytes) # 当前正在使用的堆内存大小（bytes in in-use spans）\nheap-released: 0 bytes # 归还操作系统的物理内存大小\nheap-objects: 718 # 当前分配的堆对象的数量\n# 下面是栈内存统计\n# Stacks are not considered part of the heap, but the runtime can reuse a span of heap memory for stack memory, and vice-versa.\nstack-in-use: 448.00KB (458752 bytes) # bytes in stack spans  \nstack-sys: 448.00KB (458752 bytes) # bytes of stack memory obtained from the OS\n# 下面主要是 runtime 内部数据结构分配的内存使用统计\nstack-mspan-inuse: 15.29KB (15656 bytes) \nstack-mspan-sys: 32.00KB (32768 bytes)\nstack-mcache-inuse: 6.75KB (6912 bytes)\nstack-mcache-sys: 16.00KB (16384 bytes)\nother-sys: 1.00MB (1049066 bytes) # runtime中一些杂项数据结构的内存占用\ngc-sys: 2.26MB (2371584 bytes) # 垃圾收集器元信息的内存占用\nnext-gc: when heap-alloc >= 4.00MB (4194304 bytes) # 下一次触发gc的时机\nlast-gc: 2019-08-29 16:42:26.502415917 +0800 CST # 上一次gc时间\ngc-pause-total: 946.946µs # gc总的STW的时间\ngc-pause: 134271 #  最后一次 gc 的STW时间，单位ns\nnum-gc: 9 # gc次数\nenable-gc: true\ndebug-gc: fals # 该字段当前未使用(go1.12.5)\n```\n**注意：读取内存统计信息的时候会触发`STW`**：\n```go\n// runtime/mstas.go\nfunc ReadMemStats(m *MemStats) {\n\tstopTheWorld(\"read mem stats\")\n\n\tsystemstack(func() {\n\t\treadmemstats_m(m)\n\t})\n\n\tstartTheWorld()\n}\n```\n\n##### 设置gc触发百分比\n默认`gc`的触发百分比是`100%`\n\n查看当前`gc`触发比例：\n```sh\n$ gops memstats 29573\n...\nnext-gc: when heap-alloc >= 4.00MB (4194304 bytes) \n...\n```\n当前的触发时机是，`heap-alloc`达到4MB\n\n更改`gc`触发比例：\n```sh\n$ gops setgc 29573 200\nNew GC percent set to 200. Previous value was 100.\n\n$ gops memstats 29573\n...\nnext-gc: when heap-alloc >= 8.00MB (8388608 bytes)\n...\n```\n`next-gc`的计算公式：\n```go\nnext_gc = heap_marked + heap_marked*uint64(gcpercent)/100\n```\n`heap_marked`表示在一次`gc`中，标记为存活的总对象大小，而`gcpercent`就是我们设置的`gc`触发比例，默认为`100`。\n\n**注意：比如`next_gc`为`8MB`，并不是真的等到总共分配了8MB的对象才触发GC**。`这个next_gc`实际上是`gc`后的`goal heap size`。现在的`gc`实现，对象标记与业务代码会并发执行，而在业务代码中还会申请分配新的对象。如果真的等到`next_gc`才触发`gc`，那么等到`gc`结束之后，当前的`heap size`可能会大于`next_gc`，因此实际上`gc`的触发会提前一点（有另一个字段`gc_trigger`来决定）。\n\n### gops的实现\n我们程序中的`agent`，会在指定端口监听`tcp`连接，通时默认在`~/.config/gops`目录下，以进程号为文件名创建一个记录监听端口的文件。\n当我们通过`gops`执行命令时，首先会读取该文件，获取监听的端口，然后与`agent`建立连接，发送事先定义好的命令：\n```go\nconst (\n  // StackTrace represents a command to print stack trace.\n  StackTrace = byte(0x1)\n\n  // GC runs the garbage collector.\n  GC = byte(0x2)\n\n  // MemStats reports memory stats.\n  MemStats = byte(0x3)\n\n  // Version prints the Go version.\n  Version = byte(0x4)\n\n  // HeapProfile starts `go tool pprof` with the current memory profile.\n  HeapProfile = byte(0x5)\n\n  // CPUProfile starts `go tool pprof` with the current CPU profile\n  CPUProfile = byte(0x6)\n\n  // Stats returns Go runtime statistics such as number of goroutines, GOMAXPROCS, and NumCPU.\n  Stats = byte(0x7)\n\n  // Trace starts the Go execution tracer, waits 5 seconds and launches the trace tool.\n  Trace = byte(0x8)\n\n  // BinaryDump returns running binary file.\n  BinaryDump = byte(0x9)\n\n  // SetGCPercent sets the garbage collection target percentage.\n  SetGCPercent = byte(0x10)\n)\n```\n`agent`收到`gops`的请求后，调用`runtime`的接口，并将结果返回。\n","tags":["go - 性能优化"]},{"title":"数据库事务隔离级别","url":"/2019/08/29/数据库事务隔离级别/","content":"\n### 数据库事务\n提到事务大家肯定都不陌生，最典型的例子就是银行转账。如果从账户A转100元到账户B，主要执行：\n1. 检查账户A的余额\n2. 如果余额足够，从账户A的余额扣除100元\n3. 账户B余额添加100元\n\n我们需要保证上面3步动作是不可分割的。否则，我们完全可以在步骤1和步骤2中间，再次发起另外一笔转账，这时候余额还没有扣除，检查仍然通过，这样银行不就乱套了吗？\n\n事务的概念：\n> 数据库事务通常包含了一个序列的对数据库的读/写操作。包含有以下两个目的：\n> - 为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n> - 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n并非任意的对数据库的操作序列都是数据库事务。数据库事务拥有以下四个特性，习惯上被称之为ACID特性：\n- 原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行\n- 一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束\n- 隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行\n- 持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中\n\n原子性、一致性和持久性都很好理解，我们今天就来说说是隔离性。\n\n### 事务的隔离\n当数据库上有多个事务并发执行的时候，就有可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题：\n- 脏读：读取到其他事务未提交的数据，因为这条数据之后可能会被回滚，导致我们当前读取到的是脏数据\n- 不可重复读：一个事务中前后读取同一条数据两次，并且在两次读取中间，有其他事务修改了这条记录并提交，导致前后两次读到的数据不一致\n- 幻读：一个事务前后对同一个条件进行两次搜索，并且两次搜索中间有其他事务提交了更新，导致前后两次查询得到了不同的结果集。一个简单的例子，事务A需要查询两次`id between 1 and 100`，第一次查询得到了10条数据，这时候另一个事务插入了一条`id`在100以内的数据，事务A再次查询就会得到11条数据，比前一次查询多了一条。\n\n数据不一致的问题实际上是并发编程很常见的问题，如果每个每个事务都串行执行，那么上面的问题就都不存在了。\n\n在解决并发时的数据不一致问题时，我们常常通过加锁来解决。但是，如果加锁粒度太大，并发就上不去。\n\nSQL标准根据并发事务可能出现的问题，定义了不同的事务隔离级别，隔离级别越高，串行化程度就越高，程序的并发也就越低，因此很多时候，我们需要在二者之间寻找一个平衡点。\n\n这些隔离级别从低到高分别是：\n- 读未提交（Read Uncommitted）：允许脏读，一个事务还没提交，它的变更就能被别的事务看见\n- 读已提交（Read Committed）：解决了脏读问题，一个事务只有提交之后，它的变更才能被别的事务看到\n- 可重复读（Repeatable Read）：解决了不可重复读问题，一个事务执行过程中看到的数据总是跟这个事务在启动时看到的数据是一致的。`mysql`通过`mvcc`提供快照读，两个事务的读写不会冲突。标准中只要求该级别解决不可重复读问题，但是 **`mysql`的可重复读还通过`gap`锁解决了幻读现象**。该级别是`mysql`的默认隔离级别。\n- 序列化（Serializable）：这是最高的隔离级别，也是SQL标准中的默认隔离级别。A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.\n\n上面的四种隔离级别，从上往下，隔离性越来越高，但是性能也越来越差。我们一般不会使用序列化隔离级别，该级别会导致大量的事务超时。`mysql`默认的隔离级别是`可重复读`级别，并且解决了`幻读`问题。有时候，当我们对数据的隔离性要求不高的时候，可以将其修改为`读已提交`级别。\n\n`mysql`中查看当前隔离级别：\n```sh\n$ mysql -p$MYSQL_ROOT_PASSWORD\nmysql> show variables like 'transaction_isolation'; \n+-----------------------+-----------------+\n| Variable_name         | Value           |\n+-----------------------+-----------------+\n| transaction_isolation | REPEATABLE-READ |\n+-----------------------+-----------------+\n1 row in set (0.01 sec)\n```\n\n设置隔离级别：\n```sh\nmysql> SET [global|session] transaction isolation LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE];\n```\n```sh\nmysql> SET session transaction isolation LEVEL READ COMMITTED; # 当前session级别\nmysql> SET global transaction isolation LEVEL READ COMMITTED; # 全局生效\n```\n\n","tags":["mysql"]},{"title":"gopark","url":"/2019/08/26/gopark/","content":"\n 在 go 的 runtime 包中，经常能够看到`gopark`方法，今天来看一下该函数的实现和作用。\n\n\n\n### 实现\n\n该方法的源码在`runtime/proc.go`：\n\n```go\n// Puts the current goroutine into a waiting state and calls unlockf.\n// If unlockf returns false, the goroutine is resumed.\n// unlockf must not access this G's stack, as it may be moved between\n// the call to gopark and the call to unlockf.\n// Reason explains why the goroutine has been parked.\n// It is displayed in stack traces and heap dumps.\n// Reasons should be unique and descriptive.\n// Do not re-use reasons, add new ones.\n\n// 根据上面的注释，该方法的作用是如果unlockf返回true，则将当前协程挂起，否则继续执行\n// \nfunc gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) {\n\tif reason != waitReasonSleep {\n\t\tcheckTimeouts() // timeouts may expire while two goroutines keep the scheduler busy\n\t}\n\tmp := acquirem() // 这里获取当前的m并加锁\n\tgp := mp.curg // gp即为当前运行的协程\n\tstatus := readgstatus(gp)\n    // 检查gp的状态\n\tif status != _Grunning && status != _Gscanrunning {\n\t\tthrow(\"gopark: bad g status\")\n\t}\n    // 将 lock 和 unlockf 分别保存到 mp 的 waitlock 和 waitunlockf\n    // unlockf 需要接收两个参数，一个是 gp，另一个就是参数 lock\n\tmp.waitlock = lock\n\tmp.waitunlockf = *(*unsafe.Pointer)(unsafe.Pointer(&unlockf))\n    // 等待的原因\n\tgp.waitreason = reason\n    // trace相关\n\tmp.waittraceev = traceEv\n\tmp.waittraceskip = traceskip\n\treleasem(mp) // 释放当前m\n\t// can't do anything that might move the G between Ms here.\n    // 这里mcall是一个由汇编实现的函数，接收一个函数作为参数，然后切换到 m 的 g0 栈去执行这个函数\n\tmcall(park_m) // 切换到 m 的 g0 栈执行 park_m 方法\n}\n```\n\n\n\n接下来，我们要先来看一下 `park_m` 这个方法的实现，该方法同样位于`runtime/proc.go`：\n\n```go\n// park continuation on g0.\n// mcall 方法会切换到 m 的 g0 运行，然后把原先的 g 作为参数传递给该方法\nfunc park_m(gp *g) {\n\t_g_ := getg() // 获取当前运行的 g，这时候返回的是 g0\n\t\n    // 如果开启了trace，则发送trace事件\n\tif trace.enabled {\n\t\ttraceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip)\n\t}\n\n    // cas更新gp的状态\n\tcasgstatus(gp, _Grunning, _Gwaiting)\n    // 这里将 gp 与 m 分离\n\tdropg()\n    \n\t// 这里的 waitunlockf 就是上面 gopark 方法的参数 unlockf\n    // 在 gopark 中分别将其两个参数 unlockf 和 lock 保存到 m 的 waitunlockf 和 waitlock字段中\n\tif _g_.m.waitunlockf != nil { // 如果 gopark 方法接收的 unlockf 不为空\n\t\tfn := *(*func(*g, unsafe.Pointer) bool)(unsafe.Pointer(&_g_.m.waitunlockf))\n\t\t// waitlock 就是 gopark 方法的另一个参数 lock\n        ok := fn(gp, _g_.m.waitlock)\n        // 及时清空 m 的两个这两个字段\n\t\t_g_.m.waitunlockf = nil \n\t\t_g_.m.waitlock = nil\n        // 如果 unlockf 返回了 false，那么不要挂起，继续执行\n\t\tif !ok {\n\t\t\tif trace.enabled {\n\t\t\t\ttraceGoUnpark(gp, 2)\n\t\t\t}\n            // 状态切换为 _Grunnable\n\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n            // 重新绑定 gp 到当前 m，并恢复执行，该方法不会返回\n\t\t\texecute(gp, true) // Schedule it back, never returns.\n\t\t}\n\t}\n    // unlockf 为 nil 或者返回 true，则挂起 gp，并重新调度新的 g 到当前 m\n\tschedule() // 协程调度，该方法也不会返回\n}\n```\n\n根据上面的 `park_m`的分析，我们知道 `mcall` 方法需要保存 `gp` 的上下文信息，并切换到 `g0` 栈，以 `gp` 作为参数调用 `park_m`。\n\n\n\n在看 `mcall` 方法的实现之前，我们先来看一下 `g` 中的 几个字段，`g`的实现在`runtime/runtime2.go`：\n\n```go\ntype g struct {\n\tstack       stack   // 当前g的栈信息\n \t...\n\tm              *m      // 当前g绑定的m\n\tsched          gobuf   // 保存g的运行上下文信息，下次调度时从哪里开始执行\n\t... \n}\n\ntype stack struct {\n\tlo uintptr // 栈底\n\thi uintptr // 栈顶\n}\n\ntype gobuf struct {\n\tsp   uintptr \t // 栈顶\n\tpc   uintptr\t // pc，恢复点\n\tg    guintptr    // 对应的g指针值\n\tctxt unsafe.Pointer\n\tret  sys.Uintreg\n\tlr   uintptr\n\tbp   uintptr     // 栈底\n}\n```\n\n\n\n接下来我们来看 `mcall` 在 `amd64` 体系下的实现，源代码位于 `runtime/asm_amd64.s`：\n\n```assembly\n// func mcall(fn func(*g))\n// Switch to m->g0's stack, call fn(g).\n// Fn must never return. It should gogo(&g->sched)\n// to keep running g.\n// \n// 根据注释，mcall 切换到 g0 栈调用 fn(g), fn 不能返回\nTEXT runtime·mcall(SB), NOSPLIT, $0-8 // 栈帧大小为0，这种情况0(SP)保存的为返回地址\n\t// 关于go的函数调用栈的布局，可以参考 http://mcll.top/2019/04/29/go函数栈布局\n\t\n\tMOVQ\tfn+0(FP), DI // 将mcall的参数保存到DI寄存器，实际上是一个funcval指针\n\t// 关于funcval的介绍，可以参考http://mcll.top/2019/03/06/go中的猴子补丁\n\n\tget_tls(CX)  // 将TLS保存到CX寄存器，TLS是一个伪寄存器，get_tls是一个宏，一直没找到其定义😓\n\tMOVQ\tg(CX), AX\t// 保存当前g到AX寄存器中\n\tMOVQ\t0(SP), BX\t// 0(SP)保存返回地址，也就是mcall调用者的PC\n\tMOVQ\tBX, (g_sched+gobuf_pc)(AX) // 保存到g.sched.pc字段中\n\tLEAQ\tfn+0(FP), BX\t// fn+0(FP)也就是调用者的SP\n\tMOVQ\tBX, (g_sched+gobuf_sp)(AX) // 保存到g.sched.sp字段中\n\tMOVQ\tAX, (g_sched+gobuf_g)(AX) // 保存当前g到g.sched.g中\n\tMOVQ\tBP, (g_sched+gobuf_bp)(AX) // 因为栈帧为0，BP就是调用者的BP，保存到g.sched.bp\n\n\t// switch to m->g0 & its stack, call fn\n\tMOVQ\tg(CX), BX  // 将原来的g保存到BX\n\tMOVQ\tg_m(BX), BX  // 获取g.m，还是保存到BX\n\tMOVQ\tm_g0(BX), SI // 保存g0到SI\n\tCMPQ\tSI, AX\t// AX寄存器上面已经设置为原来的g了，禁止在g0栈上调用mcall，这里要判断一下\n\tJNE\t3(PC) // 如果原来的g不是g0，跳转到pc+3的位置执行，也就是从这里往下第三条指令\n\tMOVQ\t$runtime·badmcall(SB), AX // 如果在g0上调用mcall，直接panic\n\tJMP\tAX // 跳转到 badmcall 方法，最终会panic\n\tMOVQ\tSI, g(CX)\t// 将g0设置到TLS中\n\tMOVQ\t(g_sched+gobuf_sp)(SI), SP\t// 恢复g0的SP寄存器\n\tPUSHQ\tAX\t// AX保存原来的g，入栈\n\tMOVQ\tDI, DX // 上面说过，DI保存了要调用函数的funcval值，将其保存的DX寄存器，DX寄存器与闭包实现有关\n\tMOVQ\t0(DI), DI // 将要调用函数的入口地址保存到DI寄存器\n\tCALL\tDI // 调用该函数，参数就是刚刚入栈的AX中的值，也就是原来的g，该函数禁止返回\n\tPOPQ\tAX // 出栈\n\tMOVQ\t$runtime·badmcall2(SB), AX // 如果调用函数返回了，panic\n\tJMP\tAX \n\tRET\n```\n\n\n\n### 使用\n\n`runtime`包中有很多用到`gopark`方法的地方，这里举例几个\n\n\n\n##### 场景一：写空 channel\n\n```go\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {\n\t// 根据go语义，向一个nil的channel会导致阻塞\n    if c == nil {\n     \t// 一般block都是true\n\t\tif !block {\n\t\t\treturn false\n\t\t}\n        \n        // 根据上面的分析，unlockf为nil会挂起当前协程\n\t\tgopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2)\n\t\tthrow(\"unreachable\")\n\t}\n\n    ...\n}\n```\n\n\n\n##### 场景二：select\n\n`selectgo`实现了`select`语句的功能\n\n```go\nfunc selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) {\n\t...\n    // selparkcommit会释放所有case的锁，并阻塞等待当前g，等待有case被触发\n    gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1)\n    ...\n}\n```\n\n","tags":["go - runtime"]},{"title":"系统平均负载","url":"/2019/08/25/load-average/","content":"\n当我们发现系统变慢时，通常会执行`top`或者`uptime`命令，来了解系统的负载情况，比如：\n\n```sh\n$ uptime\n 07:48:28 up 8 min,  1 user,  load average: 0.00, 0.08, 0.07\n```\n\n`uptime`命令输出的信息分别是：当前系统时间，系统运行时间，当前登陆用户数，以及**最近1分钟、最近5分钟和最近15分钟的系统评价负载情况**\n\n\n\n### 什么是系统平均负载？\n\n我们可以通过`man uptime`查看命令帮助手册，有这么一段话介绍什么是系统的平均负载：\n\n> ​    System load averages is the average number of processes that are either in a runnable or uninterruptable state.  A process in a runnable state is either using the CPU or waiting to use  the  CPU.   A process  in  uninterruptable state is waiting for some I/O access, eg waiting for disk.  The averages are taken over the three time intervals.  Load averages are not normalized for the number of CPUs in a system, so a load average of 1 means a single CPU system is loaded all the time while on a 4 CPU system it means it was idle 75% of the time.\n\n简单来说，平均负载就是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数，也就是**平均活跃进程数**。这里的**可运行状态**包括**正在使用CPU（Running状态）**和**正在等待CPU（Runnable状态）**。而**不可中断状态**是指进程运行在内核态，正在等待`I/O`，**为了能够尽快完成`I/O`操作**，等待`I/O`的进程不会被中断。\n\n通过`ps`命令查看进程时，可运行状态对应的是`R`，而不可中断状态对应的是`D`。\n\n系统平均负载不会根据 CPU 数量进行标准化，也就是说，当系统平均负载是1的时候：\n\n- 在单核系统上，说明 CPU 满跑\n- 而在4核系统上，说明 CPU 有75%的空闲\n\n\n\n### 平均负载为多少合适？\n\n理想的情况下，是每个 CPU 都刚好运行着一个进程，这样每个 CPU 都能得到充分的利用，因此**理想情况下平均负载应该等于系统 CPU 的个数**。\n\n当我们在评判系统负载情况的时候，应该先要知道系统有几个CPU：\n\n```sh\n$ grep 'model name' /proc/cpuinfo| wc -l\n```\n\n`/proc/cpuinfo`文件内会记录系统 CPU 的信息，我们可以直接通过统计该文件信息获取 CPU 个数。\n\n通过与 CPU 个数对比，我们就可以知道当前系统的负载情况是否过载。\n\n平均负载的值有三个，分别对应最近1分钟，最近5分钟和最近15分钟，我们可以根据这三个值来更全面的了解系统的负载情况：\n\n- 如果这三个值基本相差不大，那就说明系统负载比较平稳\n- 如果最近1分钟的负载远小于15分钟的负载，说明过去15分钟负载很大，而现在在慢慢减小\n- 如果最近1分钟负载很大，而15分钟的很小，说明最近1分钟的负载在逐渐增加，这种增加可能是临时性的，也有可能会持续增加，需要持续观察\n\n**在生产环境中，当平均负载高于CPU数量的70%时，就应该分析排查负载高的问题了**。一旦负载过高，就会导致进程响应变慢，进而影响服务正常功能。\n\n\n\n### 平均负载与CPU使用率\n\nCPU使用率是单位时间内对CPU繁忙情况的统计。根据平均负载的含义，我们知道这两者是两个不同的概念。\n\n平均负载与CPU使用率两者之间也不一定是对应的：\n\n- CPU密集型进程：会使用大量的CPU，这时候两者一般是一致的\n- IO密集型进程：等待IO会导致平均负载升高，但是这时候进程是处于等待状态的，因此这时候的CPU使用率不一定高\n- 大量进程等待CPU调度：这时候的平均负载会很高，而CPU使用率一般也会比较高\n\n当我们发现系统的平均负载很高时，首先就需要分析是由于上面哪种原因导致的，这样才能有效的解决问题。\n\n\n\n### 案例模拟分析\n\n接下来我们来模拟一下平均负载过高的案例分析。\n\n首先要先安装一下`stress-ng`和`sysstat`两个包。其中`stress-ng`是压力测试工具，用来模拟平均负载过高的场景，而`sysstat`包含了常用的性能工具，用来监控和分析系统的性能，包括：\n\n- `mpstat`：多核cpu性能工具，用来实时查看每个cpu的性能指标，以及所有cpu的平均指标\n- `pidstat`：进程性能分析工具，用来实时查看进程的cpu、内存、i/o以及上下文切换等性能指标\n\n\n\n##### 场景一：CPU密集型进程\n\n首先，在第一个终端运行：\n\n```sh\n$ stress-ng --cpu 2 --timeout 600\n```\n\n- `-- cpu 2`：使用2个worker去不断执行cpu密集的任务，这里使用2是因为我本地测试的虚拟机只有两个cpu\n- `--timeout 600`：表示持续运行600s\n\n接着在第二个终端运行：\n\n```sh\n$ watch -d uptime # -d 会高亮显示变化的区域\n..., load average: 2.01, 1.44, 0.71\n```\n\n我们可以在这个终端看到，平均负载会逐渐升高，最后最近1分钟的平均负载会稳定在2左右\n\n最后，在第三个终端运行：\n\n```sh\n$ mpstat -P ALL 5 20 # -P All 表示统计所有cpu信息，5表示间隔5s打印一次统计，20表示共打印20组统计\n...\n09:03:00 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\n09:03:05 AM  all   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00\n09:03:05 AM    0   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00\n09:03:05 AM    1   99.80    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00\n\nAverage:     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\nAverage:     all   99.71    0.00    0.28    0.00    0.00    0.01    0.00    0.00    0.00    0.00\nAverage:       0   99.78    0.00    0.22    0.00    0.00    0.00    0.00    0.00    0.00    0.00\nAverage:       1   99.64    0.00    0.34    0.00    0.00    0.02    0.00    0.00    0.00    0.00\n```\n\n我们可以看到，两个cpu的用户使用率都接近100%，说明是因为cpu密集进程导致的平均负载过高\n\n接下来，我们可以使用`pidstat`命令来查询异常进程：\n\n```sh\n$ pidstat -u 5 2 # -u输出cpu使用率，间隔5s打印一次统计信息，共打印2组\nLinux 4.4.0-159-generic (ubuntu-xenial) \t08/25/2019 \t_x86_64_\t(2 CPU)\n\n09:11:59 AM   UID       PID    %usr %system  %guest    %CPU   CPU  Command\n09:12:04 AM     0      5336   99.20    0.00    0.00   99.20     1  stress\n09:12:04 AM     0      5337   99.60    0.00    0.00   99.60     0  stress\n09:12:04 AM  1000      5344    0.20    0.20    0.00    0.40     1  watch\n09:12:04 AM  1000      5538    0.00    0.20    0.00    0.20     1  pidstat\n\n09:12:04 AM   UID       PID    %usr %system  %guest    %CPU   CPU  Command\n09:12:09 AM     0      1112    0.00    0.20    0.00    0.20     1  iscsid\n09:12:09 AM     0      1179    0.20    0.00    0.00    0.20     0  containerd\n09:12:09 AM     0      5336   99.60    0.00    0.00   99.60     1  stress\n09:12:09 AM     0      5337  100.00    0.20    0.00  100.20     0  stress\n09:12:09 AM  1000      5344    0.20    0.00    0.00    0.20     1  watch\n\nAverage:      UID       PID    %usr %system  %guest    %CPU   CPU  Command\nAverage:        0      1112    0.00    0.10    0.00    0.10     -  iscsid\nAverage:        0      1179    0.10    0.00    0.00    0.10     -  containerd\nAverage:        0      5336   99.40    0.00    0.00   99.40     -  stress-ng-cpu\nAverage:        0      5337   99.80    0.10    0.00   99.90     -  stress-ng-cpu\nAverage:     1000      5344    0.20    0.10    0.00    0.30     -  watch\nAverage:     1000      5538    0.00    0.10    0.00    0.10     -  pidstat\n```\n\n从上面可以看到，大量占用cpu使用率的进程是`stress-ng`命令\n\n\n\n##### 场景二：io密集型进程\n\n首先在第一个终端执行命令：\n\n```sh\n$ stress-ng --hdd 2 --timeout 600\n```\n\n- `--hdd 2`：表示使用两个`worker`不断的读写临时文件\n- `--timeout`：表示持续运行600s\n\n\n\n接着，在第二个终端运行：\n\n```sh\n$ watch -d uptime\n..., load average: 2.79, 1.80, 1.16\n```\n\n可以看到，平均负载不断上升，最终文档在2.7~2.8之间\n\n最后，在第三个终端运行：\n\n```sh\n$ mpstat -P ALL 5 1\nLinux 4.4.0-159-generic (ubuntu-xenial) \t08/25/2019 \t_x86_64_\t(2 CPU)\n\n09:30:45 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\n09:30:50 AM  all    0.11    0.00    5.09   41.86    0.00    0.11    0.00    0.00    0.00   52.82\n09:30:50 AM    0    0.21    0.00    1.47   21.85    0.00    0.21    0.00    0.00    0.00   76.26\n09:30:50 AM    1    0.23    0.00    9.09   63.87    0.00    0.23    0.00    0.00    0.00   26.57\n\nAverage:     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\nAverage:     all    0.11    0.00    5.09   41.86    0.00    0.11    0.00    0.00    0.00   52.82\nAverage:       0    0.21    0.00    1.47   21.85    0.00    0.21    0.00    0.00    0.00   76.26\nAverage:       1    0.23    0.00    9.09   63.87    0.00    0.23    0.00    0.00    0.00   26.57\n```\n\n> %iowait: the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.\n>\n> %idle: the percentage of time that the CPU or CPUs were idle and the system did not have an outstanding disk I/O request.\n\n从输出结果我们可以看到，尽管系统的平均负载很高，但是cpu使用率并不高，而iowait却很高，可以判断是由于io密集导致的系统负载过高。\n\n\n\n接下来，我们使用`pidstat`来查找出导致过载的进程：\n\n```sh\n$ pidstat -d # -d输出io统计信息\nLinux 4.4.0-159-generic (ubuntu-xenial) \t08/25/2019 \t_x86_64_\t(2 CPU)\n\n09:36:31 AM   UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n...\n09:36:31 AM     0      6564      0.01      0.00      0.00       0  stress-ng\n09:36:31 AM     0      6565   4394.88   8064.26      0.00   38334  stress-ng-hdd\n09:36:31 AM     0      6566   4486.84   6272.20      0.00   39617  stress-ng-hdd\n...\n```\n\n\n\n##### 场景三：大量进程场景\n\n首先在第一个终端运行：\n\n```sh\n$ stress-ng -c 8 --timeout 600 # -c 8 表示开启8个进程\n```\n\n\n\n然后在第二个终端运行：\n\n```sh\n$ watch -d uptime\n..., load average: 8.03, 5.47, 1.72\n```\n\n可以观察到，经过一段时间之后，系统负载上升到8.0左右，远远高于正常系统负载，我们使用`pidstat`看一下进程情况：\n\n```sh\n$ pidstat -u 3 1\nLinux 4.4.0-159-generic (ubuntu-xenial) \t08/25/2019 \t_x86_64_\t(2 CPU)\n\n10:11:03 AM   UID       PID    %usr %system  %guest    %CPU   CPU  Command\n10:11:07 AM     0      7489   24.75    0.00    0.00   24.75     0  stress-ng-cpu\n10:11:07 AM     0      7490   24.75    0.00    0.00   24.75     0  stress-ng-cpu\n10:11:07 AM     0      7491   24.75    0.00    0.00   24.75     1  stress-ng-cpu\n10:11:07 AM     0      7492   24.75    0.00    0.00   24.75     1  stress-ng-cpu\n10:11:07 AM     0      7493   24.75    0.00    0.00   24.75     0  stress-ng-cpu\n10:11:07 AM     0      7494   24.75    0.00    0.00   24.75     1  stress-ng-cpu\n10:11:07 AM     0      7495   25.08    0.00    0.00   25.08     0  stress-ng-cpu\n10:11:07 AM     0      7496   25.08    0.00    0.00   25.08     1  stress-ng-cpu\n\nAverage:      UID       PID    %usr %system  %guest    %CPU   CPU  Command\nAverage:        0      7489   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7490   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7491   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7492   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7493   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7494   24.75    0.00    0.00   24.75     -  stress-ng-cpu\nAverage:        0      7495   25.08    0.00    0.00   25.08     -  stress-ng-cpu\nAverage:        0      7496   25.08    0.00    0.00   25.08     -  stress-ng-cpu\n```\n\n我们看到有8个进程在争夺系统的两个cpu，平均每个进程占用25%的cpu使用率。\n\n\n\n### 总结\n\n平均负载提供了一个快速查看系统整体性能的手段，反应了系统整体的负载情况，但是只看平均负载本身并不能直接发现系统瓶颈。\n\n- 平均负载高有可能是cpu密集型进程导致的\n- 平均负载高不一定代表cpu使用率高，有可能是I/O繁忙导致的\n- 当发现平均负载过高时，可以使用`mpstat`和`pidstat`等工具辅助分析\n\n\n\n### 参考\n\n- [极客时间专栏：Linux性能优化实践](<https://time.geekbang.org/column/intro/140>)","tags":["linux - 性能优化"]},{"title":"gctrace","url":"/2019/08/23/gctrace/","content":"\n在优化`go`程序时，内存优化是其中一项很重要的内容，减轻`gc`的压力，能够极大的优化我们的程序运行效率。\n\n今天先来看一下两个与`gc`相关的环境变量：`gctrace`和`GOGC`\n\n### gctrace\n\n`gctrace`本身是[`GODEBUG`](<https://golang.org/pkg/runtime/#hdr-Environment_Variables>)这个环境变量中的一个选项，用来开启`gc`日志的。每次当完成一次`gc`扫描时，就会打印出本次`gc`的相关信息，我们可以用来监控程序的内存情况\n\n```sh\n$ GODEBUG=gctrace=1 GO_BIN # 开启gctrace\ngc 1 @1.569s 0%: 0.28+2.2+4.6 ms clock, 1.1+0/0.96/4.6+18 ms cpu, 4->4->1 MB, 5 MB goal, 4 P\ngc 2 @3.124s 0%: 0.026+1.4+0.058 ms clock, 0.10+1.3/0.033/1.5+0.23 ms cpu, 4->4->1 MB, 5 MB goal, 4 P\ngc 3 @5.326s 0%: 0.58+0.53+0.12 ms clock, 2.3+0.36/0.44/0.30+0.48 ms cpu, 4->4->1 MB, 5 MB goal, 4 P\n...\nscvg1: inuse: 4, idle: 58, sys: 63, released: 0, consumed: 63 (MB)\n...\n```\n\n每一行对应一次`gc`，具体的输出格式如下：\n\n```\ngc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #->#-># MB, # MB goal, # P [(forced)]\n```\n\n- `gc #`：第几轮gc，从1开始递增\n\n- `@#s`：程序总的运行时间，单位`s`\n- `#%`：从程序开始到现在运行`gc`的时间占比\n- `#+#+# ms clock`：对应 <font color=red>第一次`STW`，终止`SWEEP`、开启写屏障</font> +  <font color=red>并发`Mark`和`Scan`</font> + <font color=red>第二次`STW`，结束`Mark`</font> 这三个阶段`wall-clock`的耗时，单位为`ms`\n- `#+#/#/#+# ms cpu`：对应 <font color=red>第一次`STW`</font> + <font color=red>并发标记：`Assist Time `</font>/<font color=red> 并发标记：`Background GC time`</font> /<font color=red> 并发标记：`Idle GC time`</font> + <font color=red>第二次`STW`结束`Mark`</font> 这几个阶段的`cpu`时间，单位`ms`\n- `#->#-># MB `：分别对应`gc`开始时的堆大小、`gc`结束时的堆大小以及`live heap`（`gc mark`阶段标记为黑色的内存总量）的大小\n\n- `# MB goal`：目标`heap size`\n- `# P`：使用的`P`数量\n- `(forced)`：如果调用`runtime.GC()`强制触发`gc`\n\n除了输出`gc`的信息，当`runtime`向操作系统归还内存时，也会打印出信息，比如上面的：\n\n```\nscvg1: inuse: 4, idle: 58, sys: 63, released: 0, consumed: 63 (MB)\n```\n\n- `scvg#`：第几次归还，从1开始计数\n- `inuse: #`：正在使用或部分被使用的`spans`的内存大小，单位`MB`\n- `idle: #`：空闲等待归还给操作系统的`spans`的内存大小，单位`MB`；\n- `sys: #`：从操作系统映射的内存大小，实际上是`gc`堆可访问的内存虚拟地址空间，单位`MB`\n- `released: #`：本次归还给操作系统的内存大小，单位`MB`\n- `consumed: #`：从操作系统分配的内存大小，等于`sys - released`\n\n\n\n### GOGC\n\n根据`runtime/mgc.go`中的注释：\n\n>  Next GC is after we've allocated an extra amount of memory proportional to\n>  the amount already in use. The proportion is controlled by GOGC environment variable\n>  (100 by default). If GOGC=100 and we're using 4M, we'll GC again when we get to 8M\n>  (this mark is tracked in next_gc variable). This keeps the GC cost in linear\n>  proportion to the allocation cost. Adjusting GOGC just changes the linear constant\n>  (and also the amount of extra memory used).\n\n在`go1.5`之前，运行`gc mark`阶段会`stop the world`， 能够根据`next_gc`变量（也就是**goal heap size**，可以直接通过`GOGC`变量调整）精确地控制堆内存的增长：\n\n![](/img/gc_stw.jpg)\n\n但是`go1.5`之后，`gc mark`可以跟用户协程并发运行，因此在`gc`执行过程中仍然会有新的内存被分配，因此`gc`的触发点需要相对`next_gc`提前：\n\n![](/img/gc_bg.jpg)\n\n如上图所示，`Hm(n-1)`表示上一次`gc`结束后的堆大小,而`Hg`是`next_gc`，而我们在`Ht`触发`gc`，因为gc过程中可能会有新的内存分配，当`gc`结束时，当前的堆大小为`Ha`。`go`的`gc`实现，需要提供一种动态调整的机制，根据内存分配情况调整`Ht`的值，使得`Ha`能够与`Hg`尽量接近。\n\n\n\n总体来说，我们可以通过设置`GOGC`的值来调整`gc`的触发阈值：\n\n- **当小于零或者等于`off`时，将会关闭`gc`**\n- 设置较大的值：减少`gc`触发，但是会增加内存占用\n- 设置偏小的值：频繁触发`gc`\n\n","tags":["go"]},{"title":"grpc client端分析1","url":"/2019/07/29/grpc-client端分析1/","content":"\n### grpc客户端连接创建\n\n`grpc`本身提供了服务发现和负载均衡的接口，当需要创建`grpc`连接时，就会使用到这些接口。\n\n我们先来看一下创建`grpc`连接时的主要流程：\n\n![](/img/grpc_client1.png)\n\n![](/img/grpc_client2.png)\n\n\n\n##### 服务发现：Resolver\n\n相关接口声明在`resolver/resolver.go`中\n\n```go\n// scheme://authority/endpoint\ntype Target struct {\n\tScheme    string\n\tAuthority string\n\tEndpoint  string\n}\n\n// 向grpc注册服务发现实现时，实际上注册的是Builder\ntype Builder interface {\n    // 创建Resolver，当resolver发现服务列表更新，需要通过ClientConn接口通知上层\n\tBuild(target Target, cc ClientConn, opts BuildOption) (Resolver, error)\n\tScheme() string\n}\n\ntype Resolver interface {\n    // 当有连接被出现异常时，会触发该方法，因为这时候可能是有服务实例挂了，需要立即实现一次服务发现\n\tResolveNow(ResolveNowOption)\n\tClose()\n}\n\n//\ntype ClientConn interface {\n\t// 服务列表和服务配置更新回调接口\n\tUpdateState(State)\n\t// 服务列表更新通知接口\n\tNewAddress(addresses []Address)\n \t// 服务配置更新通知接口\n\tNewServiceConfig(serviceConfig string)\n}\n```\n\n其中`Builder`接口用来创建`Resolver`，我们可以提供自己的服务发现实现，然后将其注册到`grpc`中，其中通过`scheme`来标识，而`Resolver`接口则是提供服务发现功能。当`resover`发现服务列表发生变更时，会通过`ClientConn`回调接口通知上层。\n\n当我们使用`Dial`或者`DialContext`接口创建grpc的客户端连接时，首先会解析参数`target`，然后创建对应的`resolver`：\n\n```go\nfunc DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {\n\tcc := &ClientConn{\n\t\t...\n\t}\n\t\n    ... \n    \n\t// resolverBuilder，用于解析target为目标服务列表\n\t// 如果没有指定resolverBuilder\n\tif cc.dopts.resolverBuilder == nil {\n\t\t// 解析target，根据target的scheme获取对应的resolver\n\t\tcc.parsedTarget = parseTarget(cc.target)\n \t\tcc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)\n\t\t// 如果scheme没有注册对应的resolver\n\t\tif cc.dopts.resolverBuilder == nil {\n            // 使用默认的resolver\n\t\t\tcc.parsedTarget = resolver.Target{\n\t\t\t\tEndpoint: target, // 这时候参数target就是endpoint，passthrough的实现就是直接返回endpoint，即不使用服务发现功能，参数Dial传进来的地址就是grpc server的地址\n\t\t\t}\n            // 获取默认的resolver，也就是passthrough\n\t\t\tcc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)\n\t\t}\n\t} else {\n        // 如果Dial的option中手动指定了需要使用的resolver，那么endpoint也是target\n\t\tcc.parsedTarget = resolver.Target{Endpoint: target}\n\t}\n    \n\t... \n    \n\t// newCCResolverWrapper方法内调用builder的Build接口创建resolver\n\trWrapper, err := newCCResolverWrapper(cc)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to build resolver: %v\", err)\n\t}\n\n\tcc.mu.Lock()\n\tcc.resolverWrapper = rWrapper\n\tcc.mu.Unlock()\n    \n \t... \n\n\treturn cc, nil\n}\n\n// 有效的target：scheme://authority/endpoint\nfunc parseTarget(target string) (ret resolver.Target) {\n\tvar ok bool\n\tret.Scheme, ret.Endpoint, ok = split2(target, \"://\")\n\tif !ok {\n        // 如果没有scheme，则整个target作为endpoint\n\t\treturn resolver.Target{Endpoint: target}\n\t}\n    // 如果指定了sheme，那么必须有`/`，分割authorigy和endpoint\n    // 当不需要指定authorigy，比如使用dnsResolver时:`dns:///www.demo.com`\n\tret.Authority, ret.Endpoint, ok = split2(ret.Endpoint, \"/\")\n\tif !ok {\n\t\treturn resolver.Target{Endpoint: target}\n\t}\n\treturn ret\n}\n\nfunc newCCResolverWrapper(cc *ClientConn) (*ccResolverWrapper, error) {\n\t// 在DialContext方法中，已经初始化了resolverBuilder\n    rb := cc.dopts.resolverBuilder\n\tif rb == nil {\n\t\treturn nil, fmt.Errorf(\"could not get resolver for scheme: %q\", cc.parsedTarget.Scheme)\n\t}\n\n \t// ccResolverWrapper实现resolver.ClientConn接口，用于提供服务列表变更之后的通知回调接口\n\tccr := &ccResolverWrapper{\n\t\tcc:     cc,\n\t\taddrCh: make(chan []resolver.Address, 1),\n\t\tscCh:   make(chan string, 1),\n\t}\n\n\tvar err error\n\t// 创建resovler，resovler创建之后，需要立即执行服务发现逻辑，然后将发现的服务列表通过resolver.ClientConn回调接口通知上层\n\tccr.resolver, err = rb.Build(cc.parsedTarget, ccr, resolver.BuildOption{DisableServiceConfig: cc.dopts.disableServiceConfig})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn ccr, nil\n}\n```\n\n\n\n##### Balancer：负载均衡接口\n\n相关接口声明在`balancer/balancer.go`文件中：\n\n```go\n// 声明了balancer需要用到的回调接口\ntype ClientConn interface {\n  \t// 根据地址创建网路连接\n\tNewSubConn([]resolver.Address, NewSubConnOptions) (SubConn, error)\n    // 移除无效网络连接\n\tRemoveSubConn(SubConn)\n    // 更新Picker，Picker用于在执行rpc调用时执行负载均衡策略，选举一条连接发送请求\n\tUpdateBalancerState(s connectivity.State, p Picker)\n    // 立即触发服务发现\n\tResolveNow(resolver.ResolveNowOption)\n\tTarget() string\n}\n\n// 根据当前的连接列表，执行负载均衡策略选举一条连接发送rpc请求\ntype Picker interface {\n\tPick(ctx context.Context, opts PickOptions) (conn SubConn, done func(DoneInfo), err error)\n}\n\n// Builder用于创建Balancer，注册的时候也是注册builder\ntype Builder interface {\n\tBuild(cc ClientConn, opts BuildOptions) Balancer\n\tName() string\n}\n\ntype Balancer interface {\n    // 当有连接状态变更时，回调\n\tHandleSubConnStateChange(sc SubConn, state connectivity.State)\n    // 当resolver发现新的服务地址列表时调用（有可能地址列表并没有真的更新）\n\tHandleResolvedAddrs([]resolver.Address, error)\n\tClose()\n}\n```\n\n当`Resolver`发现新的服务列表时，最终会调用`Balancer`的`HandleResolvedAddrs`方法进行通知；`Balancer`通过`ClientConn`的接口创建网络连接，然后根据当前的网络连接连接构造新的`Picker`，然后回调`ClientConn.UpdateBalancerState`更新`Picker`。当发送`grpc`请求时，会先执行`Picker`的接口，根据具体的负载均衡策略选举一条网络连接，然后发送`rpc`请求。\n\n当`resolver`发现新的服务列表之后，同通过`NewAddress`回调通知：\n\n```go\n// ccResolverWrapper是resolver.ClientConn的实现\nfunc (ccr *ccResolverWrapper) NewAddress(addrs []resolver.Address) {\n\tif ccr.isDone() {\n\t\treturn\n\t}\n\t\n\tccr.curState.Addresses = addrs\n\tccr.cc.updateResolverState(ccr.curState)\n}\n\n// 更新ClientConn的地址和ServiceConfig\nfunc (cc *ClientConn) updateResolverState(s resolver.State) error {\n\tcc.mu.Lock()\n\tdefer cc.mu.Unlock()\n\t\n    // ClientConn已经close\n\tif cc.conns == nil {\n\t\treturn nil\n\t}\n\n\t...\n\n\t// 负载均衡器变更\n\tvar balCfg serviceconfig.LoadBalancingConfig\n\t// 如果调用Dial时没有手动指定要使用的LoadBalancer\n\tif cc.dopts.balancerBuilder == nil {\n\t\tvar newBalancerName string\n\t\t// 如果serviceConfig中指定了负载均衡器配置\n\t\tif cc.sc != nil && cc.sc.lbConfig != nil {\n\t\t\tnewBalancerName = cc.sc.lbConfig.name\n\t\t\tbalCfg = cc.sc.lbConfig.cfg\n\t\t} else {\n\t\t\tvar isGRPCLB bool\n            // 判断是否存在grpclb类型的地址\n\t\t\tfor _, a := range s.Addresses {\n\t\t\t\tif a.Type == resolver.GRPCLB {\n\t\t\t\t\tisGRPCLB = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\t// 存在grpclb类型的addr，使用grpclb负载均衡器\n\t\t\tif isGRPCLB {\n\t\t\t\tnewBalancerName = grpclbName\n                // 如果配置中指定了负载均衡器\n\t\t\t} else if cc.sc != nil && cc.sc.LB != nil { \n\t\t\t\tnewBalancerName = *cc.sc.LB\n\t\t\t} else {\n                // 默认使用PickFirst负载均衡器，每次都使用第一条连接\n\t\t\t\tnewBalancerName = PickFirstBalancerName \n\t\t\t}\n\t\t}\n\t\t// 使用新的负载均衡器\n\t\tcc.switchBalancer(newBalancerName)\n\t} else if cc.balancerWrapper == nil { // options指定了balancerBuilder但是还没有初始化\n\t\t// 初始化balancer\n\t\tcc.curBalancerName = cc.dopts.balancerBuilder.Name()\n\t\tcc.balancerWrapper = newCCBalancerWrapper(cc, cc.dopts.balancerBuilder, cc.balancerBuildOpts)\n\t}\n\n\t// 通知Balancer服务列表变更了\ncc.balancerWrapper.updateClientConnState(&balancer.ClientConnState{ResolverState: s, BalancerConfig: balCfg})\n\treturn nil\n}\n```\n\n接下来，看一下`Balancer`接收到新的服务列表之后的执行逻辑：\n\n```go\n// baseBalancer可以看成是Balancer接口实现的基类，当要实现自己的负载均衡策略时只需要在器基础上实现Picker接口\nfunc (b *baseBalancer) UpdateClientConnState(s balancer.ClientConnState) {\n    // 记录新的服务列表\n    addrsSet := make(map[resolver.Address]struct{})\n\t// 为新的地址创建连接\n\tfor _, a := range s.ResolverState.Addresses {\n\t\taddrsSet[a] = struct{}{}\n        // 如果该地址之前不存在\n\t\tif _, ok := b.subConns[a]; !ok {\n            // 创建连接\n\t\t\tsc, err := b.cc.NewSubConn([]resolver.Address{a}, balancer.NewSubConnOptions{HealthCheckEnabled: b.config.HealthCheck})\n\t\t\tif err != nil {\n\t\t\t\tgrpclog.Warningf(\"base.baseBalancer: failed to create new SubConn: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n            // 保存到subConns\n\t\t\tb.subConns[a] = sc\n            // 设置初始状态\n            // 连接状态有：\n            //     - IDLE: 未连接\n            //     - CONNECTING: 连接中\n            //     - READY: 已经连接，可用\n            //     - TRANSIENT_FAILURE: 连接异常\n            //     - SHUTDOWN: 连接关闭\n\t\t\tb.scStates[sc] = connectivity.Idle\n            // 开始连接，更新状态为CONNECTING，然后异步执行连接逻辑\n\t\t\tsc.Connect()\n\t\t}\n\t}\n\t// 移除无效addr\n\tfor a, sc := range b.subConns {\n\t\t// 如果不在新的连接列表中，则需要移除\n\t\tif _, ok := addrsSet[a]; !ok {\n            // 更新状态为SHUTDOWN，并关闭连接\n\t\t\tb.cc.RemoveSubConn(sc)\n\t\t\tdelete(b.subConns, a)\n\t\t}\n\t}\n}\n```\n\n接下来我们看一下`Connect`的逻辑：\n\n```go\nfunc (ac *addrConn) connect() error {\n\tac.mu.Lock()\n\t// 如果连接已经被移除\n\tif ac.state == connectivity.Shutdown {\n\t\tac.mu.Unlock()\n\t\treturn errConnClosing\n\t}\n    \n\t// 如果状态不是Idle，表示已经执行过connect方法，直接返回\n\tif ac.state != connectivity.Idle {\n\t\tac.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// 更新状态为Connecting，表示正在连接中\n\tac.updateConnectivityState(connectivity.Connecting)\n\tac.mu.Unlock()\n\n\t// 异步启动一个协程去执行网络连接\n\tgo ac.resetTransport()\n\treturn nil\n}\n\nfunc (ac *addrConn) resetTransport() {\n\tfor i := 0; ; i++ {\n\t\t// 如果发生重试，说明有可能要连接的服务已经挂掉了，这时候服务列表应该发生变化了，触发一下立即执行服务发现\n\t\tif i > 0 {\n            // 该方法异步执行，最终调用resolver.ResolveNow\n\t\t\tac.cc.resolveNow(resolver.ResolveNowOption{})\n\t\t}\n\n\t\tac.mu.Lock()\n\t\t// 检查连接是否已经移除\n\t\tif ac.state == connectivity.Shutdown {\n\t\t\tac.mu.Unlock()\n\t\t\treturn\n\t\t}\n        \n\t\t// 要连接的addr\n\t\taddrs := ac.addrs\n        // backoffIdx保存一次连接的重试次数\n\t\tbackoffFor := ac.dopts.bs.Backoff(ac.backoffIdx)\n \n        dialDuration := minConnectTimeout\n\t\tif ac.dopts.minConnectTimeout != nil {\n\t\t\tdialDuration = ac.dopts.minConnectTimeout()\n\t\t}\n\n\t\tif dialDuration < backoffFor {\n\t\t\t// Give dial more time as we keep failing to connect.\n\t\t\tdialDuration = backoffFor\n\t\t}\n\t\t \n\t\tconnectDeadline := time.Now().Add(dialDuration)\n\t\t// 更新状态为Connecting\n\t\tac.updateConnectivityState(connectivity.Connecting)\n\t\tac.transport = nil\n\t\tac.mu.Unlock()\n\n\t\t// 尝试创建连接，只要一个addr成功立即返回\n\t\tnewTr, addr, reconnect, err := ac.tryAllAddrs(addrs, connectDeadline)\n\t\tif err != nil { // 创建失败\n\t\t\tac.mu.Lock()\n\t\t\t// 检查连接是否已经移除\n\t\t\tif ac.state == connectivity.Shutdown {\n\t\t\t\tac.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// 更新状态为TransientFailure，表示连接异常\n\t\t\tac.updateConnectivityState(connectivity.TransientFailure)\n\n\t\t\t// Backoff.\n\t\t\tb := ac.resetBackoff\n\t\t\tac.mu.Unlock()\n\t\t\t// sleep一下，然后重试\n\t\t\ttimer := time.NewTimer(backoffFor)\n\t\t\tselect {\n\t\t\tcase <-timer.C:\n\t\t\t\tac.mu.Lock()\n\t\t\t\tac.backoffIdx++\n\t\t\t\tac.mu.Unlock()\n\t\t\tcase <-b:\n\t\t\t\ttimer.Stop()\n\t\t\tcase <-ac.ctx.Done():\n\t\t\t\ttimer.Stop()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// 这里表示已经创建连接成功\n\t\tac.mu.Lock()\n\t\t// 双重检查，是否已经Shutdown\n\t\tif ac.state == connectivity.Shutdown {\n\t\t\tnewTr.Close()\n\t\t\tac.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\t// 当前连接的addr\n\t\tac.curAddr = addr\n\t\t// transport\n\t\tac.transport = newTr\n\t\tac.backoffIdx = 0\n \n        ...\n        // 不需要\n        if !healthcheckManagingState {\n\t\t\tac.updateConnectivityState(connectivity.Ready)\n\t\t}\n\t\tac.mu.Unlock()\n        // 等待连接异常，触发重连\n        <-reconnect.Done()\n        ...\n\t}\n}\n```\n\n接下来，看一下连接的状态变更的逻辑：\n\n```go\nfunc (ac *addrConn) updateConnectivityState(s connectivity.State) {\n\tif ac.state == s {\n\t\treturn\n\t}\n\n\tupdateMsg := fmt.Sprintf(\"Subchannel Connectivity change to %v\", s)\n\t// 状态变更\n\tac.state = s\n \n\t// 调用ClientConn的SubConn状态变更回调\n\tac.cc.handleSubConnStateChange(ac.acbw, s)\n}\n\nfunc (cc *ClientConn) handleSubConnStateChange(sc balancer.SubConn, s connectivity.State) {\n\tcc.mu.Lock()\n\tif cc.conns == nil {\n\t\tcc.mu.Unlock()\n\t\treturn\n\t}\n \t\n\tcc.balancerWrapper.handleSubConnStateChange(sc, s)\n\tcc.mu.Unlock()\n}\n\n// ccBalancerWrapper实现了balancer.ClientConn接口\nfunc (ccb *ccBalancerWrapper) handleSubConnStateChange(sc balancer.SubConn, s connectivity.State) {\n\tif sc == nil {\n\t\treturn\n\t}\n    // 将变更事件加入到队列中\n\tccb.stateChangeQueue.put(&scStateUpdate{\n\t\tsc:    sc,\n\t\tstate: s,\n\t})\n}\n```\n\n上面将连接的状态变更事件加入到了一个队列中，那么必然有地方从队列中取出事件，通知到balancer：\n\n```go\nfunc (ccb *ccBalancerWrapper) watcher() {\n\tfor {\n\t\tselect {\n        // 获取连接状态变更事件\n\t\tcase t := <-ccb.stateChangeQueue.get():\n            ...\n\t\t\t// V2Balancer是新版的Balancer接口\n\t\t\tif ub, ok := ccb.balancer.(balancer.V2Balancer); ok {\n                // 通知连接状态更新\n\t\t\t\tub.UpdateSubConnState(t.sc, balancer.SubConnState{ConnectivityState: t.state})\n\t\t\t} else {\n                // 通知连接状态更新\n\t\t\t\tccb.balancer.HandleSubConnStateChange(t.sc, t.state)\n\t\t\t}\n\t\tcase s := <-ccb.ccUpdateCh:\n\t\t\t...\n\t\tcase <-ccb.done:\n\t\t}\n\t\t... \n\t}\n}\n```\n\n```go\nfunc (b *baseBalancer) UpdateSubConnState(sc balancer.SubConn, state balancer.SubConnState) {\n   // 新的状态\n   s := state.ConnectivityState\n   // 旧的状态\n   oldS, ok := b.scStates[sc]\n   if !ok {\n      return\n   }\n   // 设置新的状态\n   b.scStates[sc] = s\n   switch s {\n   case connectivity.Idle:\n      sc.Connect()        // Idle触发连接 \n   case connectivity.Shutdown: \n      delete(b.scStates, sc) // 连接已经删除\n   }\n\n   // balancer原先的状态\n   oldAggrState := b.state\n   // 更新balancer的状态：\n   //   - 如果存在Ready的subConn，则状态为ready\n   //   - 否则如果存在connecting，则为connecting\n   //   - 否则为TransientFailure\n   b.state = b.csEvltr.RecordTransition(oldS, s)\n\n   // 当下面情况发生时，需要重新创建Picker：\n   //    - 连接由其他状态转变为Ready状态\n   //    - 连接由Ready状态转变为其他状态\n   //    - balancer转变为TransientFailure状态\n   //    - balancer由TransientFailure转变为Non-TransientFailure状态\n   if (s == connectivity.Ready) != (oldS == connectivity.Ready) ||\n      (b.state == connectivity.TransientFailure) != (oldAggrState == connectivity.TransientFailure) {\n      // 重新生成Picker\n      b.regeneratePicker()\n   }\n\n   // 回调更新状态和picker\n   b.cc.UpdateBalancerState(b.state, b.picker)\n}\n```\n\n\n\n##### 客户端连接创建：DialContext\n\n```go\nfunc DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {\n\tcc := &ClientConn{\n\t\ttarget:            target,                       // target\n\t\tcsMgr:             &connectivityStateManager{},  // 连接状态管理器\n\t\tconns:             make(map[*addrConn]struct{}), // 连接\n\t\tdopts:             defaultDialOptions(),         // 默认的options\n\t\tblockingpicker:    newPickerWrapper(),           // balancer.Picker的wrapper\n\t\tczData:            new(channelzData),\n\t\tfirstResolveEvent: grpcsync.NewEvent(),\n\t}\n\n\tcc.retryThrottler.Store((*retryThrottler)(nil))\n\tcc.ctx, cc.cancel = context.WithCancel(context.Background())\n\n\t// 应用options\n\tfor _, opt := range opts {\n\t\topt.apply(&cc.dopts)\n\t}\n\n\t// 如果存在多个Interceptors，则组装成一个调用链\n\tchainUnaryClientInterceptors(cc)\n\tchainStreamClientInterceptors(cc)\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tcc.Close()\n\t\t}\n\t}()\n\n\t...\n\n\t// tls连接加密证书相关检查\n\tif !cc.dopts.insecure {\n\t\tif cc.dopts.copts.TransportCredentials == nil && cc.dopts.copts.CredsBundle == nil {\n\t\t\treturn nil, errNoTransportSecurity\n\t\t}\n\t\tif cc.dopts.copts.TransportCredentials != nil && cc.dopts.copts.CredsBundle != nil {\n\t\t\treturn nil, errTransportCredsAndBundle\n\t\t}\n\t} else {\n\t\tif cc.dopts.copts.TransportCredentials != nil || cc.dopts.copts.CredsBundle != nil {\n\t\t\treturn nil, errCredentialsConflict\n\t\t}\n\t\tfor _, cd := range cc.dopts.copts.PerRPCCredentials {\n\t\t\tif cd.RequireTransportSecurity() {\n\t\t\t\treturn nil, errTransportCredentialsMissing\n\t\t\t}\n\t\t}\n\t}\n\n\t// 如果提供了服务配置\n\tif cc.dopts.defaultServiceConfigRawJSON != nil {\n\t\tsc, err := parseServiceConfig(*cc.dopts.defaultServiceConfigRawJSON)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %v\", invalidDefaultServiceConfigErrPrefix, err)\n\t\t}\n\t\t// 设置默认服务配置\n\t\tcc.dopts.defaultServiceConfig = sc\n\t}\n\n    // keepAlive配置\n\tcc.mkp = cc.dopts.copts.KeepaliveParams\n\n\t// 如果没有提供dial，则默认使用ProxyDialer，会根据系统环境变量的代理配置进行网络连接\n\tif cc.dopts.copts.Dialer == nil {\n\t\tcc.dopts.copts.Dialer = newProxyDialer(\n\t\t\tfunc(ctx context.Context, addr string) (net.Conn, error) {\n\t\t\t\tnetwork, addr := parseDialTarget(addr)\n\t\t\t\treturn (&net.Dialer{}).DialContext(ctx, network, addr)\n\t\t\t},\n\t\t)\n\t}\n\n\t// 用户代理添加grpcUA\n\tif cc.dopts.copts.UserAgent != \"\" {\n\t\tcc.dopts.copts.UserAgent += \" \" + grpcUA\n\t} else {\n\t\tcc.dopts.copts.UserAgent = grpcUA\n\t}\n\n\t// 如果options设置了timeout\n\tif cc.dopts.timeout > 0 {\n\t\tvar cancel context.CancelFunc\n\t\tctx, cancel = context.WithTimeout(ctx, cc.dopts.timeout)\n\t\tdefer cancel()\n\t}\n\tdefer func() {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tconn, err = nil, ctx.Err()\n\t\tdefault:\n\t\t}\n\t}()\n\n\tscSet := false\n\t// 如果提供了scChan，支持对serviceConfig进行热更\n\tif cc.dopts.scChan != nil {\n\t\t// Try to get an initial service config.\n\t\tselect {\n\t\t// 尝试获取初始的serviceConfig\n\t\tcase sc, ok := <-cc.dopts.scChan:\n\t\t\tif ok {\n\t\t\t\tcc.sc = &sc\n\t\t\t\tscSet = true // 成功获取初始的serviceConfig\n\t\t\t}\n\t\tdefault:\n\t\t}\n\t}\n    \n\t// 提供retry时的退避算法\n\tif cc.dopts.bs == nil {\n\t\tcc.dopts.bs = backoff.Exponential{\n\t\t\tMaxDelay: DefaultBackoffConfig.MaxDelay,\n\t\t}\n\t}\n\n\t// resolverBuilder，用于解析target为目标服务列表\n\t// 如果没有指定resolverBuilder\n\tif cc.dopts.resolverBuilder == nil {\n\t\t// Only try to parse target when resolver builder is not already set.\n\t\t// 解析target，根据target的scheme获取对应的resolver\n\t\tcc.parsedTarget = parseTarget(cc.target)\n\t\tgrpclog.Infof(\"parsed scheme: %q\", cc.parsedTarget.Scheme)\n\t\tcc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)\n\t\t// 如果没有的话，使用默认的resolver\n\t\tif cc.dopts.resolverBuilder == nil {\n\t\t\t// If resolver builder is still nil, the parsed target's scheme is\n\t\t\t// not registered. Fallback to default resolver and set Endpoint to\n\t\t\t// the original target.\n\t\t\tgrpclog.Infof(\"scheme %q not registered, fallback to default scheme\", cc.parsedTarget.Scheme)\n\t\t\tcc.parsedTarget = resolver.Target{\n\t\t\t\tScheme:   resolver.GetDefaultScheme(),\n\t\t\t\tEndpoint: target,\n\t\t\t}\n\t\t\tcc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)\n\t\t}\n\t} else {\n\t\tcc.parsedTarget = resolver.Target{Endpoint: target}\n\t}\n\n\t// 连接证书\n\tcreds := cc.dopts.copts.TransportCredentials\n\tif creds != nil && creds.Info().ServerName != \"\" {\n\t\tcc.authority = creds.Info().ServerName\n\t} else if cc.dopts.insecure && cc.dopts.authority != \"\" {\n\t\tcc.authority = cc.dopts.authority\n\t} else {\n\t\t// Use endpoint from \"scheme://authority/endpoint\" as the default\n\t\t// authority for ClientConn.\n\t\tcc.authority = cc.parsedTarget.Endpoint\n\t}\n\n\t// 如果提供了scChan但是还没有获取到初始的serviceConfig，则阻塞等待serviceConfig\n\tif cc.dopts.scChan != nil && !scSet {\n\t\t// Blocking wait for the initial service config.\n\t\tselect {\n\t\tcase sc, ok := <-cc.dopts.scChan:\n\t\t\tif ok {\n\t\t\t\tcc.sc = &sc\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n    \n\t// 启动子协程，监听scChan，进行serviceConfig的热更\n\tif cc.dopts.scChan != nil {\n\t\tgo cc.scWatcher()\n\t}\n\n\tvar credsClone credentials.TransportCredentials\n\tif creds := cc.dopts.copts.TransportCredentials; creds != nil {\n\t\tcredsClone = creds.Clone()\n\t}\n    \n\t// balancerBuild的options\n\tcc.balancerBuildOpts = balancer.BuildOptions{\n\t\tDialCreds:        credsClone,\n\t\tCredsBundle:      cc.dopts.copts.CredsBundle,\n\t\tDialer:           cc.dopts.copts.Dialer,\n\t\tChannelzParentID: cc.channelzID,\n\t\tTarget:           cc.parsedTarget,\n\t}\n\n\t// Build the resolver.\n\t// 创建resovler，并包装成resolverWrapper\n\trWrapper, err := newCCResolverWrapper(cc)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to build resolver: %v\", err)\n\t}\n\n\tcc.mu.Lock()\n\tcc.resolverWrapper = rWrapper\n\tcc.mu.Unlock()\n\t// A blocking dial blocks until the clientConn is ready.\n\t// 默认Dial不会等待网络连接完成，如果指定了blcok，则会阻塞等待网络连接完成才返回\n\tif cc.dopts.block {\n\t\tfor {\n\t\t\ts := cc.GetState()\n\t\t\t// 如果已经Ready\n\t\t\tif s == connectivity.Ready {\n\t\t\t\tbreak\n\t\t\t} else if cc.dopts.copts.FailOnNonTempDialError && s == connectivity.TransientFailure {\n\t\t\t\tif err = cc.blockingpicker.connectionError(); err != nil {\n\t\t\t\t\tterr, ok := err.(interface {\n\t\t\t\t\t\tTemporary() bool\n\t\t\t\t\t})\n\t\t\t\t\tif ok && !terr.Temporary() {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// 等待状态变更\n\t\t\tif !cc.WaitForStateChange(ctx, s) {\n\t\t\t\t// ctx got timeout or canceled.\n\t\t\t\treturn nil, ctx.Err()\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cc, nil\n}\n```\n\n\n\n##### 总结\n\n到此，对grpc客户端的连接创建流程应该有了一个大体的了解，并且我们能够很容易的根据`Resolver`接口提供自己的服务发现逻辑。","tags":["grpc - go"]},{"title":"grpc server解析","url":"/2019/07/22/grpc_server解析/","content":"\n`grpc`是由谷歌开源的一个高性能、通用的开源`rpc`框架，具体的使用可以参考[该文章](<http://mcll.top/2018/12/21/grpc%E4%B8%8A%E6%89%8B%E4%BD%BF%E7%94%A8/>)。本文主要看一下`go`版本的`grpc`的服务端实现。\n\n### grpc server\n\n我们先看一下启动一个grpc server时的代码：\n\n```go\nfunc main() {\n\tlis, err := net.Listen(\"tcp\", \":6060\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\ts := grpc.NewServer()\n\t// 注册服务，EchoServer实现了.proto中声明的服务接口\n\tproto.RegisterEchoSvcServer(s, &EchoServer{})\n\t// 启动服务\n\tif err := s.Serve(lis); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n```\n\n可以看到，第6行创建了一个grpc server，第8行将具体的服务注册到server中，然后第10行开始启动服务。\n\n其中，`RegisterEchoSvcServer`这个函数由插件`protoc-gen-go`通过`.proto`文件自动生成的，我们先来看一下其实现：\n\n```go\n// 服务描述，由插件通过`.proto`文件自动生成\nvar _EchoSvc_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"proto.EchoSvc\", // 服务名\n\tHandlerType: (*EchoSvcServer)(nil), // 这里声明了该服务要实现的接口\n    // 服务具有的方法列表\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Echo\", // rpc方法名\n\t\t\tHandler:    _EchoSvc_Echo_Handler, // rpc请求的handler\n\t\t},\n\t},\n    // stream方法列表，该服务没有\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"echo.proto\",\n}\n\nfunc RegisterEchoSvcServer(s *grpc.Server, srv EchoSvcServer) {\n    // 传入服务描述和具体的服务实现对象\n\ts.RegisterService(&_EchoSvc_serviceDesc, srv)\n}\n```\n\n上面的_EchoSvc_serviceDesc是有插件根据我们的服务声明自动生成的，grpc中的rpc方法主要有两种类型。第一种就是常见的普通的rpc方法，第二种是[stream rpc方法](<https://grpc.io/docs/guides/concepts/>)\n\n可以看到，实际上调用的是grpc server的服务注册方法：\n\n```go\nfunc (s *Server) RegisterService(sd *ServiceDesc, ss interface{}) {\n\t// ss需要实现的接口类型\n    ht := reflect.TypeOf(sd.HandlerType).Elem()\n    // ss实际的类型\n\tst := reflect.TypeOf(ss)\n    // ss需要实现sd.HandlerType中指定的接口\n\tif !st.Implements(ht) {\n\t\tgrpclog.Fatalf(\"grpc: Server.RegisterService found the handler of type %v that does not satisfy %v\", st, ht)\n\t}\n    // 注册接口到server\n\ts.register(sd, ss)\n}\n\nfunc (s *Server) register(sd *ServiceDesc, ss interface{}) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.printf(\"RegisterService(%q)\", sd.ServiceName)\n    // 如果server已经开始运行，不允许注册\n\tif s.serve {\n\t\tgrpclog.Fatalf(\"grpc: Server.RegisterService after Server.Serve for %q\", sd.ServiceName)\n\t}\n    // 同一个服务名不允许重复注册\n\tif _, ok := s.m[sd.ServiceName]; ok {\n\t\tgrpclog.Fatalf(\"grpc: Server.RegisterService found duplicate service registration for %q\", sd.ServiceName)\n\t}\n\tsrv := &service{\n\t\tserver: ss, // 具体的服务实现对象\n\t\tmd:     make(map[string]*MethodDesc), // 普通rpc方法描述\n\t\tsd:     make(map[string]*StreamDesc), // stream类型的rpc方法描述\n\t\tmdata:  sd.Metadata,\n\t}\n    // 添加方法描述到srv的md\n\tfor i := range sd.Methods {\n\t\td := &sd.Methods[i]\n\t\tsrv.md[d.MethodName] = d\n\t}\n    // 添加方法描述到srv.sd\n\tfor i := range sd.Streams {\n\t\td := &sd.Streams[i]\n\t\tsrv.sd[d.StreamName] = d\n\t}\n    // 将servicer添加到server的services表中\n\ts.m[sd.ServiceName] = srv\n}\n```\n\n可以看到，grpc的server中有一个service表，相当于http服务中的路由表。\n\n接下来看一下，grpc server如果提供服务：\n\n```go\nfunc (s *Server) Serve(lis net.Listener) error {\n\ts.mu.Lock()\n\ts.printf(\"serving\")\n    // 开始运行\n\ts.serve = true\n \t\n    s.serveWG.Add(1)\n\tdefer func() {\n\t\ts.serveWG.Done()\n\t\tselect {\n\t\t// serve会阻塞直到退出服务\n\t\tcase <-s.quit:\n\t\t\t<-s.done\n\t\tdefault:\n\t\t}\n\t}()\n\n    // 把lis添加到lis表中，可以看到一个server可以同时监听多个端口提供服务\n\tls := &listenSocket{Listener: lis}\n\ts.lis[ls] = true\n\n\ts.mu.Unlock()\n\n\tdefer func() {\n        // 服务退出时，从lis表中移除\n\t\ts.mu.Lock()\n\t\tif s.lis != nil && s.lis[ls] {\n\t\t\tls.Close()\n\t\t\tdelete(s.lis, ls)\n\t\t}\n\t\ts.mu.Unlock()\n\t}()\n\n\tfor {\n        // 接受客户端请求\n\t\trawConn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\t// 错误处理...\n\t\t}\n\t\t\n\t\ts.serveWG.Add(1)\n\t\tgo func() {\n            // 处理客户端连接\n\t\t\ts.handleRawConn(rawConn)\n\t\t\ts.serveWG.Done()\n\t\t}()\n\t}\n}\n\nfunc (s *Server) handleRawConn(rawConn net.Conn) {\n    // 设置read和write的Deadline\n\trawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout))\n\t// 可能开启了tls/ssl，需要证书认证，完成tls/ssl握手\n    conn, authInfo, err := s.useTransportAuthenticator(rawConn)\n\tif err != nil {\n\t\t// ...\n\t\treturn\n\t}\n\n\ts.mu.Lock()\n\tif s.conns == nil {\n\t\ts.mu.Unlock()\n\t\tconn.Close()\n\t\treturn\n\t}\n\ts.mu.Unlock()\n\n\t// grpc是基于http2协议进行通信的，完成http2协议的握手\n\tst := s.newHTTP2Transport(conn, authInfo)\n\tif st == nil {\n\t\treturn\n\t}\n\t// 前面设置Deadline是为了尽快完成握手操作\n    // 因为客户端连接之后，并不是一直在发送请求，设置Deadline没有意义，因此这里取消deadline的设置\n\trawConn.SetDeadline(time.Time{})\n    // 保存客户端的http2连接\n\tif !s.addConn(st) {\n\t\treturn\n\t}\n    \n\tgo func() {\n        // 等待客户端rpc请求到来，并提供服务\n        // 基于http2的多路复用，客户端可以使用一条连接同时发送多个请求\n\t\ts.serveStreams(st)\n        // 移除客户端的http2连接\n\t\ts.removeConn(st)\n\t}()\n}\n\nfunc (s *Server) serveStreams(st transport.ServerTransport) {\n\tdefer st.Close()\n\tvar wg sync.WaitGroup\n\n    // HandleStreams接收两个参数：handler和tracer\n    // 该方法有两个参数：hanlder和tracer\n    // 该方法循环读取客户端连接发送过来的帧：\n    //    1. 如果是HEADER帧，说明有新的rpc请求到来，回调handler\n    //    2. 如果是DATA帧，将数据分发到对应的stream\n    //    3. ...\n\tst.HandleStreams(func(stream *transport.Stream) {\n\t\twg.Add(1)\n        // 回调中开启子协程，处理rpc请求\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n            // grpc基于http2，同一条连接会分成多个stream，每个rpc请求使用一个stream\n            // 这样多个客户端请求可以复用同一条连接\n            // 当有新的rpc请求到来，会进入该回调，然后调用server的handleStream处理rpc请求\n\t\t\ts.handleStream(st, stream, s.traceInfo(st, stream))\n\t\t}()\n\t}, func(ctx context.Context, method string) context.Context {\n\t\tif !EnableTracing {\n\t\t\treturn ctx\n\t\t}\n\t\ttr := trace.New(\"grpc.Recv.\"+methodFamily(method), method)\n\t\treturn trace.NewContext(ctx, tr)\n\t})\n\twg.Wait()\n}\n```\n\n接下来我们看一下server的handleStream方法，该方法处理rpc请求：\n\n```go\nfunc (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {\n    // rpc所请求的方法：服务名/方法名\n\tsm := stream.Method()\n    // 去掉开头的`/`\n\tif sm != \"\" && sm[0] == '/' {\n\t\tsm = sm[1:]\n\t}\n\tpos := strings.LastIndex(sm, \"/\")\n\tif pos == -1 {\n        // 应该满足：服务名/方法名\n        // ...\n\t\treturn\n\t}\n    // 请求的服务\n\tservice := sm[:pos]\n    // 请求的方法\n\tmethod := sm[pos+1:]\n    // 在server的服务表中查找对应的服务实现\n    // server运行时不允许注册新的service，因此这里并发读，不需要加锁\n\tsrv, ok := s.m[service]\n    // 如果请求的服务不存在\n\tif !ok {\n        // 如果server的配置中，指定了处理未知服务的方法，则交由其处理\n\t\tif unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {\n\t\t\ts.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)\n\t\t\treturn\n\t\t}\n        // ...\n\t\treturn\n\t}\n\t// 先在普通的rpc方法表中查找\n\tif md, ok := srv.md[method]; ok {\n        // 处理普通的rpc方法\n\t\ts.processUnaryRPC(t, stream, srv, md, trInfo)\n\t\treturn\n\t}\n    // 尝试在strem rpc找\n\tif sd, ok := srv.sd[method]; ok {\n        // 处理stream rpc\n\t\ts.processStreamingRPC(t, stream, srv, sd, trInfo)\n\t\treturn\n\t}\n    \n    // 请求未知方法\n    \n    // ...\n\tif unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {\n\t\ts.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)\n\t\treturn\n\t}\n \t\n    // ...\n}\n```\n\n限于篇幅，我们这里主要看一下`processUnaryRPC`方法，`processStreamingRPC`方法大同小异：\n\n```go\nfunc (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, md *MethodDesc, trInfo *traceInfo) (err error) {\n    \n    // ...\n    \n\tvar comp, decomp encoding.Compressor\n\tvar cp Compressor\n\tvar dc Decompressor\n\t// ...\t\n    // 设置压缩选项\n\tif s.opts.cp != nil {\n\t\tcp = s.opts.cp\n\t\tstream.SetSendCompress(cp.Type())\n\t} else if rc := stream.RecvCompress(); rc != \"\" && rc != encoding.Identity {\n\t\t// Legacy compressor not specified; attempt to respond with same encoding.\n\t\tcomp = encoding.GetCompressor(rc)\n\t\tif comp != nil {\n\t\t\tstream.SetSendCompress(rc)\n\t\t}\n\t}\n\n\tvar payInfo *payloadInfo\n\tif sh != nil || binlog != nil {\n\t\tpayInfo = &payloadInfo{}\n\t}\n\t// 接收并解压缩数据\n\td, err := recvAndDecompress(&parser{r: stream}, stream, dc, s.opts.maxReceiveMessageSize, payInfo, decomp)\n\tif err != nil {\n\t\t// ...\n\t\treturn err\n\t}\n\n\t// df方法用于从接收的数据包d中反序列化为v\n\tdf := func(v interface{}) error {\n        // 反序列化请求参数\n\t\tif err := s.getCodec(stream.ContentSubtype()).Unmarshal(d, v); err != nil {\n\t\t\treturn status.Errorf(codes.Internal, \"grpc: error unmarshalling request: %v\", err)\n\t\t}\n\t\t// ...\n\t\treturn nil\n\t}\n    \n\t// 创建context，该方法和header的获取以及写入有关，下面分析\n\tctx := NewContextWithServerTransportStream(stream.Context(), stream)\n\t// 执行handler，这个handler是通过.proto文件生成的，该方法内会去调用server的对应的方法，该方法返回对应的resp\n    // 这里第三个参数是反序列化方法，第四个参数是创建server时指定的interceptor选项\n\treply, appErr := md.Handler(srv.server, ctx, df, s.opts.unaryInt)\n\t// 如果返回的错误，这里的err可能是由我们的rpc方法返回的\n\tif appErr != nil {\n\t\tappStatus, ok := status.FromError(appErr)\n\t\tif !ok {\n\t\t\t// 如过没有实现 interface{GRPCStatus()*Status} 接口\n\t\t\tappErr = status.Error(codes.Unknown, appErr.Error())\n\t\t\tappStatus, _ = status.FromError(appErr)\n\t\t}\n\t\t// ...\n\t\t// 写入错误信息到stream中\n\t\tif e := t.WriteStatus(stream, appStatus); e != nil {\n\t\t\tgrpclog.Warningf(\"grpc: Server.processUnaryRPC failed to write status: %v\", e)\n\t\t}\n\t\t// ...\n\t\treturn appErr\n\t}\n \n\topts := &transport.Options{Last: true}\n\t// 序列化reply给客户端\n\tif err := s.sendResponse(t, stream, reply, cp, opts, comp); err != nil {\n\t\t// ...\n\t\treturn err\n\t}\n\n\terr = t.WriteStatus(stream, status.New(codes.OK, \"\"))\n    \n\treturn err\n}\n\n```\n\n上面的代码略有删减，主要是删掉一些和统计、trace以及日志相关的代码。主要的逻辑就是从stream读取请求参参数，反序列化后调用methodDesc中的handler方法，然后把返回的内容序列化后写入stream返回给客户端。\n\n我们知道，grpc是基于http2协议的，因此也是存在`header`的，grpc和http一样，可以设置和获取请求的header。在服务端，主要有获取客户端传递过来的header以及传递header给客户端两个操作。\n\n我们先看上面出现的`NewContextWithServerTransportStream`方法：\n\n```go\n// 该接口用于服务端设置传递给客户端的header\ntype ServerTransportStream interface {\n\tMethod() string\n\tSetHeader(md metadata.MD) error\n\tSendHeader(md metadata.MD) error\n\tSetTrailer(md metadata.MD) error\n}\n\nfunc NewContextWithServerTransportStream(ctx context.Context, stream ServerTransportStream) context.Context {\n    // 基于ctx创建新的context，并把stream保存到新的context中\n    // 当调用grpc.SetHeader时，会执行stream.SetHeader方法\n\treturn context.WithValue(ctx, streamKey{}, stream)\n}\n```\n\n我们可以看到在`processUnaryRPC`方法中，对该方法的调用如下：\n\n```go\nfunc (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, md *MethodDesc, trInfo *traceInfo) (err error) {\n \t// ...\n\tctx := NewContextWithServerTransportStream(stream.Context(), stream)\n\treply, appErr := md.Handler(srv.server, ctx, df, s.opts.unaryInt)\n \t// ...\n}\n```\n\n可以看到，传入的是当前请求的`stream`的`context`，接下来看一下`stream`的`context`创建：\n\n```go\nfunc (t *http2Server) operateHeaders(frame *http2.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := decodeState{serverSide: true}\n    // 解析header帧，包括获取header中的各个字段\n\tif err := state.decodeHeader(frame); err != nil {\n\t \t// ...\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.encoding,\n\t\tmethod:         state.method,\n\t\tcontentSubtype: state.contentSubtype,\n\t}\n \n    // ...\n    \n \t// 除了grpc预定义的几个header之外，其他header都保存到mdata中\n\tif len(state.mdata) > 0 {\n        // 这里会将state.mdata保存到新的context中\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.mdata)\n\t}\n    // ...\n\thandle(s)\n\treturn false\n}\n\nfunc NewIncomingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdIncomingKey{}, md)\n}\n```\n\n当收到一个`Header`帧，就表明有新的rpc请求到来，这时候就会解析header帧并创建stream，在创建stream的时候，会把用户自定义的header字段保存到stream.context中\n\n在我们实际编码时，可以通过`metadata`包来读取客户端传递过来的`header`：\n\n```go\nfunc (EchoServer) Echo(ctx context.Context, req *proto.EchoReq) (resp *proto.EchoResp, err error) {\n\tmd, ok := metadata.FromIncomingContext(ctx)\n\tif ok {\n\t\tlog.Printf(\"%s: %v\", md.Get(\"key\"))\n\t}\n    \n\treturn &proto.EchoResp{\n\t\tMsg: VERSION,\n\t}, err\n}\n```\n\n而设置header返回给客户端可以如下：\n\n```go\nfunc (EchoServer) Echo(ctx context.Context, req *proto.EchoReq) (resp *proto.EchoResp, err error) {\n\t// 最终会设置stream的header\n\tgrpc.SetHeader(ctx, metadata.Pairs(\"key1\", \"val1\"))\n\n\treturn &proto.EchoResp{\n\t\tMsg: VERSION,\n\t}, err\n}\n```\n\n接下来看一下写回返回内容给客户端的逻辑：\n\n```go\nfunc (s *Server) sendResponse(t transport.ServerTransport, stream *transport.Stream, msg interface{}, cp Compressor, opts *transport.Options, comp encoding.Compressor) error {\n    // 反序列化返回内容\n\tdata, err := encode(s.getCodec(stream.ContentSubtype()), msg)\n\tif err != nil {\n\t\tgrpclog.Errorln(\"grpc: server failed to encode response: \", err)\n\t\treturn err\n\t}\n    // 压缩\n\tcompData, err := compress(data, cp, comp)\n\tif err != nil {\n\t\tgrpclog.Errorln(\"grpc: server failed to compress response: \", err)\n\t\treturn err\n\t}\n    // 创建消息头部\n\thdr, payload := msgHeader(data, compData)\n\t// TODO(dfawley): should we be checking len(data) instead?\n\tif len(payload) > s.opts.maxSendMessageSize {\n\t\treturn status.Errorf(codes.ResourceExhausted, \"grpc: trying to send message larger than max (%d vs. %d)\", len(payload), s.opts.maxSendMessageSize)\n\t}\n    // 写回内容\n\terr = t.Write(stream, hdr, payload, opts)\n\tif err == nil && s.opts.statsHandler != nil {\n\t\ts.opts.statsHandler.HandleRPC(stream.Context(), outPayload(false, msg, data, payload, time.Now()))\n\t}\n\treturn err\n}\n\nfunc (t *http2Server) Write(s *Stream, hdr []byte, data []byte, opts *Options) error {\n    // 如果header还没有发送，先发送header\n\tif !s.isHeaderSent() { // Headers haven't been written yet.\n\t\tif err := t.WriteHeader(s, nil); err != nil {\n\t\t\treturn status.Errorf(codes.Internal, \"transport: %v\", err)\n\t\t}\n\t} else {\n\t\t// ...\n\t}\n\n\temptyLen := http2MaxFrameLen - len(hdr)\n\tif emptyLen > len(data) {\n\t\temptyLen = len(data)\n\t}\n\thdr = append(hdr, data[:emptyLen]...)\n\tdata = data[emptyLen:]\n    // 数据帧\n\tdf := &dataFrame{\n\t\tstreamID: s.id,\n\t\th:        hdr,\n\t\td:        data,\n\t\tonEachWrite: func() {\n\t\t\tatomic.StoreUint32(&t.resetPingStrikes, 1)\n\t\t},\n\t}\n\tif err := s.wq.get(int32(len(hdr) + len(data))); err != nil {\n\t\tselect {\n\t\tcase <-t.ctx.Done():\n\t\t\treturn ErrConnClosing\n\t\tdefault:\n\t\t}\n\t\treturn ContextErr(s.ctx.Err())\n\t}\n    // 把数据帧加入到发送队列\n\treturn t.controlBuf.put(df)\n}\n```\n\n最后，看一下methodDesc中的handler，这个是由插件自动生成的包装方法：\n\n```go\nfunc (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, md *MethodDesc, trInfo *traceInfo) (err error) {\n    // ...\n    reply, appErr := md.Handler(srv.server, ctx, df, s.opts.unaryInt)\n    // ...\n}\n```\n\n```go\nfunc _EchoSvc_Echo_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\t// \n    in := new(EchoReq)\n    // dec是传入的反序列化方法\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n    \n    // 如果没有指定interceptor\n\tif interceptor == nil {\n        // 直接调用service对应的方法\n\t\treturn srv.(EchoSvcServer).Echo(ctx, in)\n\t}\n    \n    // 服务信息\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/proto.EchoSvc/Echo\",\n\t}\n    \n    // 回调handler\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(EchoSvcServer).Echo(ctx, req.(*EchoReq))\n\t}\n    // 先执行interceptor，然后在执行handler\n\treturn interceptor(ctx, in, info, handler)\n}\n```\n\n可以看到，用户创建`server`时，如果设置了`interceptor`选项，那么在执行具体的服务方法前，会先执行用户设置的`interceptor`，声明如下：\n\n```go\ntype UnaryServerInterceptor func(ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler) (resp interface{}, err error)\n```\n\n在`interceptor`中，可以做一些通用处理，比如日志记录，异常处理或者请求拦截等\n\n\n\n\n\n","tags":["grpc - go"]},{"title":"mmap使用","url":"/2019/07/22/mmap使用/","content":"\n`mmap`可以进程的虚拟内存中创建内存映射，常用来将磁盘文件映射到内存，读写文件转变成读写对应的内存\n\n使用`mmap`读写文件，通常比普通的`read`和`write`系统调用要更快：\n\n- 直接操作内存，避免了系统调用的执行，从而避免用户态/内核态之间的切换开销\n- 直接读写内存，不需要经过内核缓冲区，减少数据拷贝\n\n用户访映射的内存时，如果磁盘数据还没有加载到内存中，会触发缺页异常，然后操作系统会以`page`为单位将磁盘文件加载到内存中；如果内存数据被修改了，操作系统会将脏页回写到对应的文件中（脏页回写并不是实时的）。缺页异常的处理开销会比较大，尤其是当物理内存不足时，还会涉及到内存页的淘汰，如果被淘汰的内存页是脏页的话，还需要将其同步到磁盘中。在某些情况下，使用`mmap`映射内存，可能会频繁的触发缺页异常，从而导致性能下降，甚至不如直接使用`read/write`系统调用，比如物理内存很小，而映射的文件很大时。\n\n而且使用`mmap`并不会更改映射文件的大小，当需要更改文件大小时，需要重新映射。\n\n\n\n`mmap`除了支持将文件映射到内存，还支持匿名映射，匿名映射不需要底层文件。匿名映射的内存也可以在多个进程间共享，因此可以用来实现进程间的内存共享通信。`go`运行时的内存管理，便是使用匿名映射向操作系统申请了一块大内存，然后基于`tcmalloc`进行内存管理。\n\n\n\n### 在go中使用mmap\n\n`mmap`相关的系统调用有：\n\n- mmap：创建内存映射\n- munmap：取消内存映射\n- msync：同步内存到磁盘文件；内存的修改并不是实时写回磁盘的，当对实时性要求很高，比如数据库的写操作，需要在写内存后手动刷新到磁盘中\n\n接下来通过一个实例看一下如果在go中使用这几个接口：\n\n```go\nfunc checkErr(err error) {\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\nfunc main() {\n    // 获取int类型的大小\n\tconst INT_SIZE = unsafe.Sizeof(int(0))\n    // 这里a.txt是我们要映射的底层文件\n\tfd, err := os.OpenFile(\"a.txt\", os.O_RDWR|os.O_CREATE, os.ModePerm)\n\tcheckErr(err)\n\n\tinfo, _ := fd.Stat()\n    // mmap不会更改底层文件的大小，我们要确保访问的映射地址不会超过文件大小，否则会panic\n    // 这里设置一下底层文件大小\n\tif info.Size() != int64(INT_SIZE) {\n\t\tfd.Truncate(int64(INT_SIZE))\n\t}\n\n    // 使用syscall的mmap接口，创建内存映射\n    // mmap接口相比posix接口，少了一个addr参数，如果有需要可以使用syscall.Syscall6接口\n    // MAP_SHARED指定映射的类型，该模式下对映射空间的更新对其他进程的映射可见，并且会写回底层文件\n    // 映射内存会通过[]byte的形式返回\n\tbuf, err := syscall.Mmap(int(fd.Fd()), 0, int(INT_SIZE), syscall.PROT_WRITE|syscall.PROT_READ, syscall.MAP_SHARED)\n\t// mmap返回之后，底层文件的设备描述符可以立即close掉\n    fd.Close() // After the mmap() call has returned, the file descriptor can be closed immediately\n\tcheckErr(err)\n\t\n    // 这里，直接将映射的内存强制类型转换成一个int指针p\n\tp := (*int)(unsafe.Pointer(&buf[0]))\n\t// 对指针p的读取就是读取文件内容\n\tlog.Printf(\"the value saved on file is %d\", *p)\n    // 对指针p的更新就是更新文件内容\n\t*p = rand.New(rand.NewSource(time.Now().UnixNano())).Intn(1000)\n\t// 脏页写回并不是及时的，使用msync系统调用强制将更新写回磁盘文件\n\t_, _, errno := syscall.Syscall(syscall.SYS_MSYNC, uintptr(unsafe.Pointer(p)), uintptr(INT_SIZE), syscall.MS_SYNC)\n\tif errno != 0 {\n\t\tlog.Fatal(syscall.Errno(errno))\n\t}\n\t// 使用munmap系统调用需求内存映射\n\t_, _, errno = syscall.Syscall(syscall.SYS_MUNMAP, uintptr(unsafe.Pointer(p)), uintptr(INT_SIZE), 0)\n\tif errno != 0 {\n\t\tlog.Fatal(syscall.Errno(errno))\n\t}\n\n}\n```\n\n可以看到，我们可以直接将`mmap`创建的内存，强制转换成具体的类型值，然后像普通的变量一样操作，连序列化/反序列化都省了。","tags":["mmap - go"]},{"title":"tcp keepalive in go","url":"/2019/07/20/tcp-keepalive-in-go/","content":"\n### overview\n\n建立`tcp`连接后，可以通过`FIN`包或者`RST`包通知对端关闭连接，其中`FIN`包是正常连接双方四次握手关闭连接过程中发送的，需要接收对方的`ack`，而`RST`包是通知对方立即关闭，也不需要等待对方的`ack`。\n\n但是，如果客户端在连接过程中宕机了，服务端不会收到任何消息，这时候服务端认为连接还存在，但是客户端并不了解。这时候，需要等到服务端向客户端写入消息时，因为客户端并没有这条连接的信息，向服务端返回`rst`包，服务端才会关闭这条连接，并释放相应的资源。\n\n`tcp`连接建立之后，并不是一直处于读写状态，当有一方由于某种原因意外断开，另一方需要等到下一次发送数据时才能关闭连接，而这中间会一直占用系统资源。\n\n这就需要有一种心跳机制，能够定时检查对方连接是否存活，而[`tcp keepalive`](<https://tools.ietf.org/html/rfc1122#page-101>)就算实现该功能的机制。`keepalive`不是`tcp`标准的一部分，并且默认是禁用的，但是目前大多数`tcp`实现都支持。\n\n服务端开启`tcp keepalive`之后，当连接空闲的时候，会定时发送空`body`的`packet`给客户端。根据`tcp`的规范，当客户端接收到一个包之后，需要回复`ACK`，即使之前已经回复过相同的`ACK`了，因此即使客户端没有实现`keepalive`功能也可以正常工作。\n\n在大多数实现中，设置`keepalive`主要有三个参数：\n\n- `tcp_keepalive_time`：间隔多久没有发送数据后，就发送一个心跳包\n\n- `tcp_keepalive_intvl`：发送的心跳包如果没有收到`ack`，间隔多久后，重新发送\n\n- `tcp_keepalive_probes`：最多发送多少个心跳包没有收到回复后，认为对方挂掉了\n\n比如，`tcp_keepalive_time`设置为30s，`tcp_keepalive_intvl`设置为5s，`tcp_keepalive_probes`设置为3，那么当连接空闲30s没有发送数据，会发送第一个心跳包，如果接收到了`ack`，那么会等待空闲30s后再次发送心跳包；而如果没有收到`ack`，5s后会重试，发送第二个心跳包，如果再没有收到`ack`包，那么等待5s后会重试，发送第三个心跳包，如果还没有收到`ack`包，那么就任务对方连接已经挂掉了。\n\n在linux中，可以查看这三个参数的默认值：\n\n```sh\n$ cat /proc/sys/net/ipv4/tcp_keepalive_time \n7200\n$ cat /proc/sys/net/ipv4/tcp_keepalive_intvl \n75\n$ cat /proc/sys/net/ipv4/tcp_keepalive_probes \n9 \n```\n\n我们可以通过编辑`/etc/sysctl.conf`，来修改这三个参数的默认值，并使用`sysctl -p`使其生效，程序不需要重启，内核直接生效。\n\n`keepalive`还有一个作用是当使用NAT代理或者防火墙的时候，防止连接因为不活动而被断开。\n\n\n\n### code\n\n`go`中的`net.TCPConn`提供了`SetKeepAlive`和`SetKeepAlivePeriod`两个方法\n\n```go\nfor {\n\tconn, err := ln.Accept()\n\tif err != nil {\n\t\tlog.Printf(\"failed to accept new conn: %s\", err.Error())\n\t\tcontinue\n\t}\n\n\ttcpConn := conn.(*net.TCPConn)\n\ttcpConn.SetKeepAlive(true) // 开启keepalive\n\ttcpConn.SetKeepAlivePeriod(time.Second * 30) // 设置tcp_keepalive_time\n\tgo handleConn(conn)\n}\n```\n\n如果需要设置`tcp_keepalive_intvl`和`tcp_keepalive_probes`两个参数，则需要`syscall`包中的方法：\n\n```go\n\tfd, err := tcpConn.File()\n\tif err == nil {\n\t\t// 设置tcp_keepalive_probes\n\t\terr = syscall.SetsockoptInt(int(fd.Fd()), syscall.IPPROTO_TCP, syscall.TCP_KEEPCNT, 3)\n\t\tif err != nil {\n\t\t\t// handle error\n\t\t}\n\t\t// 设置tcp_keepalive_intvl\n\t\terr = syscall.SetsockoptInt(int(fd.Fd()), syscall.IPPROTO_TCP, syscall.TCP_KEEPINTVL, 5)\n\t\tif err != nil {\n\t\t\t// handle error\n\t\t}\n\t}\n```\n\n上面使用`File.Fd`方法，该方法在`os/file_unix.go`的实现如下：\n\n```go\nfunc (f *File) Fd() uintptr {\n\tif f == nil {\n\t\treturn ^(uintptr(0))\n\t}\n\n\t// If we put the file descriptor into nonblocking mode,\n\t// then set it to blocking mode before we return it,\n\t// because historically we have always returned a descriptor\n\t// opened in blocking mode. The File will continue to work,\n\t// but any blocking operation will tie up a thread.\n\tif f.nonblock {\n        // 设置成阻塞模式\n\t\tf.pfd.SetBlocking()\n\t}\n\n\treturn uintptr(f.pfd.Sysfd)\n}\n```\n\n在`go`中，网路连接默认是非阻塞模式，对网路连接的读写会通过[netpoll](<http://mcll.top/2019/04/07/go%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/>)来实现非阻塞读写，而当转换成阻塞模式之后，每次读写都会变成一次阻塞的系统调用，从而导致**大量的系统线程**被创建。\n\n`go1.11`之后，添加了`syscall.RawConn`接口，我们可以通过这个接口来规避使用`File.Fd`：\n\n```go\n\trawConn, err := tcpConn.SyscallConn()\n\tif err == nil {\n\t\trawConn.Control(func(fd uintptr) {\n\t\t\t// 设置tcp_keepalive_probes\n\t\t\terr = syscall.SetsockoptInt(int(fd), syscall.IPPROTO_TCP, syscall.TCP_KEEPCNT, 3)\n\t\t\tif err != nil {\n\t\t\t\t// handle error\n\t\t\t}\n\t\t\t// 设置tcp_keepalive_intvl\n\t\t\terr = syscall.SetsockoptInt(int(fd), syscall.IPPROTO_TCP, syscall.TCP_KEEPINTVL, 5)\n\t\t\tif err != nil {\n\t\t\t\t// handle error\n\t\t\t}\n\t\t})\n\t}\n```\n\n\n\n### 参考链接\n\n- [聊聊 TCP 中的 KeepAlive 机制](<https://zhuanlan.zhihu.com/p/28894266>)\n- [Notes on TCP keepalive in Go](<https://thenotexpert.com/golang-tcp-keepalive/?utm_campaign=The%20Go%20Gazette&utm_medium=email&utm_source=Revue%20newsletter#an-important-note-on-file-descriptors>)\n- [TCP keepalive overview](<http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html>)","tags":["go - tcp"]},{"title":"sync扩展包","url":"/2019/07/19/sync扩展包/","content":"\n[glang.org/x/sync](<https://github.com/golang/sync>)包提供了一些方便使用的用于并发操作的扩展\n\n\n\n### errgroup\n\n`errgroup`用于执行一个整体任务的一组子任务\n\n```go\n// Group代表一组子任务\ntype Group struct {\n\tcancel func()\n\n\twg sync.WaitGroup\n\n\terrOnce sync.Once // 用于初始化err\n\terr     error\n}\n\n// 当一个子任务返回error或者所有子任务成功运行结束，cancel会被执行\nfunc WithContext(ctx context.Context) (*Group, context.Context) {\n    ctx, cancel := context.WithCancel(ctx)\n\treturn &Group{cancel: cancel}, ctx\n}\n\n// 等带所有子任务都运行完成\nfunc (g *Group) Wait() error {\n\tg.wg.Wait() \n\tif g.cancel != nil {\n\t\tg.cancel() // cancel\n\t}\n\treturn g.err\n}\n\n// Go开始运行一个子任务\nfunc (g *Group) Go(f func() error) {\n\tg.wg.Add(1)\n\n\tgo func() {\n\t\tdefer g.wg.Done()\n\t\t// 如果返回了error\n\t\tif err := f(); err != nil {\n            // errOnce确保只执行一次\n\t\t\tg.errOnce.Do(func() {\n\t\t\t\tg.err = err\n                // 返回错误，则执行cancel\n\t\t\t\tif g.cancel != nil {\n\t\t\t\t\tg.cancel()\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}()\n}\n```\n\n可以看到`Group`的源码很简单，接下来看一下官方给的demo，看一下使用方法：\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"crypto/md5\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n    \"path/filepath\"\n\n    \"golang.org/x/sync/errgroup\"\n)\n\nfunc main() {\n    // 计算指定路径下所有文件的md5\n    m, err := MD5All(context.Background(), \".\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\t\n    for k, sum := range m {\n        fmt.Printf(\"%s:\\t%x\\n\", k, sum)\n    }\n}\n\n// 保存文件md5\ntype result struct {\n    path string // 文件路径 \n    sum  [md5.Size]byte // md5信息\n}\n\nfunc MD5All(ctx context.Context, root string) (map[string][md5.Size]byte, error) {\n    // 创建Group\n    g, ctx := errgroup.WithContext(ctx)\n    paths := make(chan string)\n\n    // 第一个子任务遍历目录树，并把文件路径通过paths传给其他计算md5的子任务\n    g.Go(func() error {\n        defer close(paths)\n        // 遍历目录树\n        return filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n            if err != nil {\n                return err\n            }\n            if !info.Mode().IsRegular() {\n                return nil\n            }\n            select {\n            case paths <- path:\n            // 如果有子任务返回了error，则结束当前子任务\n            case <-ctx.Done():\n                return ctx.Err()\n            }\n            return nil\n        })\n    })\n\n    c := make(chan result)\n    const numDigesters = 20\n    // 运行多个计算md5的子任务\n    for i := 0; i < numDigesters; i++ {\n        g.Go(func() error {\n            // 从paths中读取文件路径，paths关闭则退出循环\n            for path := range paths {\n                data, err := ioutil.ReadFile(path)\n                if err != nil {\n                    return err\n                }\n                select {\n                // 写入计算结果\n                case c <- result{path, md5.Sum(data)}:\n                // 如果某个子任务返回error，结束当前子任务    \n                case <-ctx.Done():\n                    return ctx.Err()\n                }\n            }\n            return nil\n        })\n    }\n    go func() {\n       \t// 等待所有任务结束，关闭c\n        g.Wait()\n        close(c)\n    }()\n\n    m := make(map[string][md5.Size]byte)\n    // 读取计算结果\n    for r := range c {\n        m[r.path] = r.sum\n    }\n    \n    // 检查是否存在error\n    if err := g.Wait(); err != nil {\n        return nil, err\n    }\n    return m, nil\n}\n```\n\n\n\n### semaphore\n\n`semaphore`实现了一个加权信号量\n\n```go\n// 一个正在等待分配权重的请求\ntype waiter struct {\n\tn     int64 // 请求分配的权重\n\tready chan<- struct{} // 用于通知已经分配成功\n}\n\n// 创建一个加权信号量，n表示最大可获取的权重\nfunc NewWeighted(n int64) *Weighted {\n\tw := &Weighted{size: n}\n\treturn w\n}\n\ntype Weighted struct {\n\tsize    int64 // 最大权重\n\tcur     int64 // 当前已经分配的权重\n\tmu      sync.Mutex\n\twaiters list.List // 等待队列\n}\n```\n\n看一下信号量的acquire逻辑：\n\n```go\n// Acquire acquires the semaphore with a weight of n, blocking until resources\n// are available or ctx is done. On success, returns nil. On failure, returns\n// ctx.Err() and leaves the semaphore unchanged.\n//\n// If ctx is already done, Acquire may still succeed without blocking.\nfunc (s *Weighted) Acquire(ctx context.Context, n int64) error {\n\t// 加锁保护\n    s.mu.Lock()\n    // 如何剩余权重足够并且等待队列为空，直接分配\n\tif s.size-s.cur >= n && s.waiters.Len() == 0 {\n\t\ts.cur += n // 更新当前分配的权重\n\t\ts.mu.Unlock()\n\t\treturn nil\n\t}\n\t\n    // 如果请求权重超过最大限制，阻塞直到context取消，直接返回，不要加入到waiters防止阻塞其他协程\n\tif n > s.size {\n\t\ts.mu.Unlock()\n\t\t<-ctx.Done()\n\t\treturn ctx.Err()\n\t}\n\t\n    // 加入到等待队列的末尾\n\tready := make(chan struct{})\n\tw := waiter{n: n, ready: ready}\n\telem := s.waiters.PushBack(w)\n\ts.mu.Unlock()\n\t\n    // 等待上下文取消，或者信号量分配成功\n\tselect {\n\tcase <-ctx.Done():\n\t\terr := ctx.Err()\n\t\ts.mu.Lock()\n\t\tselect {\n\t\tcase <-ready:\n            // 取消的时候分配成功了，这时候忽略掉上下文取消操作\n\t\t\terr = nil\n\t\tdefault:\n            // 上下文取消，从等待队列中移除\n\t\t\ts.waiters.Remove(elem)\n\t\t}\n\t\ts.mu.Unlock()\n\t\treturn err\n\t// 成功分配\n\tcase <-ready:\n\t\treturn nil\n\t}\n}\n```\n\n接着看一下信号量释放逻辑：\n\n```go\n// Release releases the semaphore with a weight of n.\nfunc (s *Weighted) Release(n int64) {\n\ts.mu.Lock()\n    // 更新当前分配的权重\n\ts.cur -= n\n\tif s.cur < 0 {\n\t\ts.mu.Unlock()\n\t\tpanic(\"semaphore: released more than held\")\n\t}\n    \n    // 唤醒等待队列\n\tfor {\n\t\tnext := s.waiters.Front()\n\t\tif next == nil {\n\t\t\tbreak // No more waiters blocked.\n\t\t}\n\t\t\n\t\tw := next.Value.(waiter)\n\t\tif s.size-s.cur < w.n {\n\t\t\tbreak\n\t\t}\n\n\t\ts.cur += w.n\n\t\ts.waiters.Remove(next)\n\t\tclose(w.ready)\n\t}\n\ts.mu.Unlock()\n}\n```\n\n\n\n### singleflight\n\n`singleflight`提供了防止函数同一时刻重复执行的功能\n\n```go\n// call表示一个函数调用\ntype call struct {\n   wg sync.WaitGroup\n\n   // 函数返回值\n   val interface{}\n   // 返回的错误\n   err error\n\n   // forgotten indicates whether Forget was called with this call's key\n   // while the call was still in flight.\n   forgotten bool\n\n   // 表示该函数有多少次重复调用\n   dups  int\n   // 异步返回执行结果\n   chans []chan<- Result\n}\n\n// Result holds the results of Do, so they can be passed\n// on a channel.\ntype Result struct {\n   Val    interface{}\n   Err    error\n   Shared bool\n}\n\n// Group represents a class of work and forms a namespace in\n// which units of work can be executed with duplicate suppression.\ntype Group struct {\n   mu sync.Mutex       // protects m\n   m  map[string]*call // lazily initialized\n}\n\n\n```\n\n```go\n// Do executes and returns the results of the given function, making\n// sure that only one execution is in-flight for a given key at a\n// time. If a duplicate comes in, the duplicate caller waits for the\n// original to complete and receives the same results.\n// The return value shared indicates whether v was given to multiple callers.\n// 具有相同key的函数，同一时刻多次调用只会执行一次\nfunc (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) {\n\tg.mu.Lock()\n    // lazy init\n\tif g.m == nil {\n\t\tg.m = make(map[string]*call)\n\t}\n    \n    // 如果已经存在\n\tif c, ok := g.m[key]; ok {\n\t\tc.dups++\n\t\tg.mu.Unlock()\n\t\tc.wg.Wait() // 等待执行结束\n\t\treturn c.val, c.err, true // 执行返回调用结果\n\t}\n    // 创建一个新的call，加入到g.m中\n\tc := new(call)\n\tc.wg.Add(1)\n\tg.m[key] = c\n\tg.mu.Unlock()\n\n    // 同步执行函数调用\n\tg.doCall(c, key, fn)\n\treturn c.val, c.err, c.dups > 0\n}\n```\n\n\n\n```go\n// DoChan is like Do but returns a channel that will receive the\n// results when they are ready.\nfunc (g *Group) DoChan(key string, fn func() (interface{}, error)) <-chan Result {\n   ch := make(chan Result, 1)\n   g.mu.Lock()\n   if g.m == nil {\n      g.m = make(map[string]*call)\n   }\n   if c, ok := g.m[key]; ok {\n      c.dups++\n      c.chans = append(c.chans, ch)\n      g.mu.Unlock()\n      return ch\n   }\n   c := &call{chans: []chan<- Result{ch}}\n   c.wg.Add(1)\n   g.m[key] = c\n   g.mu.Unlock()\n   // 异步执行函数调用\n   go g.doCall(c, key, fn)\n\n   return ch\n}\n```\n\n```go\n// doCall handles the single call for a key.\nfunc (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) {\n\t// 调用函数\n    c.val, c.err = fn()\n    // 通知函数调用结束\n\tc.wg.Done()\n    \n    // 从g.m中移除\n\tg.mu.Lock()\n\tif !c.forgotten {\n\t\tdelete(g.m, key)\n\t}\n    // 如果存在异步调用，通知执行结果\n\tfor _, ch := range c.chans {\n\t\tch <- Result{c.val, c.err, c.dups > 0}\n\t}\n\tg.mu.Unlock()\n}\n```\n\n```go\n// Forget tells the singleflight to forget about a key.  Future calls\n// to Do for this key will call the function rather than waiting for\n// an earlier call to complete.\n// 从g.m中移除指定key的函数调用\nfunc (g *Group) Forget(key string) {\n   g.mu.Lock()\n   if c, ok := g.m[key]; ok {\n      c.forgotten = true\n   }\n   delete(g.m, key)\n   g.mu.Unlock()\n}\n```\n\n\n\n### syncmap\n\n`syncmap`提供了一个并发安全的`map`实现，已经加入到了标准库中\n\n```go\n// Map is a concurrent map with amortized-constant-time loads, stores, and deletes.\n// It is safe for multiple goroutines to call a Map's methods concurrently.\n//\n// The zero Map is valid and empty.\n//\n// A Map must not be copied after first use.\ntype Map struct {\n   mu sync.Mutex\n   // 查询时会先从read中查询，如果没有才到dirty中查询\n   read atomic.Value // readOnly\n   \n   dirty map[interface{}]*entry\n   // 记录到dirty中查询的次数，当达到一定阈值，会使用dirty作为新的read\n   misses int\n}\n\n// readOnly is an immutable struct stored atomically in the Map.read field.\ntype readOnly struct {\n\tm       map[interface{}]*entry // entry保存value\n    // 是否dirty中包含m中不存在的key\n\tamended bool // true if the dirty map contains some key not in m.\n}\n\n// An entry is a slot in the map corresponding to a particular key.\n// readOnly虽然是只读的，但是entry可以通过cas更新p字段\ntype entry struct {\n\tp unsafe.Pointer // *interface{}\n}\n```\n\n```go\n// Load returns the value stored in the map for a key, or nil if no\n// value is present.\n// The ok result indicates whether value was found in the map.\nfunc (m *Map) Load(key interface{}) (value interface{}, ok bool) {\n\t// 先尝试直接从read中查找，readOnly是只读的，因此并发访问安全\n    read, _ := m.read.Load().(readOnly)\n\te, ok := read.m[key]\n    // 如果查询不到并且dirty中包含read中不存在的key，则到dirty中查找\n\tif !ok && read.amended {\n        // 需要加锁\n\t\tm.mu.Lock()\n\t\t// 首先先再次从read中查找一遍，防止加锁过程中，其他协程触发了read的更新\n\t\tread, _ = m.read.Load().(readOnly)\n\t\te, ok = read.m[key]\n        // 如果read中没有，并且dirty包含read中没有的key，从dirty中查找\n\t\tif !ok && read.amended {\n\t\t\te, ok = m.dirty[key]\n\t\t\t// 更新misses字段，并且如果达到阈值，则更新read为dirty\n\t\t\tm.missLocked()\n\t\t}\n\t\tm.mu.Unlock()\n\t}\n\tif !ok {\n\t\treturn nil, false\n\t}\n\treturn e.load()\n}\n```\n\n```go\n// Store sets the value for a key.\nfunc (m *Map) Store(key, value interface{}) {\n    // 首先判断是否read包含要更新的key\n\tread, _ := m.read.Load().(readOnly)\n    // 更新对应的entry，tryStore使用cas操作，保证并发安全\n\tif e, ok := read.m[key]; ok && e.tryStore(&value) {\n\t\treturn\n\t}\n    \n\t// 如果read中没有，则保存到dirty中\n\tm.mu.Lock()\n    // 首先再次检查一下read\n\tread, _ = m.read.Load().(readOnly)\n\tif e, ok := read.m[key]; ok {\n        // read中的val已经被删除了，同时保存到dirty中\n\t\tif e.unexpungeLocked() {\n\t\t\t// The entry was previously expunged, which implies that there is a\n\t\t\t// non-nil dirty map and this entry is not in it.\n\t\t\tm.dirty[key] = e\n\t\t}\n\t\te.storeLocked(&value)\n        // if m.dirty == nil, then ok == false\n\t} else if e, ok := m.dirty[key]; ok {\n\t\te.storeLocked(&value)\n\t} else {\n        // dirty中没有包含read中没有的key，但是read中可能包含dirty中没有的key\n        // 这时候的dirty应该还没有初始化\n\t\tif !read.amended {\n            // 初始化dirty，并将read中没有被标记为删除的kv拷贝到dirty中\n \t\t\tm.dirtyLocked()\n            // 更新read，应该readOnly是只读的，这里重新创建一个readOnly\n\t\t\tm.read.Store(readOnly{m: read.m, amended: true})\n\t\t}\n        // 把新的kv保存到dirty中\n\t\tm.dirty[key] = newEntry(value)\n\t}\n\tm.mu.Unlock()\n}\n```\n\n```go\n// Delete deletes the value for a key.\nfunc (m *Map) Delete(key interface{}) {\n\t// 先尝试直接从read中查找\n    read, _ := m.read.Load().(readOnly)\n\te, ok := read.m[key]\n    // read中不存在，并且可能在dirty中\n\tif !ok && read.amended {\n\t\tm.mu.Lock()\n        // 再次检查read\n\t\tread, _ = m.read.Load().(readOnly)\n\t\te, ok = read.m[key]\n\t\tif !ok && read.amended {\n            // 直接从dirty中删除\n\t\t\tdelete(m.dirty, key)\n\t\t}\n\t\tm.mu.Unlock()\n\t}\n    // read中存在，直接标记为已经删除\n\tif ok {\n\t\te.delete()\n\t}\n}\n```\n\n```go\nfunc (m *Map) Range(f func(key, value interface{}) bool) {\n\tread, _ := m.read.Load().(readOnly)\n\t// dirty中包含read中不存在的kv\n    if read.amended {\n\t\tm.mu.Lock()\n\t\tread, _ = m.read.Load().(readOnly)\n\t\tif read.amended {\n\t\t\t// 替换read为dirty\n            read = readOnly{m: m.dirty}\n\t\t\tm.read.Store(read)\n\t\t\tm.dirty = nil\n\t\t\tm.misses = 0\n\t\t}\n\t\tm.mu.Unlock()\n\t}\n    \n\t// 遍历read\n\tfor k, e := range read.m {\n\t\tv, ok := e.load()\n        // 如果已经标记为删除，跳过\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif !f(k, v) {\n\t\t\tbreak\n\t\t}\n\t}\n}\n```\n\n\n\n","tags":["go"]},{"title":"map中的hash","url":"/2019/06/22/map中的hash/","content":"\n### 问题引入\n\n```go\ntype Key struct {\n\ta     int\n\tIface interface{}\n}\n\nfunc main() {\n\tm := map[Key]bool{}\n\tm[Key{a: 10, Iface: 10}] = true // 运行通过\n\tm[Key{a: 10, Iface: map[string]interface{}{}}] = true // panic\n}\n```\n\n运行上面的代码，我们会得到一个panic：\n\n```sh\npanic: runtime error: hash of unhashable type map[string]interface {}\n```\n\n原因是因为，当使用map存储时，`key`要求是可以计算`hash`值的，而在`go`中，`map`、`slice`、`channel`和`func`类型都是不能计算hash值的，因此当使用这些类型作为`map`的`key`时会发生panic，而当key是复合类型，并且包含了这四种类型，也会发生panic，如上面的例子中，如果包含了接口类型，那么在运行时会动态检查其类型。\n\n### 具体实现\n\n接下来，我们来看一下其背后的实现逻辑。\n\n首先，我们先看一下map中关于hash的计算逻辑：\n\n```go\nalg := t.key.alg // 这里的key是maptype\nhash := alg.hash(key, uintptr(h.hash0)) \n```\n\n接下来看一下`maptype`的结构定义：\n\n```go\ntype maptype struct {\n\ttyp        _type\n\tkey        *_type\n\telem       *_type\n\tbucket     *_type // internal type representing a hash bucket\n\tkeysize    uint8  // size of key slot\n\tvaluesize  uint8  // size of value slot\n\tbucketsize uint16 // size of bucket\n\tflags      uint32\n}\n\ntype _type struct {\n\tsize       uintptr\n\tptrdata    uintptr // size of memory prefix holding all pointers\n\thash       uint32\n\ttflag      tflag\n\talign      uint8\n\tfieldalign uint8\n\tkind       uint8\n\talg        *typeAlg \n\t// gcdata stores the GC type data for the garbage collector.\n\t// If the KindGCProg bit is set in kind, gcdata is a GC program.\n\t// Otherwise it is a ptrmask bitmap. See mbitmap.go for details.\n\tgcdata    *byte\n\tstr       nameOff\n\tptrToThis typeOff\n}\n\ntype typeAlg struct {\n\t// function for hashing objects of this type\n\t// (ptr to object, seed) -> hash\n\thash func(unsafe.Pointer, uintptr) uintptr\n\t// function for comparing objects of this type\n\t// (ptr to object A, ptr to object B) -> ==?\n\tequal func(unsafe.Pointer, unsafe.Pointer) bool\n}\n```\n\n实际上，每个类型都会包含`_type`，而`_type`中的`alg`字段则定义了该类型对象的`hash`和`equal`方法\n\n而在`runtime`包中，已经声明了一些基本的`alg`：\n\n```go\nvar algarray = [alg_max]typeAlg{\n\talg_NOEQ:     {nil, nil}, // 表示没有hash方法和equal方法 \n\talg_MEM0:     {memhash0, memequal0},\n\talg_MEM8:     {memhash8, memequal8},\n\talg_MEM16:    {memhash16, memequal16},\n\talg_MEM32:    {memhash32, memequal32},\n\talg_MEM64:    {memhash64, memequal64},\n\talg_MEM128:   {memhash128, memequal128},\n\talg_STRING:   {strhash, strequal},\n\talg_INTER:    {interhash, interequal}, // 用于计算iface类型\n\talg_NILINTER: {nilinterhash, nilinterequal}, // 用于计算eface类型\n\talg_FLOAT32:  {f32hash, f32equal},\n\talg_FLOAT64:  {f64hash, f64equal},\n\talg_CPLX64:   {c64hash, c64equal},\n\talg_CPLX128:  {c128hash, c128equal},\n}\n```\n\n我们就看其中的`nilinterhash`的实现：\n\n```go\nfunc nilinterhash(p unsafe.Pointer, h uintptr) uintptr {\n\ta := (*eface)(p) // p实际上就是一个空接口指针\n\tt := a._type \n\tif t == nil { \n\t\treturn h\n\t}\n\tfn := t.alg.hash \n\tif fn == nil { // 如果没有指定hash方法，表明该类型不支持计算hash\n        // 这里的panic不就跟上面例子中的panic一样嘛\n\t\tpanic(errorString(\"hash of unhashable type \" + t.string()))\n\t}\n\tif isDirectIface(t) {\n\t\treturn c1 * fn(unsafe.Pointer(&a.data), h^c0)\n\t} else {\n\t\treturn c1 * fn(a.data, h^c0)\n\t}\n}\n```\n\n接下来，我们看一下文章开头例子反编译后的汇编代码，我们主要是要看一下`Key`类型的`hash`方法\n\n首先，看一下`Key`类型的`type`：\n\n```go\ntype.\"\".Key SRODATA size=144\n\t0x0000 18 00 00 00 00 00 00 00 18 00 00 00 00 00 00 00  ................\n\t0x0010 18 cb 58 54 07 08 08 19 00 00 00 00 00 00 00 00  ..XT............\n\t0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0040 02 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00  ................\n\t0x0050 00 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00  ........@.......\n\t0x0060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0080 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00  ................\n\trel 24+8 t=1 type..alg.\"\".Key+0 // 24+8偏移量对应的就是_type.alg\n\trel 32+8 t=1 runtime.gcbits.04+0\n\trel 40+4 t=5 type..namedata.*main.Key.+0\n\trel 44+4 t=5 type.*\"\".Key+0\n\trel 48+8 t=1 type..importpath.\"\".+0\n\trel 56+8 t=1 type.\"\".Key+96\n\trel 80+4 t=5 type..importpath.\"\".+0\n\trel 96+8 t=1 type..namedata.a-+0\n\trel 104+8 t=1 type.int+0\n\trel 120+8 t=1 type..namedata.Iface.+0\n\trel 128+8 t=1 type.interface {}+0\n\n// key的alg包含了两个字段，hashfunc和eqfunc\ntype..alg.\"\".Key SRODATA dupok size=16\n\t0x0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\trel 0+8 t=1 type..hashfunc.\"\".Key+0 \n\trel 8+8 t=1 type..eqfunc.\"\".Key+0\n\ntype..hashfunc.\"\".Key SRODATA dupok size=8\n\t0x0000 00 00 00 00 00 00 00 00                          ........\n\trel 0+8 t=1 type..hash.\"\".Key+0\n```\n\n接着来看一下汇编生成的`type..hash.\"\".Key`方法，只保留主要逻辑\n\n```assembly\n// 首先该函数有两个参数p和h，p表示要hash的对象指针，h表示hash seek，返回一个hash值\nTEXT\ttype..hash.\"\".Key(SB), DUPOK|ABIInternal, $40-24\n\tMOVQ\t\"\".p+48(SP), AX // 将要hash的镀锡指针mov到ax\n\tMOVQ\tAX, (SP) // memhash第一个参数\n\tMOVQ\t\"\".h+56(SP), CX\n\tMOVQ\tCX, 8(SP) // memhash第二个参数\n\tMOVQ\t$8, 16(SP) // memhash第三个参数，这里8表示只对前8个字节进行hash计算\n\tCALL\truntime.memhash(SB) // 调用memhash计算hash值\n\tMOVQ\t24(SP), AX // 将返回值保存到ax中\n\tMOVQ\t\"\".p+48(SP), CX\n\tADDQ\t$8, CX // 这里指针计算，p+8对应的就是Key.Iface\n\tMOVQ\tCX, (SP) // nilinterhash第一个参数\n\tMOVQ\tAX, 8(SP) // 将memhash的第一个参数作为nilinterhash第二个参数\n\tCALL\truntime.nilinterhash(SB) // 调用nilinterhash\n\tMOVQ\t16(SP), AX // 返回值mov到ax\n\tMOVQ\tAX, \"\".~r2+64(SP) // 设置返回值\n\tRET\n```\n\n直接看上面的逻辑，当计算`Key`的`hash`时，会先计算前8个字节的hash值，然后在调用`nilinterhash`来计算`Iface`的hash值。\n\n那么开头我们的例子中，运行的结果是`panic`，因此说明在`nilinterhash`中，因此对应的type没有hash方法而导致panic，为了验证我们的结论，我们看下`map[string]interface{}{}`对应的`_type`中的`alg`\n\n```go\nfunc main() {\n\tm := map[string]interface{}{}\n\tm[\"a\"] = nil\n}\n```\n\n查看编译后的汇编代码：\n\n```go\ntype.map[string]interface {} SRODATA dupok size=80\n\t0x0000 08 00 00 00 00 00 00 00 08 00 00 00 00 00 00 00  ................\n\t0x0010 86 62 71 0e 02 08 08 35 00 00 00 00 00 00 00 00  .bq....5........\n\t0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0040 00 00 00 00 00 00 00 00 10 10 10 01 0c 00 00 00  ................\n\t// 看这里，runtime.algarray对应就是runtime.algarray[0]\n\t// 这个数组上面提到过，algarray[0]对应的hash和eq都是nil\n\trel 24+8 t=1 runtime.algarray+0\n\trel 32+8 t=1 runtime.gcbits.01+0\n\trel 40+4 t=5 type..namedata.*map[string]interface {}-+0\n\trel 44+4 t=6 type.*map[string]interface {}+0\n\trel 48+8 t=1 type.string+0\n\trel 56+8 t=1 type.interface {}+0\n\trel 64+8 t=1 type.noalg.map.bucket[string]interface {}+0\n```\n\n","tags":["go"]},{"title":"ast","url":"/2019/06/19/ast/","content":"\n# go ast\n\n`go`的官方库提供了几个包，可以帮我们解析`go`的源文件，主要有：\n\n- [go/scanner](<https://github.com/golang/go/blob/master/src/go/scanner/scanner.go>)：词法解析，将源代码分割成一个个token\n- [go/token](<https://github.com/golang/go/blob/master/src/go/token/token.go>)：token类型及相关结构体定义\n- [go/ast](<https://github.com/golang/go/blob/master/src/go/ast/ast.go>)：ast的结构定义\n- [go/parser](<https://github.com/golang/go/blob/master/src/go/parser/parser.go#L17>)：语法分析，读取token流生成ast\n\n\n\n通过解析源文件，我们可以得到`ast`(抽象语法树)。\n\n而通过遍历`ast`，我们可以得到源码中声明的结构体、方法、类型等等信息，并根据实际需要[生成具体的代码](<https://github.com/ymcvalu/enhance/tree/master/enhance>)，比如自动生成`tag`，模板方法、手动实现泛型效果等。而且，go的注释在解析时是可以保留的，这就可以实现`java`中类似`annotation`的功能，比如根据注释自动生成接口文档（[beego的swagger文档生成](<https://github.com/beego/bee/blob/develop/generate/swaggergen/g_docs.go>)），根据注释提取接口权限信息实现统一权限校验等。\n\n\n\n解析过程：\n\n词法分析，将源代码分割成一个个token -> 语法分析，根据[go语言的文法](<https://golang.org/ref/spec>)对token流进行规约/推导 -> 生成ast\n\n\n\n### ast: 抽象语法树\n\nast是源代码结构的一种抽象表示，以树状形式来表达编程语言的语法结构。\n\n比如表达式 `a+b`，对应的ast为：\n\n![](/img/ast1.png)\n\n对应使用go表示的结构：\n\n```go\n*ast.BinaryExpr { // a+b是一个二元表达式\n.  X: *ast.Ident { // X表示第一个操作数\n.  .  Name: \"a\"\n.  .  }\n.  }\n.  Op: + // 操作符\n.  Y: *ast.Ident { // Y表示第二个操作数\n.  .  Name: \"b\"\n.  }\n}\n```\n\n\n\n### 源码解析\n\n首先要知道具体的接口怎么用，才知道源码从哪个入口开始看是吧\n\n```go\npackage main\n\nimport (\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"log\"\n)\n\nfunc main() {\n\t// 创建FileSet\n\tfset := token.NewFileSet()\n\t// 解析源文件main.go，返回ast.File代表一个源文件的node\n\tf, err := parser.ParseFile(fset, \"./main.go\", nil, parser.ParseComments)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// 打印AST\n\tast.Print(fset, f)\n}\n```\n\n首先来看第12行代码，这里创建了一个`FileSet`，顾名思义，`FileSet`就是源文件集合，因为我们一次解析可能不止解析一个文件，而是一系列文件。\n\n`FileSet`最主要的用途是用来保存`token`的位置信息，每个token在当前文件的位置可以用行号，列号，token在当前文件中的偏移量这三个属性来描述，使用[`Position`](<https://github.com/golang/go/blob/master/src/go/token/position.go#L20>)这个结构体来描述，`FileSet`中保存所有`token`的`Position`信息，而在`ast`中，只保存一个[`Pos`](<https://github.com/golang/go/blob/master/src/go/token/position.go#L76>)索引。当遍历`ast`的时候，我们需要使用`Pos`索引向[`FileSet`]()获取`Position`。\n\n现在来看一下14行`parser.ParseFile`这个方法，这个方法实现了语法分析：\n```go\nfunc ParseFile(fset *token.FileSet, filename string, src interface{}, mode Mode) (f *ast.File, err error) {\n\t// 必须要传入fset，用来保存Position信息\n    if fset == nil {\n\t\tpanic(\"parser.ParseFile: no token.FileSet provided (fset == nil)\")\n\t}\n\n\t// 读取源文件，如果src不为空，则从src读取，否则读取filename指定的文件\n\ttext, err := readSource(filename, src)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar p parser \n\tdefer func() {\n\t\t...\n\t}()\n\n\t// parse source\n\tp.init(fset, filename, text, mode) // 初始化parser\n\tf = p.parseFile() // 解析源文件，生成AST\n\n\treturn\n}\n```\n\n先来简单看一下`parser.init`方法：\n\n```go\nfunc (p *parser) init(fset *token.FileSet, filename string, src []byte, mode Mode) {\n\t// 添加当前文件到FileSet中\n    p.file = fset.AddFile(filename, -1, len(src))\n\tvar m scanner.Mode\n    // 设置scanner的mode，如果指定了ast需要保留注释，那么词法解析的时候需要解析注释\n\tif mode&ParseComments != 0 {\n\t\tm = scanner.ScanComments\n\t}\n    // 错误处理\n\teh := func(pos token.Position, msg string) { p.errors.Add(pos, msg) }\n\t// 初始化词法解析器\n    p.scanner.Init(p.file, src, eh, m)\n\tp.mode = mode\n\tp.trace = mode&Trace != 0 // for convenience (p.trace is used frequently)\n\t// parser.next会前进到下一个非注释的token，其中注释会被保留\n\tp.next()\n}\n```\n\n其中，注释有两种：\n\n```go\n// this\n// is\n// doc\nfunc foo(){\n    \n}\n```\n\n```go\nvar globalNum int // this is a comment\n```\n\n第一种是注释独自自己占一到多行的，后一种则是跟语句在同一行。`parser.next`方法中，读取`token`时，如果遇到第一种注释，会保存到`parser.leadComment`，如果是第二种注释，则保存到`parser.lineComment`中，最终会保留到具体的`ast`中的节点中。\n\n接着来看一下[`parser.parseFile`](<https://github.com/golang/go/blob/master/src/go/parser/parser.go#L2500>)方法\n\n```go\nfunc (p *parser) parseFile() *ast.File {\n\t// 如果执行parser.next时有错误发生\n\tif p.errors.Len() != 0 {\n\t\treturn nil\n\t}\n\n\t// package clause\n\tdoc := p.leadComment // package前面的注释被认为是当前文件的doc\n    // 期待第一个token是`package`关键字，该方法内会执行parser.next方法，前进到下一个token\n\tpos := p.expect(token.PACKAGE) \n\t// 解析当前的token为标识符，也就是包名\n\tident := p.parseIdent()\n\tif ident.Name == \"_\" && p.mode&DeclarationErrors != 0 {\n\t\tp.error(p.pos, \"invalid package name _\")\n\t}\n    // 读取`;`，如果没有的话，需要插入一个`;`\n    // 也就是说go会自动在语句末尾插入`;`\n\tp.expectSemi()\n\n\t// 如果前面解析标识符时失败\n\tif p.errors.Len() != 0 {\n\t\treturn nil\n\t}\n\n    // 设置topScope\n    // scope用于保存当前作用域内声明的符号引用，比如声明的方法、类型或常/变量等\n\tp.openScope()\n    // 设置包作用域\n\tp.pkgScope = p.topScope\n    // 一个源文件是由一系列声明组成的:\n    // import声明\n    // 方法声明\n    // 类型声明\n    // 全局常量/变量声明\n    // 这里的ast.Decl是这些声明的公共接口\n\tvar decls []ast.Decl\n\t// 如果不是只解析包名\n    if p.mode&PackageClauseOnly == 0 {\n\t\t// 解析导入声明\n        // 确保当前token的`import`\n\t\tfor p.tok == token.IMPORT {\n            // p.parserImportSpec解析具体的导入声明\n\t\t\tdecls = append(decls, p.parseGenDecl(token.IMPORT, p.parseImportSpec))\n\t\t}\n\t\t// 如果不是只解析导入声明\n\t\tif p.mode&ImportsOnly == 0 {\n\t\t\t// 解析源代码后面的其他内容\n\t\t\tfor p.tok != token.EOF {\n\t\t\t\tdecls = append(decls, p.parseDecl(declStart))\n\t\t\t}\n\t\t}\n\t}\n    \n    // 关闭作用域\n\tp.closeScope()\n    // 确保topScope为nil，否则说明有多余的`{}`没有匹配\n\tassert(p.topScope == nil, \"unbalanced scopes\")\n\tassert(p.labelScope == nil, \"unbalanced label scopes\")\n\n\t// resolve global identifiers within the same file\n\ti := 0\n    // 在包作用域内查找未解析的符号引用，比如在方法内引用了全局的方法，变量等\n\tfor _, ident := range p.unresolved {\n\t\t// i <= index for current ident\n\t\tassert(ident.Obj == unresolved, \"object already resolved\")\n\t\tident.Obj = p.pkgScope.Lookup(ident.Name) // also removes unresolved sentinel\n        // 有的是在同一个包的其他文件中声明的\n\t\tif ident.Obj == nil {\n\t\t\tp.unresolved[i] = ident\n\t\t\ti++\n\t\t}\n\t}\n\n\treturn &ast.File{\n\t\tDoc:        doc,\n\t\tPackage:    pos,\n\t\tName:       ident,\n\t\tDecls:      decls,\n\t\tScope:      p.pkgScope,\n\t\tImports:    p.imports,\n\t\tUnresolved: p.unresolved[0:i],\n\t\tComments:   p.comments,\n\t}\n}\n```\n上面的`decl`，包括全局的变量声明，类型声明，函数声明等，具体就不展开了。\n\n### 来个例子\n\n现在来实现一个自动生成`tag`的例子\n\n```go\nfunc main() {\n    // 使用时需要传入目标源代码路径，目标结构体包含的某个行号和列号\n\targs := os.Args[:len(os.Args)]\n\tif len(args) < 4 {\n\t\tlog.Fatal(\"参数：文件路径，行号，列号\")\n\t}\n\tfpath := args[1]\n\tlineNum, err := strconv.Atoi(args[2])\n\tif err != nil {\n\t\tlog.Fatal(\"incorrect line number\")\n\t}\n\t// columnNum, err := strconv.Atoi(args[3])\n\tif err != nil {\n\t\tlog.Fatal(\"incorrect column number\")\n\t}\n\t// 创建FileSet\n\tfset := token.NewFileSet()\n\n\t// 解析源文件\n\tf, err := parser.ParseFile(fset, fpath, nil, parser.ParseComments)\n\tif err != nil {\n\t\tlog.Fatal(\"failed to parse file: \", err.Error())\n\t}\n\t// 全局变量，用来保存找到的目标结构体声明的具体node\n\tvar target *ast.StructType\n    // 使用Inspect方法遍历ast\n\tast.Inspect(f, func(node ast.Node) bool {\n\t\t// 如果不是结构体类型声明，跳过，继续下一个遍历\n\t\tst, ok := node.(*ast.StructType)\n        // 如果不是结构体类型或者类型声明未完成\n\t\tif !ok || st.Incomplete {\n\t\t\treturn true\n\t\t}\n\t\t// 如果是结构体声明，需要包含指定的行和列，这里实际上只要包含实际行就行\n\t\tbegin := fset.Position(st.Pos())\n\t\tend := fset.Position(st.End())\n\n\t\t// 找到目标struct，返回false，结束遍历\n\t\tif begin.Line <= lineNum && end.Line >= lineNum {\n\t\t\ttarget = st // 设置目标target\n \t\t\treturn false\n\t\t}\n\n\t\treturn true\n\t})\n\n    // 如果找到了目标trget\n\tif target != nil {\n        // 生成tag，因为结构体声明是可以嵌套的，该方法会递归调用\n\t\tgenTag(target)\n        // 打开目标文件\n\t\tfd, err := os.OpenFile(fpath, os.O_TRUNC|os.O_RDWR, 0777)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\tdefer fd.Close()\n        // 使用format.Node方法将ast转换为源文件\n\t\terr = format.Node(fd, fset, f)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n}\n```\n\n接着来看一下`genTag`方法，该放方法主要就是遍历声明的字段，为其生成tag然后设置到ast中对应的node上\n\n```go\nfunc genTag(st *ast.StructType) {\n    // 遍历结构体声明的字段列表\n\tfs := st.Fields.List\n\tfor i := range fs {\n\t\tvar (\n\t\t\ttag string\n\t\t)\n\n\t\tfd := fs[i]\n        // 如果有指定字段名\n\t\tif len(fd.Names) > 0 {\n\t\t\tname := fd.Names[0].Name\n            // 只有导出字段才需要生成tag\n\t\t\tif !isExport(name) {\n\t\t\t\tcontinue\n\t\t\t}\n            // 根据字段名生成tag中的名字，比如NodeId变成node_id\n\t\t\ttag = genKey(name)\n\t\t}\n\t\t// 判断字段的类型\n\t\tswitch t := fd.Type.(type) {\n        // 如果是标识符标识引用了其他声明类型\n\t\tcase *ast.Ident:\n            // 如果tag==\"\"表示没有字段名，这时候默认字段名就是类型名\n            // 如果类型导出，则生成tag\n\t\t\tif tag == \"\" && isExport(t.Name) {\n\t\t\t\ttag = genKey(t.Name)\n\t\t\t}\n        // 嵌套结构体声明\n\t\tcase *ast.StructType:\n\t\t\t// 递归生成tag\n            genTag(t)\n\t\t}\n\n\t\tvar tagStr string\n        // 获取原来的tag\n\t\tif fd.Tag != nil {\n\t\t\ttagStr = fd.Tag.Value\n\t\t}\n\t\n        // 解析tag字符串：`json:\"sdf\" form:\"sdf\"`成tag切片\n\t\ttags, err := parseTag(tagStr)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t\n\t\tchange := false\n\t\t// 如果已经存在json这个tag，则跳过自动生成\n        if _, ok := tags.Lookup(\"json\"); !ok {\n\t\t\ttags.Append(\"json\", tag)\n\t\t\tchange = true\n\t\t}\n        // 如果已经生成form这个tag，跳过\n\t\tif _, ok := tags.Lookup(\"form\"); !ok {\n\t\t\ttags.Append(\"form\", tag)\n\t\t\tchange = true\n\t\t}\n\t\n        // 如果自动生成了tag\n\t\tif change {\n            // 根据新的tag切片生成tag字符串\n\t\t\ttagStr = tags.TagStr()\n\t\t\tif fd.Tag == nil {\n\t\t\t\tfd.Tag = &ast.BasicLit{}\n\t\t\t}\n            // 设置到目标node中\n\t\t\tfd.Tag.Kind = token.STRING\n\t\t\tfd.Tag.Value = tagStr\n\t\t}\n\t}\n}\n```\n\n[完整代码](<https://github.com/ymcvalu/auto-tag>)","tags":["golang"]},{"title":"协程抢占","url":"/2019/05/25/协程抢占/","content":"\n\n\n# 协程抢占\n\n### 协作式抢占\n\n##### morestack：执行抢占\n\n目前`go`实现是的协作抢占：在每个函数开头插入`morestack`检查，除了检查是否需要扩张栈，同时还检查是否当前协程需要抢占。那么怎么判断一个协程是否需要抢占呢？后台线程会定时扫描当前运行中的协程，如果发现一个协程运行比较久，会将其标记为抢占状态。\n\n我们首先看一下一个`main`函数的汇编代码：\n\n```go\n\t0x0000 00000 (morestack.go:5)\tTEXT\t\"\".main(SB), $64-0\n\t0x0000 00000 (morestack.go:5)\tMOVQ\tTLS, CX\n\t0x0009 00009 (morestack.go:5)\tMOVQ\t(CX)(TLS*2), CX // 获取当前g\n\t// 比较当前SP和g.stackguard0，如果小于则需要触发morestarck\n\t0x0010 00016 (morestack.go:5)\tCMPQ\tSP, 16(CX) \n\t0x0014 00020 (morestack.go:5)\tJLS\t110\n\t0x0016 00022 (morestack.go:5)\tSUBQ\t$64, SP    // SP-64，相当于设置栈帧大小64字节\n\t0x001a 00026 (morestack.go:5)\tMOVQ\tBP, 56(SP) // 保存caller的BP，可以看到BP是保存到当前函数的栈帧中的，如果一个函数栈帧大小为0，则不需要保存BP\n\t0x001f 00031 (morestack.go:5)\tLEAQ\t56(SP), BP // 设置当前BP\n    ...\n\t0x0064 00100 (morestack.go:7)\tMOVQ\t56(SP), BP // 还原BP\n\t0x0069 00105 (morestack.go:7)\tADDQ\t$64, SP    // SP+64，相当于销毁栈帧\n\t0x006d 00109 (morestack.go:7)\tRET\n\t0x006e 00110 (morestack.go:7)\tNOP\n\t0x006e 00110 (morestack.go:5)\tCALL\truntime.morestack_noctxt(SB)\n\t0x0073 00115 (morestack.go:5)\tJMP\t0\n```\n\n我们可以看到，进入函数之后，首先会检查当前函数的`SP`寄存器是否已经达到`g.stackguard0`，如果是的话，则需要先调用`runtime.morestack_noctxt`方法扩张当前函数栈（现在的实现是重新分配一个更大的函数栈，然后把旧的函数栈内容拷贝过去），然后再根据栈帧大小设置`SP`和`BP`指针，而在函数返回前需要先恢复`BP`和`SP`指针。**上面的BP和SP寄存器的相关设置是在morestack之后，也就是在执行morestack的时候，0(SP)为函数返回地址**\n\n我们接着来看一下`runtime·morestack_noctxt`的实现：\n\n```go\n// morestack but not preserving ctxt.\n// 这里noctxt表示调用方法没有context，即没有闭包或者receiver\nTEXT runtime·morestack_noctxt(SB),NOSPLIT,$0\n\tMOVL\t$0, DX // 清空DX的低32位，DX寄存器用于保存函数上下文\n\tJMP\truntime·morestack(SB) // 跳转到morestack方法，这里用的是JMP，不是CALL\n\n/*\n * support for morestack\n */\n\n// Called during function prolog when more stack is needed.\n//\n// The traceback routines see morestack on a g0 as being\n// the top of a stack (for example, morestack calling newstack\n// calling the scheduler calling newm calling gc), so we must\n// record an argument size. For that purpose, it has no arguments.\nTEXT runtime·morestack(SB),NOSPLIT,$0-0\n\t// Cannot grow scheduler stack (m->g0).\n    get_tls(CX)            // 这里get_tls(r)是一个宏：MOVQ TLS, r \n\tMOVQ\tg(CX), BX      // 保存当前的g到BX\n\tMOVQ\tg_m(BX), BX    // 保存m到BX\n\tMOVQ\tm_g0(BX), SI   // 保存g0到SI\n\tCMPQ\tg(CX), SI // 如果当前处于g0栈\n\tJNE\t3(PC)  // PC+3\n\tCALL\truntime·badmorestackg0(SB) // g0栈不允许扩张\n\tCALL\truntime·abort(SB)\n\n\t// Cannot grow signal stack (m->gsignal).\n\tMOVQ\tm_gsignal(BX), SI  // gsignal用于处理信号量的栈\n\tCMPQ\tg(CX), SI\n\tJNE\t3(PC)\n\tCALL\truntime·badmorestackgsignal(SB) // gsignal栈不允许扩张\n\tCALL\truntime·abort(SB)\n\n\t// Called from f：把调用morestack的函数记为f\n\t// 保存 f's caller的信息到m.morebuf中\n    // 8(SP)保存f的返回地址，即f's caller的PC\n\tMOVQ\t8(SP), AX\n\tMOVQ\tAX, (m_morebuf+gobuf_pc)(BX) // 设置m.morebuf.pc为f's caller的PC\n    // 16(SP)的地址为f's caller的SP\n\tLEAQ\t16(SP), AX\t// f's caller's SP\n\tMOVQ\tAX, (m_morebuf+gobuf_sp)(BX)\n\tget_tls(CX)\n\tMOVQ\tg(CX), SI\n\tMOVQ\tSI, (m_morebuf+gobuf_g)(BX)\n\n\t// Set g->sched to context in f.\n    MOVQ\t0(SP), AX // f's PC，morestack的frameSize为0，此时0(SP)为f的返回地址\n\tMOVQ\tAX, (g_sched+gobuf_pc)(SI) // 设置g.sched.pc为f的PC\n\tMOVQ\tSI, (g_sched+gobuf_g)(SI)\n    // 8(SP)的地址即为f的SP\n\tLEAQ\t8(SP), AX // f's SP\n\tMOVQ\tAX, (g_sched+gobuf_sp)(SI) // 保存f的SP\n\tMOVQ\tBP, (g_sched+gobuf_bp)(SI) // 保存f的BP\n\tMOVQ\tDX, (g_sched+gobuf_ctxt)(SI)  // 保存f的DX\n\n\t// Call newstack on m->g0's stack.\n\tMOVQ\tm_g0(BX), BX  // 获取g0\n\tMOVQ\tBX, g(CX)     // 设置当前g为g0\n\tMOVQ\t(g_sched+gobuf_sp)(BX), SP // 设置SP寄存器为g0.sched.sp\n\tCALL\truntime·newstack(SB)       // 调用newstack，该方法不会返回\n\tCALL\truntime·abort(SB)\t// crash if newstack returns\n\tRET\n```\n\n我们接着来看一下`runtime.newstack`这个方法：\n\n```go\nfunc newstack() {\n\tthisg := getg() // 这里获取的是当前执行的g，实际就是g0\n\t...\n    // 这里的curg是触发了morestack的g，不是g0\n\tgp := thisg.m.curg\n\t...\n\tmorebuf := thisg.m.morebuf\n\tthisg.m.morebuf.pc = 0\n\tthisg.m.morebuf.lr = 0\n\tthisg.m.morebuf.sp = 0\n\tthisg.m.morebuf.g = 0\n\n    // 检查是否需要抢占，当发现一个协程需要被抢占时，会将其g.stackguard0设置成stackPreempt，从而触发morestack的执行\n\tpreempt := atomic.Loaduintptr(&gp.stackguard0) == stackPreempt\n    // 触发了抢占\n \tif preempt {\n        // We are interested in preempting user Go code, not runtime code.\n\t\t// If we're holding locks, mallocing, or preemption is disabled, don't\n\t\t// preempt.\n\t\tif thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != \"\" || thisg.m.p.ptr().status != _Prunning {\n\t\t\t// 还原gp.stackguard0\n            // gp->preempt is set, so it will be preempted next time.\n\t\t\tgp.stackguard0 = gp.stack.lo + _StackGuard\n            // restore state from Gobuf; longjmp\n\t\t\tgogo(&gp.sched) // never return\n\t\t}\n\t}\n\n\tsp := gp.sched.sp\n\tif sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 || sys.ArchFamily == sys.WASM {\n\t\t// The call to morestack cost a word.\n\t\tsp -= sys.PtrSize\n\t}\n \t\n    // 再次检查抢占\n\tif preempt {\n        // g0不允许被抢占\n\t\tif gp == thisg.m.g0 {\n\t\t\tthrow(\"runtime: preempt g0\")\n\t\t}\n\t\tif thisg.m.p == 0 && thisg.m.locks == 0 {\n\t\t\tthrow(\"runtime: g is running but p is not\")\n\t\t}\n\t\t// Synchronize with scang.\n        // 更新状态为_Gwaiting\n\t\tcasgstatus(gp, _Grunning, _Gwaiting)\n\t\t// gc相关，抢占g扫描\n        if gp.preemptscan {\n\t\t\tfor !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) {\n\t\t\t\t// Likely to be racing with the GC as\n\t\t\t\t// it sees a _Gwaiting and does the\n\t\t\t\t// stack scan. If so, gcworkdone will\n\t\t\t\t// be set and gcphasework will simply\n\t\t\t\t// return.\n\t\t\t}\n\t\t\tif !gp.gcscandone {\n\t\t\t\t// gcw is safe because we're on the\n\t\t\t\t// system stack.\n\t\t\t\tgcw := &gp.m.p.ptr().gcw\n                // 扫描gp的栈\n\t\t\t\tscanstack(gp, gcw)\n\t\t\t\tif gcBlackenPromptly {\n\t\t\t\t\tgcw.dispose()\n\t\t\t\t}\n\t\t\t\tgp.gcscandone = true\n\t\t\t}\n\t\t\tgp.preemptscan = false\n\t\t\tgp.preempt = false\n\t\t\tcasfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting)\n\t\t\t// This clears gcscanvalid.\n\t\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n\t\t\tgp.stackguard0 = gp.stack.lo + _StackGuard\n\t\t\tgogo(&gp.sched) // never return\n\t\t}\n\n\t\t// Act like goroutine called runtime.Gosched.\n\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n        // 这里执行抢占，实际上就是调用schedule方法，该方法不会返回\n\t\tgopreempt_m(gp) // never return\n\t}\n\n\t// 如果不是由于抢占而执行morestack，那么就是真的因为栈不够用了，需要扩容栈\n\toldsize := gp.stack.hi - gp.stack.lo\n\tnewsize := oldsize * 2 // 新的栈是原来的两倍\n\tif newsize > maxstacksize { // 栈是有限制的哦：1<<20\n\t\tprint(\"runtime: goroutine stack exceeds \", maxstacksize, \"-byte limit\\n\")\n\t\tthrow(\"stack overflow\") // 栈溢出了\n\t}\n\n\t// The goroutine must be executing in order to call newstack,\n\t// so it must be Grunning (or Gscanrunning).\n    // 设置g的状态\n\tcasgstatus(gp, _Grunning, _Gcopystack)\n\n\t// The concurrent GC will not scan the stack while we are doing the copy since\n\t// the gp is in a Gcopystack status.\n    // coypstack会创建一个新的栈，然后把旧的栈的内容拷到新的栈中\n\tcopystack(gp, newsize, true)\n\tif stackDebug >= 1 {\n\t\tprint(\"stack grow done\\n\")\n\t}\n    // 可以开始跑了\n\tcasgstatus(gp, _Gcopystack, _Grunning)\n\tgogo(&gp.sched) // gogo开始跑了\n}\n```\n\n##### sysmon：抢占标记\n\n那么，我们的协程是什么时候被标记为可抢占的呢？当然是后台线程`sysmon`的功劳了，`sysmon`的工作之一就是找出持续运行很久的协程，然后把他标记为可抢占：\n\n```go\n// Always runs without a P, so write barriers are not allowed.\n//\n//go:nowritebarrierrec\nfunc sysmon() {\n\tlock(&sched.lock)\n\tsched.nmsys++\n\tcheckdead()\n\tunlock(&sched.lock)\n\n\t// If a heap span goes unused for 5 minutes after a garbage collection,\n\t// we hand it back to the operating system.\n\tscavengelimit := int64(5 * 60 * 1e9)\n\n\tif debug.scavenge > 0 {\n\t\t// Scavenge-a-lot for testing.\n\t\tforcegcperiod = 10 * 1e6\n\t\tscavengelimit = 20 * 1e6\n\t}\n\n\tlastscavenge := nanotime()\n\tnscavenge := 0\n\n\tlasttrace := int64(0)\n\tidle := 0 // how many cycles in succession we had not wokeup somebody\n\tdelay := uint32(0)\n\tfor {\n\t\tif idle == 0 { // start with 20us sleep...\n\t\t\tdelay = 20\n\t\t} else if idle > 50 { // start doubling the sleep after 1ms...\n\t\t\tdelay *= 2\n\t\t}\n\t\tif delay > 10*1000 { // up to 10ms\n\t\t\tdelay = 10 * 1000\n\t\t}\n\t\tusleep(delay)\n \t\t...\n        // 注释很清楚了\n\t\t// retake P's blocked in syscalls\n\t\t// and preempt long running G's\n\t\tif retake(now) != 0 {\n\t\t\tidle = 0\n\t\t} else {\n\t\t\tidle++\n\t\t}\n\t\t...\n\t}\n}\n\nfunc retake(now int64) uint32 {\n\tn := 0\n\t// 加锁\n\tlock(&allpLock)\n\t// 遍历p列表\n\tfor i := 0; i < len(allp); i++ {\n\t\t_p_ := allp[i]\n\t\tif _p_ == nil {\n\t\t\t// This can happen if procresize has grown\n\t\t\t// allp but not yet created new Ps.\n\t\t\tcontinue\n\t\t}\n\t\tpd := &_p_.sysmontick\n\t\ts := _p_.status\n\t\tif s == _Psyscall {\n\t\t\t// 这里是处理系统调用时的P\n            // 如果系统调用阻塞到一定时长，并且当前有其他g可执行时，考虑将_Psyscall的p夺回过来，标记为_Pidle\n            ...\n\t\t} else if s == _Prunning { \n            // 如果当前P已经在同一个G上运行很久了，标记抢占\n\t\t\t// Preempt G if it's running for too long.\n\t\t\tt := int64(_p_.schedtick)\n\t\t\tif int64(pd.schedtick) != t {\n\t\t\t\tpd.schedtick = uint32(t)\n\t\t\t\tpd.schedwhen = now\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif pd.schedwhen+forcePreemptNS > now {\n\t\t\t\tcontinue\n\t\t\t}\n            // 抢占\n\t\t\tpreemptone(_p_)\n\t\t}\n\t}\n\tunlock(&allpLock)\n\treturn uint32(n)\n}\n\n\nfunc preemptone(_p_ *p) bool {\n\tmp := _p_.m.ptr()\n\tif mp == nil || mp == getg().m {\n\t\treturn false\n\t}\n\tgp := mp.curg\n\tif gp == nil || gp == mp.g0 {\n\t\treturn false\n\t}\n\n    // 设置抢占标志位\n\tgp.preempt = true\n\n\t// Every call in a go routine checks for stack overflow by\n\t// comparing the current stack pointer to gp->stackguard0.\n\t// Setting gp->stackguard0 to StackPreempt folds\n\t// preemption into the normal stack overflow check.\n    // 设置gp.stackguard0 = stackPreempt，从而能够触发morestack\n\tgp.stackguard0 = stackPreempt\n\treturn true\n}\n```\n\n至此，我们对`go`中的协作式抢占机制已经很明了了，但是，抢占只是标记一下，你说任你说，我不一定照做啊。\n\n##### 缺陷\n\n之所以在函数调用的时候执行检查是因为函数切换的时候是一个`safe point`，这时候的通用寄存器是空的，切换协程不需要保存这些寄存器，而且栈中的`root pointer`是确定的，能够精确执行`gc`扫描。\n\n然而，如果有一个协程没有发生函数调用，比如下面这段代码：\n\n```go\ni:=0\nfor {\n    i++\n}\n```\n\n这个协程就没有机会被抢占，也就不会让出`cpu`。\n\n开始执行`gc`的时候先暂停所有协程的执行，然后再执行`stop the world`开启写屏障。这时候假如我们有一个协程跑的是类似上面的代码，那么我们的程序就会直接卡死：\n\n```go\nfunc main() {\n\tgo func() {\n\t\ti := 0\n\t\t// 该代码导致当前协程不会让出cpu\n\t\tfor {\n\t\t\ti++\n\t\t}\n\t}()\n    // sleep 1s，确保子协程开始执行\n\ttime.Sleep(time.Second)\n\tlog.Println(\"开始触发gc\")\n\t// 触发gc\n\truntime.GC()\n\tlog.Println(\"手动gc完成\")\n}\n```\n\n因为其他协程已经被暂停，而最后这个协程由于没有发生函数调用，无法执行抢占操作，从而垃圾收集器会一直处于等待，`gc`也一直不会被触发。当然我们平时的业务代码一般不会有这种情况发生，但是如果有一个函数需要执行比较久才会被抢占，那么就会导致`gc`的延时，同时也会严重影响程序的吞吐量。\n\n##### go:nosplit\n\nTODO\n\n\n\n### 非协作式抢占\n\n因为协作式抢占存在很明显的缺点：抢占不及时，有一种解决方案是在函数中插入一些细粒度更小的抢占检查点，但是这会对性能产生影响，比如每次循环都需要额外执行一次分支判断。\n\n因此有人提出来非协作式抢占来弥补这个缺陷，能够在任意指令触发抢占。非协作式抢占的主要难点在于如何满足`gc`安全，抢占应该发生在`safe point`。\n\n","tags":["golang"]},{"title":"一种diff算法：Myers","url":"/2019/05/23/diff算法/","content":"\n\n\n# 一种diff算法：Myers\n\n### diff\n\n`git`是我们日常工作中最经常使用到的工具，而其中`diff`又是最常用到的功能，我们使用这个功能去查看代码的变更信息，今天我们就来学习一下`diff`算法的实现。\n\n`diff`算法可以对一个文件编辑前后的状态进行比较，计算出其中删除和新增的内容，这样我们就能够直观看出该文件的变更情况。\n\n我们首先来看一个例子：\n\n- a = ABCABBA\n- b = CBABAC\n\n假设我们对文本`a`进行一系列编辑操作之后，得到了文本`b`，而`diff`操作就是要计算出这些编辑操作。\n\n一种可能的编辑序列是先把原来的内容全部删除，然后插入新的内容，使用通用的`diff`表示符号如下：\n\n```diff\n- A\n- B\n- C\n- A\n- B\n- B\n- A\n+ C\n+ B\n+ A\n+ B\n+ A\n+ C\n```\n\n然而，上面的信息无法有效展示`a`是如何一步步变更到`b`的，因此上面的`diff`并没有什么实际意义。通常源代码的变更，会保留原来的大部分内容，而只对其中部分内容做删除和添加，我们通过`diff`操作是希望看到其中被删除或者插入的代码段，而不是像上面这样，全部删除然后替换成新的内容。\n\n我们更希望看到是`diff`是下面这种：\n\n```diff\n- A\n- B\n  C\n+ B\n  A\n  B\n- B\n  A\n+ C\n```\n\n这种使用最少的变更把`a`转换成`b`，我们可以很明显的看出`a`是如何一步步转变到`b`的。然后这并不是唯一：\n\n```diff\n1.  - A       2.  - A       3.  + C\n    - B           + C           - A\n      C             B             B\n    - A           - C           - C\n      B             A             A\n    + A             B             B\n      B           - B           - B\n      A             A             A\n    + C           + C           + C\n```\n\n上面几种都是经过最少的变更。我看可以看到，一段文本从`a`变更到`b`的编辑过程并不是唯一的。\n\n`diff`算法的目的是提供一种在某些方法比较理想的`diff`生成策略。然而，我们除了希望`diff`的变更尽量少之外，也有其他方面的考量，我们希望生成的`diff`更够尽量直观，符合我们的操作逻辑。\n\n比如，当我们修改文件的时候，我们总是先删除一些旧的内容，然后再添加一些新的内容，因此在上面的例子中，我们会觉得第2种会比第3种更加直观\n\n而当我们更新一个代码块的时候，总数先全部删掉然后再插入新的内容，而不是删插交替。\n\n```diff\nGood:   - one         Bad:    - one\n        - two                 + four\n        - three               - two\n        + four                + five\n        + five                + six\n        + six                 - three\n```\n\n我们可以看到上面的例子中，第一种会比第二种更加直观。\n\n而如果对于代码的变更，我们也更希望能够符合代码的结构逻辑：\n\n```diff\nGood:   class Foo                   Bad:    class Foo\n          def initialize(name)                def initialize(name)\n            @name = name                        @name = name\n          end                             +   end\n      +                                   +\n      +   def inspect                     +   def inspect\n      +     @name                         +     @name\n      +   end                                 end\n        end                                 end\n```\n\n比如上面的例子种，我们新增了一个方法，我们希望新增的`end`是属于新增函数的一部分，而不是原来函数的一部分，这样我们就能够很直观的看出这里是新增了一个方法。\n\n而`Myers`算法就是一个能够符合这些考量的生成策略。\n\n### Myers算法\n\n`myers`通过贪心策略，在发现一个文本差异之前尽可能多的消费相同的内容，因此在上面新增`inspect`方法的例子中，第一次出现的`end`不会被识别为新增，这能有效防止第二种`diff`情况的发生。并且，删除操作总是优于插入操作，因此生成的diff中，删除操作总数能够先出现（除非没有内容被删除）。\n\n`Myers`算法是基于查找最短编辑脚本（*shortest edit script*, SES）的思想提出的。\n\n最短编辑脚本查找问题可以被建模成图搜索问题。\n\n我们继续拿上面的文本`a = ABCABBA`和文本`b = CBABAC`来说明。\n\n首先我们构建这样一个图：\n\n![](/img/ses_graph.jpg)\n\n在上面的坐标系中，当我们位于原点`(0,0)`时，表示我们现在有一个字符串`a`；\n\n当我们向右走，也就是增加`x`坐标的时候，对应的从`a`中删除一个字符，比如我们从`(0,0) - > (1,0)`时，我们从`a`中删除字符`A`，这时候我们当前的文本就变成了`BCABBA`； \n\n而当我们向下走，也就是增加`y`坐标的时候，对应的从`b`中插入一个字符，比如我们从`(1,0) -> (1,1)`的时候，对应的插入`b`中的字符`C`，这时候的文本变成了`CBCABBA`；\n\n在上面的某些位置上面还有斜向下的虚线，比如从`(1,1) -> (2,2)`，这时候表示`a[1]==b[1]`，这时候保留该字符，并且同时增加`x`坐标和`y`坐标；\n\n而当我们沿某一条路线从`(0,0)`走到`(7,6)`，就代表字符串`a`经过一系列编辑操作之后转换成了字符串`b`。\n\n我们把上面的向右走记作`del`操作，向下走记为`ins`操作，沿虚线走记为`mov`操作，只有执行`del`操作和`ins`操作才会让字符串发生变更。我们可以看到，从`(0,0)`到`(7,6)`有很多条路径可达，最多需要`7+6`次`del`和`ins`操作，也就是执行全删全增操作。\n\n`myers`算法的思想很简单，就是要找出一条从`(0,0)`到`(7,6)`的路径，让这条路径中的`del`和`ins`操作尽量的少，这就要求要尽量执行`mov`操作，并且当出现分支选择的时候，`del`操作能够优先于`ins`操作。\n\n我们对上面的图，从`(0,0)`开始，按照次序进行遍历，最终可以可到下面的遍历结果：\n\n![](/img/ses_graph_s.jpg)\n\n首先执行第一次编辑操作，我们可以从`(0,0) -> (1,0)`，或者从`(0,0) - > (0,1)`\n\n接着执行第二次编辑操作，我们要在第一次操作的基础上进行，因为`del`操作优先于`ins`操作，因此我们要优先扩展`(1,0)`这个点，从`(1,0)`可以走到`(1,1)`和`(2,0)`，又因为`(1,1)`到`(2,2)`以及`(2,0)`到`(3,1)`存在虚线，`mov`操作不会令文本内容发生变更，并不是编辑操作，还记得上面说的贪心策略吗？`myers`尽可能的消费相同文本内容，因此从`(1,0)`最终走到了`(2,2)`和`(3,1)`；接着扩展`(0,1)`这个点，`(0,1)`可以走到`(2,2)`和`(2,4)`\n\n接着执行第三次、第四次...操作，直到第一次达到`(7,6)`这个坐标，第一次到达说明这时候的编辑次数是最少的，最终的遍历结果如上图所示。\n\n我们可以看到，每一次的编辑都是基于上次的编辑结果进行的，我们在每次编辑中，都基于上次的结果选择向右或者向下走一步，然后再执行`0`次或者多次`mov`操作，从而达到当前编辑结果。\n\n为了更直观的发现算法的规律，我们把上图逆时针旋装45°：\n\n![](/img/ses_graph_d.jpg)\n\n在上图中，横坐标`d`表示编辑次数，也就是目前为止已经执行了多少次`del`或者`ins`编辑操作，也即图的搜索深度；而纵坐标`k = x - y`，当我们向由移动时，`k`值会加1，当向下移动时，`k`值会减1，而当沿着虚线移动时，`k`值并不会变化。我们需要记录指定`d`时，每个`k`值可以达到的最远记录，只需要记录当前给定`d`和`k`时，对应的`x`就行了，因为`y = x - k`。\n\n现在我们可以来看一下`myers`算法的执行过程：\n\n1. 我们首先从`0`到`(len(a)+len(b))`遍历`d`\n2. 每次迭代`b`时，我们以步长为`2`从`-d`到`d`遍历`k`；这里为什么步长是`2`呢？仔细观察上图，当前`d`的节点都是从`d-1`的节点扩展而来，`d-1`的节点要么向右走，`k`加`1`，要么向下走，`k`减`1`，这时候同一个`d`的相邻的两个`k`之间差值就是`2`。\n3. 对于给定的`d`和`k`值，我们根据上一轮遍历的结果，决定当前的最佳位置，这里的最佳位置意味着取`x`值最大的点，因为这就意味着优先执行`del`操作。当前`(d,k)`的点可以从`(d-1,k-1)`向右走一步，也可以从`(d-1,k+1)`向下走一步。当`(d-1,k+1)`的`x`值比`(d-1,k-1)`的大，则其优先级更高，因为它已经执行了更多的删除操作，而当具有相同的`x`值时，应该选择`(d-1,k-1)`，这时候向右走执行`del`操作。\n\n\n\n### Code\n\n```go\ntype Diffable interface {\n\tLenA() int           // length of src\n\tLenB() int           // length of dest\n\tEqual(int, int) bool // compare src[ai] and dest[bi]\n}\n\nfunc Myers(ab Diffable) EditScript {\n\taLen := ab.LenA() // 获取原文本a的长度\n\tbLen := ab.LenB() // 目标文本b的长度\n\tmax := aLen + bLen // 最大的编辑次数是aLen+bLen，即完全替换内容\n    // 这里v是一个稀疏数组，并且这里空间分配实际上也过大，可以进一步优化\n    v := make([]int, 2*max+1) // 保存指定d时，对应k的x值，因为k可能是负数，这里实际使用max作为0\n\ttrace := make([][]int, 0) // 保存每个d的v\nsearch:\n\t// myers\n    // 从0到max遍历d\n    // d==0的时候，可能执行MOV操作，即(d,v)=(0,0)的时候，x不一定为0\n\tfor d := 0; d <= max; d++ {\n        // 保存上一轮迭代的v\n        // 第一次迭代的时候，保存一个空的v，方便后面的回溯\n\t\tvc := make([]int, 2*max+1)\n\t\tcopy(vc, v)\n        // trace[d]保存的是d-1轮的v\n\t\ttrace = append(trace, vc)\n\t\t// 按照步长为2从-d到d迭代k\n\t\tfor k := -d; k <= d; k += 2 {\n            // 很容易发现，k最小是-bLen,最大是aLen，因此这里可以有一个判断，跳过无效的k值\n\t\t\tif k < -bLen || k > aLen {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar x int\n            // 如果k==-d，这时候只能由(d-1,k+1)向下走\n            // 如果k==d，这时候只能由(d-1,k-1)向右走\n            // 如果 (d-1,k+1)的x值大于(d-1,k-1)，则说明优先级大，往下走\n            // 否在 (d-1,k-1)往右走\n\t\t\tif k == -d || (k != d && v[max+k-1] < v[max+k+1]) {\n\t\t\t\tx = v[max+k+1]\n\t\t\t} else {\n\t\t\t\tx = v[max+k-1] + 1\n\t\t\t}\n\n            // k=x-y，因此y=x-k\n\t\t\ty := x - k\n            // 判断是否可以执行MOV操作\n\t\t\tfor x < aLen && y < bLen && ab.Equal(x, y) {\n\t\t\t\tx++\n\t\t\t\ty++\n\t\t\t}\n\n            // 设置当前(d,k)的x值\n\t\t\tv[max+k] = x\n\t\t\t// 判断是已经到到目标\n\t\t\tif x == aLen && y == bLen {\n\t\t\t\tbreak search\n\t\t\t}\n\t\t}\n\t}\n \t\n    // 回溯\n\tfor d := len(trace) - 1; d >= 0; d-- {\n\t\tv := trace[d] // 获取前一轮的v\n\t\tk := x - y // 当前k\n\n\t\tvar prevk int\n\t\t// 计算前驱结点的k值\n\t\tif k == -d || (k != d && v[max+k-1] < v[max+k+1]) {\n\t\t\tprevk = k + 1\n\t\t} else {\n\t\t\tprevk = k - 1\n\t\t}\n\t\t// 从历史状态获取x值并计算y值\n\t\tprevx := v[max+prevk]\n\t\tprevy := prevx - prevk\n        // 判断是否执行了mov操作\n\t\tfor x > prevx && y > prevy {\n\t\t\tscript = append(script, OpMov)\n\t\t\tx--\n\t\t\ty--\n\t\t}\n\t\t// 如果当前d>0\n\t\tif d > 0 {\n            // 如果x没有变化，则说明向下走，ins操作\n\t\t\tif x == prevx {\n\t\t\t\tscript = append(script, OpInsert)\n\t\t\t} else {\n                // 否在就是del操作\n\t\t\t\tscript = append(script, OpDel)\n\t\t\t}\n\t\t}\n\t\t// 更新x,y的值\n\t\tx, y = prevx, prevy\n\t}\n\t// 回溯获取到的编辑顺序是逆序的\n\treturn script.reverse()\n}\n```\n\n\n\n[完整代码](<https://github.com/ymcvalu/godiff>)\n\n### 参考\n\n- **[the-myers-diff-algorithm](<https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/>)**\n- **[pkg-diff](<https://github.com/pkg/diff>)**\n- [Git是怎样生成diff的：Myers算法](<https://cjting.me/misc/how-git-generate-diff/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io>)\n\n\n\n","tags":["diff","myers"]},{"title":"go性能分析","url":"/2019/05/12/go性能分析/","content":"\n本文总结一下go中用来进行性能调试的方法，先占个坑，后面慢慢完善\\~\\~\\~\n\n### go tool pprof\n\n### go tool trace \n\n### GODEBUG\n\n\n\n### 参考文档\n\n- [high-performance-go-workshop](<https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html>)","tags":["go","性能分析"]},{"title":"go的接口值","url":"/2019/05/11/go中的iface/","content":"# iface\n\n### 结构\n\n```go\ntype iface struct {\n\ttab  *itab\n\tdata unsafe.Pointer \n}\n\n// layout of Itab known to compilers\n// allocated in non-garbage-collected memory\n// Needs to be in sync with\n// ../cmd/compile/internal/gc/reflect.go:/^func.dumptypestructs.\ntype itab struct {\n\tinter *interfacetype // 接口类型\n\t_type *_type         // 实际类型\n\thash  uint32         // copy of _type.hash. Used for type switches.\n\t_     [4]byte        // 4字节填充，与上面的4字节hash凑成8字节，与n内存对齐相关\n    // itab末尾是实现方法的引用，如果多余1个，则其余方法引用紧跟itab内存之后分配\n\tfun   [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter.\n}\n\ntype interfacetype struct {\n\ttyp     _type // 接口类型信息\n\tpkgpath name\t \n\tmhdr    []imethod // 接口声明的方法\n}\n\n\n// Needs to be in sync with ../cmd/link/internal/ld/decodesym.go:/^func.commonsize,\n// ../cmd/compile/internal/gc/reflect.go:/^func.dcommontype and\n// ../reflect/type.go:/^type.rtype.\ntype _type struct {\n\tsize       uintptr\n\tptrdata    uintptr  // size of memory prefix holding all pointers\n\thash       uint32   // 类型hash值\n\ttflag      tflag    // 类型相关一些flag，可以在反射包中使用\n\talign      uint8    // 内存对齐\n\tfieldalign uint8    // 字段对齐\n\tkind       uint8    // 类型kind\n\talg        *typeAlg // 类型的hash和equal方法\n\t// gcdata stores the GC type data for the garbage collector.\n\t// If the KindGCProg bit is set in kind, gcdata is a GC program.\n\t// Otherwise it is a ptrmask bitmap. See mbitmap.go for details.\n\tgcdata    *byte\n\tstr       nameOff   // offset of name\n\tptrToThis typeOff   \n}\n```\n\n这里的`_type`是最基本的类型信息，而实际我们声明的类型还有包含字段、方法等信息，查看下面代码，可以看到`_type`只是实际类型结构的一部分\n\n```go\ntype uncommontype struct {\n\tpkgpath nameOff\n\tmcount  uint16 // number of methods\n\txcount  uint16 // number of exported methods\n\tmoff    uint32 // offset from this uncommontype to [mcount]method\n\t_       uint32 // unused\n}\n\nfunc (t *_type) uncommon() *uncommontype {\n\tif t.tflag&tflagUncommon == 0 {\n\t\treturn nil\n\t}\n\tswitch t.kind & kindMask {\n\tcase kindStruct:\n\t\ttype u struct {\n\t\t\tstructtype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindPtr:\n\t\ttype u struct {\n\t\t\tptrtype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindFunc:\n\t\ttype u struct {\n\t\t\tfunctype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindSlice:\n\t\ttype u struct {\n\t\t\tslicetype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindArray:\n\t\ttype u struct {\n\t\t\tarraytype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindChan:\n\t\ttype u struct {\n\t\t\tchantype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindMap:\n\t\ttype u struct {\n\t\t\tmaptype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tcase kindInterface:\n\t\ttype u struct {\n\t\t\tinterfacetype\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\tdefault:\n\t\ttype u struct {\n\t\t\t_type\n\t\t\tu uncommontype\n\t\t}\n\t\treturn &(*u)(unsafe.Pointer(t)).u\n\t}\n}\n```\n\n\n\n### 通过汇编看iface\n\n```go\nfunc main() {\n\tvar r io.Reader = Arr{}\n\tr.Read(nil)\n}\n\ntype Arr []byte\n\nfunc (Arr) Read(n []byte) (int, error) {\n\treturn 0, nil\n}\n```\n\n```sh\n$ go tool compile -S main.go > main.s\n```\n\n```assembly\n\t0x0000 00000 (test.go:7)\tTEXT\t\"\".main(SB), $88-0\n\t...\n\t0x0024 00036 (test.go:8)\tLEAQ\ttype.[0]uint8(SB), AX  // newobject方法参数\n\t0x002b 00043 (test.go:8)\tMOVQ\tAX, (SP)\n\t0x002f 00047 (test.go:8)\tCALL\truntime.newobject(SB)\n\t0x0034 00052 (test.go:8)\tMOVQ\t8(SP), AX // 这里把返回的指针保存到AX\n\t0x0039 00057 (test.go:8)\tMOVQ\tAX, \"\"..autotmp_1+56(SP) // Arr对象\n\t0x003e 00062 (test.go:8)\tXORPS\tX0, X0\n\t0x0041 00065 (test.go:8)\tMOVUPS\tX0, \"\"..autotmp_1+64(SP)\n\t0x0046 00070 (test.go:8)\tLEAQ\tgo.itab.\"\".Arr,io.Reader(SB), AX // itab\n\t0x004d 00077 (test.go:8)\tMOVQ\tAX, (SP)\n\t0x0051 00081 (test.go:8)\tLEAQ\t\"\"..autotmp_1+56(SP), AX // Arr对象指针\n\t0x0056 00086 (test.go:8)\tMOVQ\tAX, 8(SP)\n\t0x005b 00091 (test.go:8)\tCALL\truntime.convT2Islice(SB) // 生成iface\n\t0x0060 00096 (test.go:8)\tMOVQ\t24(SP), AX // 接口的data字段\n\t0x0065 00101 (test.go:8)\tMOVQ\t16(SP), CX // 接口的tab字段，即itab表\n\t0x006a 00106 (test.go:9)\tMOVQ\t24(CX), CX // itab的24~32为实际方法Read地址\n\t// 110~127构造一个 Arr{0 0 data}结构，来调用方法Read\n\t// 实际上方法的接收者就是方法第一个参数\n\t0x006e 00110 (test.go:9)\tMOVQ\t$0, 8(SP) // 0\n\t0x0077 00119 (test.go:9)\tXORPS\tX0, X0 \n\t0x007a 00122 (test.go:9)\tMOVUPS\tX0, 16(SP) // 0\n\t0x007f 00127 (test.go:9)\tMOVQ\tAX, (SP) // data\n\t0x0083 00131 (test.go:9)\tCALL\tCX // 调用Read方法\n\t0x008e 00142 (test.go:10)\tRET\n\n// 有些itab在编译期间可以自动生成\n// 可能在a文件声明了Arr，b文件中声明了接口Reader，在C包文件c和D包文件d都用到了Arr初始化接口Reader，\n// 则会在c文件和d文件都生成这个itab表，因此声明为dupok，由链接器任意选择一个\ngo.itab.\"\".Arr,io.Reader SRODATA dupok size=32\n\t0x0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n\t0x0010 d6 ed 1d 4e 00 00 00 00 00 00 00 00 00 00 00 00  ...N............\n\trel 0+8 t=1 type.io.Reader+0\n\trel 8+8 t=1 type.\"\".Arr+0\n\trel 24+8 t=1 \"\".(*Arr).Read+0 // 这里rel告诉链接器要将24~32的符号引用替换成方法的逻辑地址\n```\n\n可以看到，实现接口的方法引用列表会保存在itab末尾，调用时，需要先计算具体调用函数的偏移获取实际方法引用\n\n上面生成接口值的时候，调用了`runtime.convT2Islice`方法，其实在`runtime.iface.go`中声明了一系列`runtime.convT2IXXX`的方法，表示将`XXX`类型的值转换成一个接口值，因为这里的例子中，`Arr`的`Kind`是`slice`，因此调用的是`runtime.convT2Islice`这个方法。\n\n下面我们来看一下`convT2Islice`和`convT2I`这两个方法的实现。\n\n先来看`convT2Islice`：\n\n```go\n// 参数elem实际上是一个slice的指针\n// 将一个实际类型的值转换成接口值这种情况，itab由编译器自动生成\nfunc convT2Islice(tab *itab, elem unsafe.Pointer) (i iface) {\n\tt := tab._type //这里的_type就是该接口背后的真实数据类型\n\tif raceenabled {\n\t\traceReadObjectPC(t, elem, getcallerpc(), funcPC(convT2Islice))\n\t}\n\tif msanenabled {\n\t\tmsanread(elem, t.size)\n\t}\n\tvar x unsafe.Pointer\n    // 如果切片的底层数组是nil\n\tif v := *(*slice)(elem); uintptr(v.array) == 0 {\n\t\tx = unsafe.Pointer(&zeroVal[0])\n\t} else {\n\t\tx = mallocgc(t.size, t, true) // 分配一个slice\n\t\t*(*slice)(x) = *(*slice)(elem) // 赋值\n\t}\n\ti.tab = tab\n\ti.data = x // 返回的接口值中的data指向的内存是elem的拷贝\n\treturn\n}\n```\n\n然后是`convT2I`，这个是比较通用的转换方法：\n\n```go\n// 这里的elem是要转换成接口值的实际值的指针\n// 由实际类型转换成接口类型的情况，itab由编译器自动生成\nfunc convT2I(tab *itab, elem unsafe.Pointer) (i iface) {\n\tt := tab._type \n\tif raceenabled {\n\t\traceReadObjectPC(t, elem, getcallerpc(), funcPC(convT2I))\n\t}\n\tif msanenabled {\n\t\tmsanread(elem, t.size)\n\t}\n\tx := mallocgc(t.size, t, true) // 这里根据*elem的大小分配一块内存\n\ttypedmemmove(t, x, elem) // 内存拷贝\n\ti.tab = tab\n\ti.data = x\n\treturn\n}\n```\n\n当我们把一个**非指针值**赋给一个接口类型的变量时，就会调用该方法。**这里我们也可以看到，`iface.data`不是直接指向接口背后的实际值，而是指向其拷贝，因为这个原因，也就很好理解为什么方法接收者是指针的话，值类型就不会实现对应的接口类型了，因为访问的根本不是同一个变量。**\n\n而当把一个指针值赋给一个接口类型的变量呢？编译器会直接生成代码，这个时候`iface.data`就是该指针值，我们可以写个小`demo`验证一下：\n```go\nimport (\n\t\"fmt\"\n\t\"unsafe\"\n)\n\ntype eface struct {\n\ttyp  uintptr\n\tdata unsafe.Pointer\n}\n\nfunc main() {\n\tn := struct {\n\t\tT int\n\t}{}\n\n\tpi := interface{}(&n)\n\tvi := interface{}(n)\n\n\t_pi := *(*eface)(unsafe.Pointer(&pi))\n\t_vi := *(*eface)(unsafe.Pointer(&vi))\n\n\tfmt.Printf(\"%p %p %p\\n\", &n, _pi.data, _vi.data) // 0xc000060090 0xc000060090 0x597500\n}\n```\n代码中我们使用空接口，实际上空接口对应的定义`eface`和`iface`的差别只有第一个字段，因为它没有方法表，直接存储的就是值的类型。我们可以看到，接口值`vi`实际存储的是变量`n`的地址，而`vi.data`此时存的就是`n`的地址。\n看一下对应的汇编片段：\n```\n LEAQ\ttype.struct { T int }(SB), AX\n MOVQ\tAX, (SP)\n CALL\truntime.newobject(SB)  // 代码里面用的unsafe.Pointer，因为传到fmt.Printf里面，逃逸分析认为该变量逃逸了\n MOVQ\t8(SP), AX              // newobject返回的地址\n MOVQ\tAX, \"\"..autotmp_41+80(SP) // 保存到栈上的临时变量中\n LEAQ\ttype.*struct { T int }(SB), CX  // *struct{T int}的 _type\n MOVQ\tCX, \"\".pi+88(SP)                // 设置pi的 _type\n MOVQ\tAX, \"\".pi+96(SP)                //  设置pi的data，这里data就是n的地址\n```\n\n### 接口类型转换\n\n```go\n// 接口转换\nfunc convI2I(inter *interfacetype, i iface) (r iface) {\n   tab := i.tab\n   if tab == nil {\n      return\n   }\n   if tab.inter == inter {\n      r.tab = tab\n      r.data = i.data\n      return\n   }\n   r.tab = getitab(inter, tab._type, false)\n   r.data = i.data\n   return\n}\n```\n\n**将接口值A强制转换成接口B时，需要满足：接口A的方法集包含或者等于接口B的方法集**\n\n接口值之间的类型转换，不会考虑实际类型的方法集，而是简单的对接口的方法集进行判断，这个在编译时就可以进行检查\n\n```go\nvar r io.ReadCloser = XXX{}\nr.Read(nil)\n_ = io.Reader(r) // ok\n_ = io.ReadCloser(r) // ok\n_ = io.Writer(r) // no\nvar rc io.Reader = XXX{}\n_ = io.ReaderCloser(rc) // no\n```\n\n### 接口类型断言\n\n**接口断言：根据接口值的实际类型，判断是否实现了目标接口**\n\n类型断言时，可能需要在运行时动态生成itab\n\n```go\n// 接口断言\nfunc assertI2I(inter *interfacetype, i iface) (r iface) {\n   tab := i.tab\n   if tab == nil {\n      // explicit conversions require non-nil interface value.\n      panic(&TypeAssertionError{nil, nil, &inter.typ, \"\"})\n   }\n    \n   // 如果目标接口类型就是当前接口类型，直接返回\n   if tab.inter == inter {\n      r.tab = tab\n      r.data = i.data\n      return\n   }\n   r.tab = getitab(inter, tab._type, false) // 获取itab，如果失败直接panic\n   r.data = i.data\n   return\n}\n\nfunc assertI2I2(inter *interfacetype, i iface) (r iface, b bool) {\n\ttab := i.tab\n\tif tab == nil {\n\t\treturn\n\t}\n\tif tab.inter != inter {\n\t\ttab = getitab(inter, tab._type, true) // true表示容忍失败\n\t\tif tab == nil { // 不符合，返回false\n\t\t\treturn\n\t\t}\n\t}\n\tr.tab = tab\n\tr.data = i.data\n\tb = true\n\treturn\n}\n\nfunc getitab(inter *interfacetype, typ *_type, canfail bool) *itab {\n\tif len(inter.mhdr) == 0 {\n\t\tthrow(\"internal error - misuse of itab\")\n\t}\n\n\t// easy case\n\tif typ.tflag&tflagUncommon == 0 {\n\t\tif canfail {\n\t\t\treturn nil\n\t\t}\n\t\tname := inter.typ.nameOff(inter.mhdr[0].name)\n\t\tpanic(&TypeAssertionError{nil, typ, &inter.typ, name.name()})\n\t}\n\n\tvar m *itab\n\n\t// First, look in the existing table to see if we can find the itab we need.\n\t// This is by far the most common case, so do it without locks.\n\t// Use atomic to ensure we see any previous writes done by the thread\n\t// that updates the itabTable field (with atomic.Storep in itabAdd).\n    // 先查表是否已经存在需要的itab\n\tt := (*itabTableType)(atomic.Loadp(unsafe.Pointer(&itabTable)))\n\tif m = t.find(inter, typ); m != nil {\n\t\tgoto finish\n\t}\n\n\t// Not found.  Grab the lock and try again.\n\tlock(&itabLock)\n    // 双重锁检查\n\tif m = itabTable.find(inter, typ); m != nil {\n\t\tunlock(&itabLock)\n\t\tgoto finish\n\t}\n\n\t// Entry doesn't exist yet. Make a new entry & add it.\n    // 分配itab内存，itab的内存分配在gc堆之外，不会被垃圾扫描、回收\n\tm = (*itab)(persistentalloc(unsafe.Sizeof(itab{})+uintptr(len(inter.mhdr)-1)*sys.PtrSize, 0, &memstats.other_sys))\n\tm.inter = inter\n\tm._type = typ\n\tm.init() // 初始化\n\titabAdd(m) // 添加到itabTable中，后续直接查表，不需要重新构造\n\tunlock(&itabLock)\nfinish:\n\tif m.fun[0] != 0 { // itab初始化成功\n\t\treturn m\n\t}\n\tif canfail {\n\t\treturn nil\n\t}\n\t// this can only happen if the conversion\n\t// was already done once using the , ok form\n\t// and we have a cached negative result.\n\t// The cached result doesn't record which\n\t// interface function was missing, so initialize\n\t// the itab again to get the missing function name.\n\tpanic(&TypeAssertionError{concrete: typ, asserted: &inter.typ, missingMethod: m.init()})\n}\n\n\n// init fills in the m.fun array with all the code pointers for\n// the m.inter/m._type pair. If the type does not implement the interface,\n// it sets m.fun[0] to 0 and returns the name of an interface function that is missing.\n// It is ok to call this multiple times on the same m, even concurrently.\nfunc (m *itab) init() string {\n\tinter := m.inter\n\ttyp := m._type\n\tx := typ.uncommon() \n\n\t// both inter and typ have method sorted by name,\n\t// and interface names are unique,\n\t// so can iterate over both in lock step;\n\t// the loop is O(ni+nt) not O(ni*nt).\n    // 接口和类型的方法列表是按照名字排序的，因此实际循环时间复杂度是O(ni+nt)\n\tni := len(inter.mhdr) // 目标接口方法总数\n\tnt := int(x.mcount) // 实际类型方法总数\n    // 计算实际类型的方法引用列表的偏移\n\txmhdr := (*[1 << 16]method)(add(unsafe.Pointer(x), uintptr(x.moff)))[:nt:nt]\n\tj := 0\nimethods:\n\tfor k := 0; k < ni; k++ {\n\t\ti := &inter.mhdr[k]\n\t\titype := inter.typ.typeOff(i.ityp) // 目标接口方法类型，与参数和返回值相关\n\t\tname := inter.typ.nameOff(i.name)  // 目标接口方法名\n\t\tiname := name.name()\n\t\tipkg := name.pkgPath() // 接口的包名\n\t\tif ipkg == \"\" {\n\t\t\tipkg = inter.pkgpath.name()\n\t\t}\n\t\tfor ; j < nt; j++ {\n\t\t\tt := &xmhdr[j]\n\t\t\ttname := typ.nameOff(t.name) \n            // 如果实际方法类型和方法名与目标方法的一致\n\t\t\tif typ.typeOff(t.mtyp) == itype && tname.name() == iname {\n\t\t\t\tpkgPath := tname.pkgPath()\n\t\t\t\tif pkgPath == \"\" {\n\t\t\t\t\tpkgPath = typ.nameOff(x.pkgpath).name()\n\t\t\t\t}\n                // 如果方法是导出的或者包名一致\n                // 如果接口有未导出方法，只能在同一个包内被实现，可以用来限制其他包实现该接口\n\t\t\t\tif tname.isExported() || pkgPath == ipkg {\n\t\t\t\t\tif m != nil {\n\t\t\t\t\t\tifn := typ.textOff(t.ifn) //实际函数入口PC\n                        // 保存到itab的方法列表中\n\t\t\t\t\t\t*(*unsafe.Pointer)(add(unsafe.Pointer(&m.fun[0]), uintptr(k)*sys.PtrSize)) = ifn\n\t\t\t\t\t}\n\t\t\t\t\tcontinue imethods\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// didn't find method\n\t\tm.fun[0] = 0 // 没有找到方法，即目标类型没有实现该方法\n\t\treturn iname\n\t}\n\tm.hash = typ.hash\n\treturn \"\"\n}\n```\n\n\n\n### 其他\n\n因为go中的接口是隐式实现的，我们可以在声明类型的时候，使用一些断言来加入编译时的检查，并且也可以提示其他人该类型实现了某个接口，比如我们声明了类型`T`实现了接口`I`，我们可以：\n\n```go\ntype I interface{}\n\nvar _ I = new(T) // 这里断言T实现了接口I\ntype T struct{}\n\n```\n\n\n\n### 参考\n\n<https://github.com/teh-cmc/go-internals/tree/master/chapter2_interfaces>\n\n","tags":["go","interface"]},{"title":"iptables入门","url":"/2019/05/06/iptables入门/","content":"\n# iptables\n\n### Doc\n\n- [iptables tutorial](<https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html>)\n- `iptables -h`\n- `man iptables`\n\n### Tables & Chains\n\n表由链组成，链是一些按顺序排列的规则的列表。比如，默认的 `filter` 表包含 `INPUT`， `OUTPUT` 和 `FORWARD` 3条内建的链，这3条链作用于数据包过滤过程中的不同时间点。\n\n各个表和包含的链关系如下：\n\n| table      | chain                                                        | desc                                                        |\n| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------- |\n| `raw`      | `PREROUTING`、`OUTPUT`                                       | 关闭nat表上使用的连接追踪机制；内核模块：`iptable_raw`      |\n| `filter`   | `INPUT`、`OUTPUT`、`FORWARD`                                 | 负责过滤功能，防火墙；内核模块：`iptable_filter`            |\n| `nat`      | `PREROUTING`、 `POSTROUTING`、 `OUTPUT`、 `INPUT(部分支持)`  | 网络地址转换；内核模块：`iptable_nat`                       |\n| `mangle`   | `PREROUTING`、 `INPUT`、 `FORWARD`、 `OUTPUT `、`POSTROUTING` | 拆解、修改、重封装报文；内核模块：`iptable_mangle`          |\n| `security` |                                                              | 用于强制[访问控制网络](http://lwn.net/Articles/267140/)规则 |\n\n默认情况下，任何链中都没有规则。可以向链中添加自己想用的规则。链的默认规则通常设置为 `ACCEPT`，如果想确保任何包都不能通过规则集，那么可以重置为 `DROP`。默认的规则总是在一条链的最后生效，所以在默认规则生效前数据包需要通过所有存在的规则。用户可以加入自己定义的链，从而使规则集更方便管理，自定义链需要被内置的链引用才能生效。每个链下面可以设置一组规则，执行链时就是执行这组规则。\n\n\n\n### Traversing Chains\n\n\n\n![图1](/img/iptables_traverse.jpg)\n\n上图描述链了在任何接口上收到的网络数据包是按照怎样的顺序穿过表的交通管制链。第一个路由策略包括决定数据包的目的地是本地主机（这种情况下，数据包穿过 `INPUT` 链），还是其他主机（数据包穿过 `FORWARD` 链）；中间的路由策略包括决定给传出的数据包使用那个源地址、分配哪个接口；最后一个路由策略存在是因为先前的` mangle` 与 `nat` 链可能会改变数据包的路由信息。数据包通过路径上的每一条链时，链中的每一条规则按顺序匹配；无论何时匹配了一条规则，相应的` target` 动作将会执行。内置的链有默认的策略，但是用户自定义的链没有默认的策略。在` jump` 到的自定义链中，若每一条规则都不能提供完全匹配，那么数据包像下图描述的一样返回到调用链。在任何时候，若 `DROP` 的规则实现完全匹配，那么被匹配的数据包会被丢弃，不会进行进一步处理。如果一个数据包在链中被 `ACCEPT`，那么这个包就会被`ACCEPT`，不会再遍历后面的规则。\n\n然而，要注意的是，数据包还会以正常的方式继续遍历其他表中的其他链。\n\n![图2](/img/iptable_subtraverse.jpg)\n\n\n\n### Command\n\n```sh\n$ iptables -t 表名 <-A/I/D/R> 规则链名 [规则号] <-i/o 网卡名> -p 协议名 <-s 源IP/源子网> --sport 源端口 <-d 目标IP/目标子网> --dport 目标端口 -j 动作\n```\n\n###### 规则管理命令\n\n- `-A` or `--append` ：将规则加到`chain`末尾\n\n  ```sh\n  $ iptables -t filter -A INPUT -i lo -j DROP #在INPUT链末尾添加规则，拒绝掉来自lo网卡的包\n  ```\n\n- `-I` or `--insert` ：在指定位置添加规则，原来位置的规则后移\n\n  ```sh\n  $ iptables -t filter -I INPUT 1 -i lo -j DROP #在INPUT链头部添加规则，插入位置从1开始计算\n  ```\n\n- `-R` or `--replace` ：替换指定位置规则\n\n  ```sh\n  $ iptables -t filter -R INPUT 1 -i lo -j ACCEPT #修改INPUT链头部规则\n  ```\n\n- `-D` or `--delete`：删除指定位置规则\n\n  ```sh\n  $ iptables -t filter -D INPUT 2 #删除INPUT链第二条规则\n  ```\n\n###### 链管理命令\n\n- `-P` or `--policy`：改变指定链的默认策略，只有内置的链才有默认策略，自定义链没有默认策略\n\n  ```sh\n  $ iptables -P INPUT ACCEPT\n  ```\n\n- `-F` or `--flush` ：清空规则链的所有规则，如果省略规则链，则清空表上所有链的规则\n\n- `-N` or `--new`：创建自定义链\n\n- `-X` or `--delete-chain`：删除指定的链，这个链必须没有被其它任何规则引用，而且这条上必须没有任何规则。如果没有指定链名，则会删除该表中所有非内置的链。\n\n- `-E` or `--rename-chain`：用指定的新名字去重命名指定的链。这并不会对链内部照成任何影响。\n\n  ```sh\n  $ iptables -E oldName newName\n  ```\n\n- `-Z` or `--zero`：把指定链，或者表中的所有链上的所有计数器清零，计数器是规则命中计数。\n\n- `-L` or `--list`：查看指定链或者指定表上的所有规则\n\n###### 规则参数\n\n- `-t` or `--table`：指定操作的表，**如果不指定此选项，默认操作的是 `filter` 表**\n\n- `-p` or `--protocol`：指定协议\n\n- `-i` or `--in-interface`：network interface name，匹配流量流入的网络接口，只对`PREROUTING`、`INPUT`或者`FORWARD`生效；这里的网络接口不一定是网卡，比如`docker0`等虚拟网桥也可以；前缀`!`表示非，比如`! -i 127.0.0.1`表示非本机发送过来的数据包。\n\n- `-o` or `--out-interface`：network interface name，匹配流量输出的网络接口，只对`OUTPUT`、`FORWARD`或`POSTROUTING`生效\n\n- `-s` or `--source`：源地址，`ip`地址或者`CIDR`表示指定范围地址\n\n- `--sport `：匹配来源端口\n\n- `-d` or `--destination`：目标地址，`ip`地址或者`CIDR`表示指定范围地址\n\n- `--dport`：匹配目标端口\n\n- `-j` or `--jump`：规则目标，即满足规则时应该执行什么样的动作。目标可以是内置目标，也可以是用户自定义的链，内置的目标有：\n\n  - `ACCEPT`：接收数据包，如果当前规则匹配成功则结束当前链及父链（如果当前是自定义子链）\n  - `DROP`：丢弃数据包，不做任何响应。\n  - `REJECT`：拒绝当前包，会返回拒绝数据包。\n  - `REDIRECT`：重定向、映射、透明代理。\n  - `SNAT`：源地址转换。\n  - `DNAT`：目标地址转换。\n  - `MASQUERADE`：`IP`伪装（`NAT`），用于`ADSL`。\n  - `LOG`：日志记录，继续匹配下一个规则，不会结束当前链。\n\n- `-m` or `--match`：使用扩展包匹配模块，可以使用`man iptables-extensions`命令查看扩展模块\n\n  > iptables can use extended packet matching modules. **These are loaded in two ways: implicitly, when -p or --protocol is specified, or with the -m or --match options, followed by the matching module name**; after these, various extra command line options become available, depending on the specific module. You can specify multiple extended match modules in one line, and you can use the -h or --help options after the module has been specified to receive help specific to that module.\n\n  - `statistic`：基于一些统计条件匹配\n\n    ```sh\n    $ iptables -A INPUT -m statistic --mode random --probability 0.5 -s 127.0.0.1 -p icmp -j DROP # 来自本机的ping包，有50%的几率被丢弃\n    ```\n\n  - `comment`：允许添加注释（最多256给字符）\n\n    ```sh\n    $ iptables -A INPUT -m comment --comment \"a comment demo\" -j ACCEPT\n    ```\n\n\n\n### DNAT & SNAT\n\n##### SNAT\n\n`SNAT`: Source Network Address Translation，修改网络包源ip地址。\n\n比如内网机器只有私有ip，无法正常访问外网，可以在网关进行SNAT，将ip包的源地址替换为网关的公网ip，等请求返回的时候，网关再把返回的ip包的目标地址还原为原来的内网ip，然后由网关转发给具体的机器。\n\n`SNAT`是多对一的映射，比如多个内网机器同时映射同一个网关的公网ip，不同内网机器可能使用同一个源端口，系统是通过源IP，源端口，目标ip和目标端口和协议等5元组来区分不同的连接的，因此执行`SNAT`时，除了修改源ip，还需要重新分配源端口号。\n\n系统需要通过`SNAT`表来保存原来的ip/端口与转换后的ip/端口之间的映射关系，以便能够在数据流入流出时进行跟踪。\n\n在容器网络中，当容器内部主动向外部发起网络请求时，需要使用`SNAT`将容器ip替换成主机的ip。\n\n##### DNAT\n\n`DNAT`用于将内网机器的端口映射到外网。当网关接收到数据包时，通过DNAT将目标ip和端口替换成内网机器的ip和端口，然后进行转发。\n\n在容器网络中，容器的端口映射就是使用`DNAT`实现的。通过将容器的端口映射到主机端口上，当由数据包发送到该主机端口时，`netfilter`会将其替换成容器的ip和端口。","tags":["linux","iptables"]},{"title":"rs纠删码","url":"/2019/05/06/rs纠删码/","content":"\n# RS纠删码\n\n在存储系统中，需要采用数据冗余技术来保证数据的可靠性，相比使用多副本复制机制外，使用纠删码能够以更小的数据冗余度获得更高的数据可靠性。`Reed Solomon Coding`是存储领域常用的一种纠删码。\n\n### 基本原理\n\nRS纠删码将原始文件分成n个数据块，同时为这**n个数据块**生成**m个校验块**，而能够容忍最多丢失（只保证丢失而不保证数据篡改）这（n+m)个块中的任意m个数据或者校验块。\n\nRS编码以word为单位进行编码和解码（word字长一般为8或者16），而大的数据块拆分成一个个word进行编解码。\n\n假如有数据块内容为`ABCDEFGHIJKLMNOP`，将其分为四个数据块，每个数据块包含四个word（字长为8），我们使用矩阵来表示这四个数据块，对应的数据矩阵如下：\n\n![](/img/blog-rs-1.png)\n\n接着使用编码矩阵来对数据矩阵进行编码，产生数据块和校验块：\n\n![](/img/blog-rs-2.webp)\n\n上图左边的矩阵为编码矩阵，该矩阵由上面一个`4X4`的单位矩阵和下面一个`2X4`的`Vandermonder`矩阵组成。这里的`2`对应上面的`m`，表明需要生成2个校验块，`4`是因为原数据分成了4个数据块。我们可以看到右边经过编码矩阵处理后的结果，比原来的数据矩阵多了两行，即对应两个校验块。\n\n因为`m`取的是2，因此最多允许丢失2个块，这里我们假设`IJKL`和`MNOP`两个块被丢失了：\n\n![](/img/blog-rs-3.webp)\n\n我们在编码矩阵和结果矩阵将对应的第3，4行删除，等式两边仍然成立：\n\n![](/img/RS-post-pic-4.png)\n\n现在的编码矩阵是原来的一部分，我们将其记为`C1`，接着我们分别在两边乘以编码矩阵的逆矩阵：\n\n![](/img/RS-post-pic-5.png)\n\n![](/img/RS-post-pic-5-5.webp)\n\n![](/img/blog-rs-7.png)\n\n我们可以看到，我们可以从编码产生的6个块中的任意4个块中恢复原数据。\n\n### Vandermonder矩阵\n\n![](/img/Vandermonder.svg)\n\n在上面中，为了恢复原数据，需要计算编码矩阵（这里指的是`C1`）的逆矩阵，这也是为什么上面编码矩阵是由一个单位矩阵和一个`Vandermonder`矩阵组成的，因为`Vandermonder`矩阵任意两行之间都线性无关，可以求解出逆矩阵。\n\n### 伽罗华域\n\nRS纠删码是按照word为单位对数据进行编解码，这也就要求对一个字进行编码后，产生的结果仍然是一个字，因此RS纠删码基于伽罗华域[GF(2^n)](<http://mcll.top/2019/05/05/%E6%9C%89%E9%99%90%E5%9F%9F/>)及其四则运算来进行编解码。\n\n### 举个栗子\n\n这里使用`GF(2^4)`，并且取`n`和`m`都为3来举个例子。\n\n首先，`GF(2^4)`共有0~15这16个元素，首先构造`exp`表和`log`表：\n\n| i    | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| log  | -    | 0    | 1    | 4    | 2    | 8    | 5    | 10   | 3    | 14   | 9    | 7    | 6    | 13   | 11   | 12   |\n| exp  | 1    | 2    | 4    | 8    | 3    | 6    | 12   | 11   | 5    | 10   | 7    | 14   | 15   | 13   | 9    | 1    |\n\n假设原始数据 D, Vandermonde 矩阵 F, 冗余数据 C，则：\n\n![](/img/blog-rs-demo1.png)\n\n上面计算矩阵F时，根据`GF(2^4)`的计算规则，3^2^ = 3*3 = exp(log(3)+log(3)) = 5\n\n将矩阵F与3*3的单位矩阵E集合，可得：\n\n![](/img/blog-rs-demo2.png)\n\n根据上面的定义，任意丢失3份数据，都可以恢复原来的数据，这里假设丢失了D2，D3和C3，则恢复过程：\n\n![](/img/blog-rs-demo3.png)\n\n可以看到，成功恢复原来的数据。\n\n\n\n\n### 参考\n\n- [RS纠删算法原理](<https://github.com/RobinLiew/RobinLiew.github.io/blob/master/%E7%AE%97%E6%B3%95/RS%E7%BA%A0%E5%88%A0%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81.md>)\n- [Reed-Solomon 编码算法](<https://www.gaccob.com/publish/2017-08-27-reed-solomon.html>)\n- [reed-solomon](<https://www.backblaze.com/blog/reed-solomon/>)","tags":["rs纠删码"]},{"title":"有限域","url":"/2019/05/05/有限域/","content":"\n# Finite fields - 有限域\n\n有限域（`Finite fields`）是代数的一个分支，在加密算法、数据压缩算法和纠错算法中都有重要的作用。\n\n### 概念\n\n##### 域 - *field*\n\n如果集合`F`的所有元素与定义在其上的二元操作符`+`和` · `满足下面条件，即构成一个域：\n\n- 闭包（`Closure`）： 对于任意`F`中的元素`x`和`y`，`x+y`和`x·y`也都是`F`中的元素\n- 结合律（`Associative`)：对于`F`中的任意`x`，`y`和`z`，满足`(x+y)+z = x+(y+z)`和`(x·y)·z = x·(y·z)`\n- 交换律（`Commutative`）：对于`F`中的任意元素`x`和`y`，满足`x+y = y+x`和`x·y = y·x`\n- 分配率（`Distributive`）：对于`F`中的任意元素`x`，`y`和`z`，满足`x·(y+z) = (x·y)+(x·z)`\n- `Identity`：`F`中存在加法单位元，记作`0`，对`F`中的任意元素`x`满足`x+0 = x`；存在乘法单位元，记作`1`，对`F`中的任意元素`x`满足`x·1 = x`\n- `Inverse`：对`F`中的任意元素`x`，在`F`中都能找到对应的`y`满足`x+y = 0`，即`y = -x`，`x`与`y`互为负元；对于`F`中除`0`外的任意元素`x`，在`F`中都能找到对应的`y`满足`x·y = 1`，即`y = 1/x`，`x`与`y`互为逆元\n\n比如，实数集`R`与定义在其上的加法和乘法运算，就构成了一个域。\n\n但是整数集`Z`不是一个域，因为`Z`中的非零元素并不都有乘法逆元，比如`1/2`不在`Z`中。\n\n虽然`Z`不是一个域，但是**整数模上任意一个素数的集合可以构成一个域**。比如`mod 5`的集合是`{0, 1, 2, 3, 4}`，记为`Z/5`，对应的`+`运算为`(x+y)%5`，`·`为`(x·y)%5`，这时候加法单位元是`0`，乘法单位元是`1`，可以发现，`(2*3)%5 = 1`，所以`2`的乘法逆元为`3`\n\n**对于任意一个素数`p`，`Z/p`是一个域，`+`运算为`(x+y) % p`，`·`运算为`(x·y) %p`**\n\n##### 有限域 - *finite field*\n\n如果一个域的元素个数是有限的，那么这个域称为有限域。\n\n作为一个程序员，我们接触到最多的有限域是`Z/2`，只包含`0`和`1`两个元素。`+`运算即为`XOR`，而`·`运算为`AND`。\n\n### 多项式\n\n早在19世纪初期，数学家最开始提出域的概念就是为了构造多项式。\n\n我们引入一个符合变量`x`，那么所有使用域`F`内的元素作为系数与`x`构成的多项式的集合，记为`F[x]`。\n\n例如，如果我们使用`R`作为构造多项式的域，那么`x^2+1`、`x+2`、`3.14x^2-2.72x+1.41`等都属于`R[x]`。同整数一样，多项式也可以有加法和乘法运算，但不总是有除法运算，比如无法得出`(x^2+1)/(x+2)`的结果，因此`R[x]`并不是一个域，因为`1/(x+2)`可以看成是`(x+2)`的逆元，但是并不属于`R[x]`中。\n\n然而，正如整数模上任意一个素数构成的集合可以构成一个域，**多项式模上一个素多项式构成的集合可以构成域**\n\n**素多项式（`prime polynomial`）：系数为整数并且不能被因式分解成低阶多项式。**比如上面的`x^2+1`，在实数范围内不可被因式分解。因此，`R[x]/(x^2+1)`就是一个域，这个域中的多项式中`x`的最高次幂最大只能是`1`\n\n对于域`Z/p`，`mod`一个`n`次素多项式`f(x)`的结果为一个具有`p^n`个元素的域`(Z/p)[x]/f(x)`。关于`f(x)`这个素多项式的选择并不重要，因为任意两个大小为`p^n`的有限域具有相同的结构，**将大小为`p^n`的有限域记为`GF(p^n)`。**\n\n对于多项式，例如`x^8+x^4+x^3+x+1`，我们可以看作是一个向量`[1,0,0,0,1,1,0,1,1]`。\n\n### GF(2^n)\n\n作为程序员，对我们来说最有趣的是`GF(2^n)`，即`Z/2`的多项式扩展，因为`GF(2^n)`的元素为长度为`n`的位向量。\n\n例如，`(Z/2)[x]/(x^8+x^4+x^3+x+1)`，这个域共有`2^8`个元素，每一个元素为一个长度为`8`的位向量，代表一个单字节：一个字节的二进制形式 *b~7~b~6~b~5~b~4~b~3~b~2~b~1~b~0~* 代表多项式 *b~7~x^7^+b~6~x^6^+b~5~x^5^+b~4~x^4^+b~3~x^3^+b~2~x^2^+b~1~x+b~0~*。\n\n##### 加法操作\n\n多项式的加法操作，即把相同次数的项系数相加，因为系数是`Z/2`的元素，因此系数相加意味着`XOR`操作\n\n```\n(x^2 + x) + (x + 1) = x^2 + 2x + 1 = x^2 + 1\n```\n\n换成二进制形式：*110~2~+011~2~=101~2~*\n\n##### 乘法操作\n\n多项式的乘法操作比较复杂，需要基于加法的`XOR`操作\n\n```\n(x^2 + x) · (x + 1) = x^3 + x^2 + x^2 + x = x^3 + x\n```\n\n换成二进制形式，具体的计算过程如下：\n\n```\n     110\n x   011\n---------\n     110\n    110\n + 000\n---------\n   01010\n```\n\n计算后的结果可能溢出，需要模上对应的素多项式，比如 *(Z/2)[x]/(x^8^+x^4^+x^3^+x+1)*这个域内的元素的乘法，最后的结果需要模上素多项式式*x^8^+x^4^+x^3^+x+1*。\n\n我们可以看到，上面的乘法过程中，需要分别执行`n`次乘法和加法，这个`n`为具体位向量的长度。而在实际实现域的乘法时，我们会使用查找表来进行优化。这里的查找表不是说建立例如九九乘法表那样的表格，在介绍具体的优化原理之前，需要先介绍一下**生成元**的概念。\n\n**在一个有限域中，至少存在一个元素α，使得域内任意一个非零元素都可以表示为该元素的方幂**。\n\n**假设一个有限域的非零元素个素为n，并且一个生成元为α，那么 α^n^ = 1**。\n\n例如，2是 *Z/5* 的一个生成元，{α, α^2^, α^3^, α^4^} = {2, 4, 3, 1}。对于`GF(p^n)`计算会更复杂，但是同样生效。\n\n通过生成元`α`，域中的任意非零元素都可以表示为`α`的方幂，我们就可以将元素的乘法转换为幂的加法。因此，**我们在实现乘法时，可以先生成一张`exp`表和一张`log`表，其中 *exp[i] = α^i^*， 而 *log[α^i^] = i*。通过这两张查找表，针对非零元素a和b的乘法可以转换为：*a·b = exp[log[a] + log[b]]*，只需要执行1次加法和3次查表操作即可。**\n\n\n\n### Code\n\n因为`GF(2^8)`中一个元素就是一个长度为8的位向量，正好是一个字节大小，因此`GF(2^8)`在计算机科学中使用最广泛。现在来实现一下`GF(2^8)`的运算。\n\n首先定义一个域，包含用来优化乘法运算的`exp`表和`log`表和素多项式：\n\n```go\ntype Field struct {\n\tpoly int\n\texp  [255]byte\n\tlog  [256]byte // log[0] is unused\n}\n```\n\n构造函数的实现，需要传入一个素多项式和生成元，`x^8+x^4+x^3+x+1`是`GF(2^8)`最广泛使用的素多项式，对于的二进制为`100011011`，对于十进制为`283`，`3`是`GF(2^8)`的一个生成元。\n\n```go\nfunc NewField(poly, a int) *Field {\n\tvar f Field\n\tx := 1\n    // 填充exp和log表\n\tfor i := 0; i < 255; i++ {\n\t\tf.exp[i] = byte(x)\n\t\tf.log[x] = byte(i)\n\t\tx = mul(x, a, poly)\n\t}\n\tf.poly = poly\n\treturn &f\n}\n\nfunc mul(x, y, poly int) int {\n\tz := 0\n\tfor x > 0 {\n\t\tif x&1 != 0 {\n\t\t\tz ^= y\n\t\t}\n\t\tx >>= 1\n\t\ty <<= 1\n\t\tif y&0x100 != 0 {\n\t\t\ty ^= poly\n\t\t}\n\t}\n\treturn z\n}\n```\n\n两个查表方法：\n\n```go\nfunc (f *Field) Exp(e int) byte {\n\tif e < 0 {\n\t\treturn 0\n\t}\n\treturn f.exp[e%255] // e^255 = 1\n}\n\nfunc (f *Field) Log(x byte) int {\n\tif x == 0 {\n\t\treturn -1\n\t}\n\n\treturn int(f.log[x])\n}\n```\n\n优化后的乘法运算就是`exp[log[a]+log[b]]`：\n\n```go\nfunc (f *Field) Mul(a, b byte) byte {\n\tif a == 0 || b == 0 {\n\t\treturn 0\n\t}\n\treturn f.Exp(f.Log(a) + f.Log(b))\n}\n```\n\n加法操作就是异或操作：\n\n```go\nfunc (f *Field) Add(a, b byte) byte {\n\treturn a ^ b\n}\n```\n\n求乘法逆元：\n\n```go\nfunc (f *Field) Inv(x byte) byte {\n\tif x == 0 {\n\t\treturn 0\n\t}\n\treturn f.Exp(255 - f.Log(x))\n}\n```\n\n\n\n### 参考\n\n- [Finite Field Arithmetic and Reed-Solomon Coding](<https://research.swtch.com/field>)\n\n","tags":["代数"]},{"title":"go函数栈布局","url":"/2019/04/29/go函数栈布局/","content":"\n# Go函数调用布局\n\n### 函数具有局部变量（栈帧大小大于0）\n\n![](/img/go-func-statck1.png)\n\n从上图我们可以看到，函数调用时，参数和返回值是通过栈来传递的，通过栈来传递函数，能够很好的实现多返回值，而且当发生`goroutine`的调度时，只需要切换`SP/BP`等少量寄存器，而不需要对通用寄存器进行切换，而且，当我们使用**命名返回值**时，是直接在对应的栈上进行更新，因此我们可以在`defer`函数内更新返回值。\n\n##### 测试代码\n\n```go\nfunc frameInfo(i int) (uintptr, uintptr, uintptr, uintptr,int,int)\nfunc main() {\n\tfmt.Println(frameInfo(15))\n}\n```\n\n```assembly\nTEXT ·frameInfo(SB),$8-54 // 这里8表示函数局部栈帧8个字节，参数和返回值共54个字节\n    MOVQ SP, AX // 取硬件寄存器SP的内容，作为第一个返回值\n    MOVQ AX, ret0+8(FP)\n    LEAQ i+0(SP), AX // 取pseudo_sp的值，作为第二个返回值\n    MOVQ AX, ret2+16(FP)\n    MOVQ BP, AX  // 取硬件寄存器BP的值，作为第三个返回值\n    MOVQ AX, ret1+24(FP)\n    LEAQ i+0(FP), AX // 取pseudo_fp的值，作为第四个返回值\n    MOVQ AX, ret3+32(FP)\n    MOVQ i+0(FP), AX // 通过伪寄存器fp获取参数i作为第五个返回值\n    MOVQ AX, ret4+40(FP)\n    MOVQ i+16(SP), AX // 通过伪寄存器sp获取参数i作为第六给返回值\n    MOVQ AX, ret5+48(FP)\n    RET\n\n```\n\n查看输出内容：\n\n```sh\n$ go run .\n824634146480 824634146488 824634146488 824634146504 15 15\n```\n\n可以看到伪寄存器`SP`和栈底寄存器`BP`指向的是同一个地址，而伪寄存器`FP`比伪寄存器`SP`大`16`个字节，其中高`8`个字节保存函数的返回地址，而低`8`个字节保存函数调用者的`BP`；而硬件`SP`和伪寄存器`SP\t`之间相差的字节数刚好是函数栈帧（这里的函数栈帧不包含保存调用者`BP`的`8`个字节）的大小，可以通过调整函数的局部栈帧大小观察其关系\n\n```assembly\nTEXT ·frameInfo(SB),$16-54 // 调整栈帧大小为16字节\n```\n\n查看输出内容：\n\n```sh\n$ go run .\n824634187432 824634187448 824634187448 824634187464 15 15\n```\n\n可以看到，新的输出中，硬件`SP`和伪寄存器`SP`之间相差为`16`字节，正好为栈帧大小。\n\n\n\n### 函数栈帧大小为0\n\n当函数栈帧大小为`0`时，情况就有点不同了。\n\n因为这个时候，当前函数没有分配栈帧，因此硬件寄存器`BP`不需要保存当前函数栈的栈底，也就不需要在栈上额外分配一个`8`字节的空间来保存函数调用者的`BP`寄存器内容，也就是说当前`BP`寄存器直接保存的就是函数调用者的`BP`寄存器信息。\n\n这时候的栈结构：\n\n![](/img/go-func-statck2.png)\n\n##### 测试代码\n\n```go\nfunc getBp() (uintptr, uintptr)\nfunc zeroFrame(i int) (uintptr, uintptr, uintptr, uintptr, int, int, uintptr)\nfunc main() {\n\tfmt.Println(getBp())\n\tfmt.Println(zeroFrame(11))\n}\n```\n\n```assembly\nTEXT ·getBp(SB),$8-16\n    MOVQ 0(BP), AX // 获取调用者函数的栈底地址\n    MOVQ AX, ret0+0(FP)\n    MOVQ ra+8(SP), AX // 获取当前函数的返回地址\n    MOVQ AX, ret0+8(FP)\n    RET\n\nTEXT ·zeroFrame(SB),$0-64\n    MOVQ SP, AX // 取硬件寄存器SP的内容，作为第一个返回值\n    MOVQ AX, ret0+8(FP)\n    LEAQ i+0(SP), AX // 取pseudo_sp的值，作为第二个返回值\n    MOVQ AX, ret1+16(FP)\n    MOVQ BP, AX  // 取硬件寄存器BP的值，作为第三个返回值\n    MOVQ AX, ret2+24(FP)\n    LEAQ i+0(FP), AX // 取pseudo_fp的值，作为第四个返回值\n    MOVQ AX, ret3+32(FP)\n    MOVQ i+0(FP), AX // 通过伪寄存器fp获取参数i作为第五个返回值\n    MOVQ AX, ret4+40(FP)\n    MOVQ i+8(SP), AX // 通过伪寄存器sp获取参数i作为第六给返回值\n    MOVQ AX, ret5+48(FP)\n    MOVQ addr+0(SP), AX // 获取当前函数的返回地址\n    MOVQ AX, ret6+56(FP)\n    RET\n\n```\n\n查看输出内容：\n\n```sh\n$ go run .\n824634187656 4776270\n824634187392 824634187392 824634187656 824634187400 11 11 4776438\n```\n\n函数`getBp`返回了调用者也即`main`函数的栈底信息，和当前函数的返回地址\n\n然后调用函数`zeroFrame`，该函数栈帧大小为`0`，可以看到执行该函数时，`BP`寄存器依然是`824634187656`，而且硬件寄存器`SP`和伪寄存器`SP`指向同一个位置，并且和伪寄存器`FP`只相差`8`个字节，而这`8`个字节保存的是当前函数的返回地址，我们可以看到两个函数的返回地址是在同一个段中的。","tags":["go"]},{"title":"mutex解析","url":"/2019/04/14/go中的锁实现探究/","content":"\n`go`的基础包`sync`提供了两种锁的实现，分别是`Mutex`和`RWMutex`。其中`Mutex`是互斥锁，一次只允许一个协程获取锁，`RWMutex`是读写锁，允许同时有多个协程获取读锁，但是只能有一个协程获取读锁，并且读写互斥。\n\n### Mutex\n\n好习惯，看源码先看注释：\n\n```\n// Mutex fairness.\n//\n// Mutex can be in 2 modes of operations: normal and starvation.\n// In normal mode waiters are queued in FIFO order, but a woken up waiter\n// does not own the mutex and competes with new arriving goroutines over\n// the ownership. New arriving goroutines have an advantage -- they are\n// already running on CPU and there can be lots of them, so a woken up\n// waiter has good chances of losing. In such case it is queued at front\n// of the wait queue. If a waiter fails to acquire the mutex for more than 1ms,\n// it switches mutex to the starvation mode.\n//\n// In starvation mode ownership of the mutex is directly handed off from\n// the unlocking goroutine to the waiter at the front of the queue.\n// New arriving goroutines don't try to acquire the mutex even if it appears\n// to be unlocked, and don't try to spin. Instead they queue themselves at\n// the tail of the wait queue.\n//\n// If a waiter receives ownership of the mutex and sees that either\n// (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms,\n// it switches mutex back to normal operation mode.\n//\n// Normal mode has considerably better performance as a goroutine can acquire\n// a mutex several times in a row even if there are blocked waiters.\n// Starvation mode is important to prevent pathological cases of tail latency.\n```\n\n##### 结构声明\n\n首先我们来看一下`Mutex`的声明\n\n```go\ntype Mutex struct {\n\tstate int32 \n\tsema  uint32  \n}\n```\n\n可以看到，`Mutex`只包含两个字段，其中`state`用于记录锁的状态，第一位表示锁是否被占用，第二位用于通知`unlock`方法不要唤醒一个`waiter`参与锁的抢夺，第三位表示是当前锁否处于饥饿模式，从第四位到第32位则用于记录当前阻塞在等待锁的协程数量；而`sema`是用于在多个协程之间进行同步的信号量，这个后面再说。\n\n##### 抢占锁\n\n我们首先来看一下`Lock`方法实现：\n\n```go\n// Lock locks m.\n// If the lock is already in use, the calling goroutine\n// blocks until the mutex is available.\nfunc (m *Mutex) Lock() {\n\t// 首先先尝试获取unlock的锁\n    if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {\n        // 竞争检查，忽略。。。\n\t\tif race.Enabled {\n\t\t\trace.Acquire(unsafe.Pointer(m))\n\t\t}\n        // 表示cas操作成功，获取锁，直接返回\n\t\treturn\n\t}\n\t\n    // 记录开始等待的时间\n\tvar waitStartTime int64\n\t// 是否处于饥饿模式\n    starving := false\n    // 是否设置了state的mutexWoken状态位\n\tawoke := false\n    // 记录自旋次数\n\titer := 0\n    // 获取当前的状态\n\told := m.state\n\tfor {\n\t\t// 如果当前锁处于Locked状态，并且允许自旋，则进入自旋状态\n        // 允许自旋的条件：不处于饥饿模式，自旋次数小于4，running on a multicore machine and GOMAXPROCS>1 and there is at least one other running P and local runq is empty\n\t\tif old&(mutexLocked|mutexStarving) == mutexLocked && runtime_canSpin(iter) {\n\t\t\t// 更新state的第二位，表示当前有运行中的协程在抢占锁\n\t\t\tif !awoke && old&mutexWoken == 0 && old>>mutexWaiterShift != 0 &&\n\t\t\t\tatomic.CompareAndSwapInt32(&m.state, old, old|mutexWoken) {\n\t\t\t\tawoke = true\n\t\t\t}\n            // 执行自旋，所谓自旋，就是一种busy wait，当前协程不阻塞，等待一会儿重新尝试获取锁，这里的doSpin是在runtime包中实现的\n\t\t\truntime_doSpin()\n\t\t\titer++ // 添加自旋次数\n\t\t\told = m.state\n\t\t\tcontinue \n\t\t}\n        \n        // 参与锁的抢夺\n        \n\t\tnew := old\n\t\t// 如果不处于饥饿模式，设置状态位尝试获取锁\n        if old&mutexStarving == 0 {\n\t\t\tnew |= mutexLocked\n\t\t}\n        \n        // 如果已经锁住或者处于饥饿模式，需要进入等待队列，waiter数量加1\n\t\tif old&(mutexLocked|mutexStarving) != 0 {\n\t\t\tnew += 1 << mutexWaiterShift\n\t\t}\n        \n\t\t// 如果需要进入饥饿状态，则设置饥饿标志位\n\t\tif starving && old&mutexLocked != 0 {\n\t\t\tnew |= mutexStarving\n\t\t}\n        \n        // state的mutexWoken状态位\n\t\tif awoke {\n\t\t\t// The goroutine has been woken from sleep,\n\t\t\t// so we need to reset the flag in either case.\n\t\t\tif new&mutexWoken == 0 {\n\t\t\t\tthrow(\"sync: inconsistent mutex state\")\n\t\t\t}\n\t\t\tnew &^= mutexWoken\n\t\t}\n        // cas更新锁状态\n\t\tif atomic.CompareAndSwapInt32(&m.state, old, new) {\n            // 更新成功，并且原来锁为unlock并且不属于饥饿模式，直接返回\n            // 这种情况是锁处于正常模式，新的协程与从等待中唤醒的协程竞争锁，并且竞争成功\n\t\t\tif old&(mutexLocked|mutexStarving) == 0 {\n\t\t\t\tbreak // locked the mutex with CAS\n\t\t\t}\n            \n\t\t\t// 是否是从等待状态中唤醒的\n\t\t\tqueueLifo := waitStartTime != 0\n            // 设置开始等待时间\n\t\t\tif waitStartTime == 0 {\n\t\t\t\twaitStartTime = runtime_nanotime()\n\t\t\t}\n            \n            // 当前协程抢占锁失败，阻塞等待，这里需要传入信号量sema和queueLifo\n            // 信号量sema用来在多个协程之间同步\n            // queueLifo如果为true，表示当前协程与新的协程竞争锁失败，加入队首，否则加入队尾\n\t\t\truntime_SemacquireMutex(&m.sema, queueLifo)\n            \n            // 执行到这里表明协程被从等待队列中唤醒了\n            // 如果等待时间大于1ms则进入饥饿模式\n\t\t\tstarving = starving || runtime_nanotime()-waitStartTime > starvationThresholdNs\n\t\t\told = m.state\n            // 当前锁处于饥饿模式，表明没有其他协程会与当前协程竞争锁\n\t\t\tif old&mutexStarving != 0 {\n\t\t\t\t// 检查状态位\n\t\t\t\tif old&(mutexLocked|mutexWoken) != 0 || old>>mutexWaiterShift == 0 {\n\t\t\t\t\tthrow(\"sync: inconsistent mutex state\")\n\t\t\t\t}\n                // 更新状态位，饥饿模式，只有当前协程能够抢占锁\n                // 阻塞等待锁的协程数量需要减1\n\t\t\t\tdelta := int32(mutexLocked - 1<<mutexWaiterShift)\n                // 如果不需要进入饥饿模式，或者当前等待队列为空，则清空饥饿模式\n\t\t\t\tif !starving || old>>mutexWaiterShift == 1 {\n\t\t\t\t\tdelta -= mutexStarving\n\t\t\t\t}\n                // 这里使用原子Add而不是CAS操作，因为可能在这个时刻有新的协程因为等待锁而阻塞，这时候如果使用CAS会失败\n\t\t\t\tatomic.AddInt32(&m.state, delta)\n\t\t\t\tbreak // 返回\n\t\t\t}\n            // 当前锁处于正常模式，因此需要和新加入的协程竞争锁\n\t\t\tawoke = true\n\t\t\titer = 0\n\t\t} else { // cas失败，重试\n\t\t\told = m.state\n\t\t}\n\t}\n\n\tif race.Enabled {\n\t\trace.Acquire(unsafe.Pointer(m))\n\t}\n}\n```\n\n接下来我们看一下`runtime_SemacquireMutex`这个方法，该方法的实现在`runtime`包中：\n\n```go\n// 这里使用`go:linkname`告诉链接器将该方法与sync包的runtime_SemacquireMutex方法链接\n//go:linkname sync_runtime_SemacquireMutex sync.runtime_SemacquireMutex\nfunc sync_runtime_SemacquireMutex(addr *uint32, lifo bool) {\n\tsemacquire1(addr, lifo, semaBlockProfile|semaMutexProfile)\n}\n\nfunc semacquire1(addr *uint32, lifo bool, profile semaProfileFlags) {\n    // 获取当前g\n\tgp := getg()\n    // 不允许在系统栈执行Lock方法\n\tif gp != gp.m.curg {\n\t\tthrow(\"semacquire not on the G stack\")\n\t}\n\n\t// 尝试捕获信号量，成功则直接返回\n\tif cansemacquire(addr) {\n\t\treturn\n\t}\n\t\n    // sudog用来表示一个阻塞的g，同一个锁的等待协程会构成一条sudog链表，多条链表会组织成一颗平衡树\n\ts := acquireSudog()\n   // 通过sema的地址获取对应的root，一个root中有多个sema的等待队列\n\troot := semroot(addr)\n\tt0 := int64(0)\n\ts.releasetime = 0\n\ts.acquiretime = 0\n\ts.ticket = 0\n\tif profile&semaBlockProfile != 0 && blockprofilerate > 0 {\n\t\tt0 = cputicks()\n\t\ts.releasetime = -1\n\t}\n\tif profile&semaMutexProfile != 0 && mutexprofilerate > 0 {\n\t\tif t0 == 0 {\n\t\t\tt0 = cputicks()\n\t\t}\n\t\ts.acquiretime = t0\n\t}\n\tfor {\n\t\tlock(&root.lock)\n\t\t// Add ourselves to nwait to disable \"easy case\" in semrelease.\n\t\tatomic.Xadd(&root.nwait, 1)\n\t\t// 再次尝试捕获信号量\n\t\tif cansemacquire(addr) {\n\t\t\tatomic.Xadd(&root.nwait, -1)\n\t\t\tunlock(&root.lock)\n\t\t\tbreak\n\t\t}\n\t\t// 加入等待队列，一个root内有多个等待队列，这些等待队列通过平衡树来组织，等待队列通过addr来标识，每个等待队列就是一个sudog列表\n\t\troot.queue(addr, s, lifo)\n        // 阻塞当前g\n\t\tgoparkunlock(&root.lock, waitReasonSemacquire, traceEvGoBlockSync, 4)\n\t\t// g被唤醒，如果ticket不为0或者再次尝试捕获信号量成功\n        if s.ticket != 0 || cansemacquire(addr) {\n\t\t\tbreak // 捕获成功\n\t\t}\n\t}\n\tif s.releasetime > 0 {\n\t\tblockevent(s.releasetime-t0, 3)\n\t}\n    // 释放sudog\n\treleaseSudog(s)\n}\n```\n\n##### 释放锁\n\n现在来看一下`Unlock`方法实现：\n\n```go\nfunc (m *Mutex) Unlock() {\n\tif race.Enabled {\n\t\t_ = m.state\n\t\trace.Release(unsafe.Pointer(m))\n\t}\n\n\t// Fast path: drop lock bit.\n    // 清空锁状态\n\tnew := atomic.AddInt32(&m.state, -mutexLocked)\n    // 检查状态位\n\tif (new+mutexLocked)&mutexLocked == 0 {\n\t\tthrow(\"sync: unlock of unlocked mutex\")\n\t}\n    // 如果不处于饥饿模式\n\tif new&mutexStarving == 0 {\n\t\told := new\n\t\tfor {\n\t\t\t// 如果没有等待锁的协程，或者设置了mutexWoken标志位，直接返回\n\t\t\tif old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken|mutexStarving) != 0 {\n\t\t\t\treturn\n\t\t\t}\n            \n\t\t\t// 唤醒一个等待的协程，参与锁竞争\n\t\t\tnew = (old - 1<<mutexWaiterShift) | mutexWoken\n\t\t\tif atomic.CompareAndSwapInt32(&m.state, old, new) {\n\t\t\t\truntime_Semrelease(&m.sema, false)\n\t\t\t\treturn\n\t\t\t}\n\t\t\told = m.state\n\t\t}\n\t} else {\n\t\t// 当前处于饥饿模式，从等待队列唤醒协程\n\t\truntime_Semrelease(&m.sema, true)\n\t}\n}\n```\n\n接下来看一下`runtime_Semrelease`方法：\n\n```go\n//go:linkname sync_runtime_Semrelease sync.runtime_Semrelease\nfunc sync_runtime_Semrelease(addr *uint32, handoff bool) {\n\tsemrelease1(addr, handoff)\n}\n\nfunc semrelease1(addr *uint32, handoff bool) {\n\troot := semroot(addr)\n\tatomic.Xadd(addr, 1) // addr值加1\n\n\t// Easy case: no waiters?\n\t// This check must happen after the xadd, to avoid a missed wakeup\n\t// (see loop in semacquire).\n\tif atomic.Load(&root.nwait) == 0 {\n\t\treturn\n\t}\n\n\t// Harder case: search for a waiter and wake it.\n\tlock(&root.lock)\n\tif atomic.Load(&root.nwait) == 0 {\n\t\t// The count is already consumed by another goroutine,\n\t\t// so no need to wake up another goroutine.\n\t\tunlock(&root.lock)\n\t\treturn\n\t}\n    // 从队首获取等待的sudog\n\ts, t0 := root.dequeue(addr)\n\tif s != nil {\n\t\tatomic.Xadd(&root.nwait, -1)\n\t}\n\tunlock(&root.lock)\n\tif s != nil { // May be slow, so unlock first\n\t\tacquiretime := s.acquiretime\n\t\tif acquiretime != 0 {\n\t\t\tmutexevent(t0-acquiretime, 3)\n\t\t}\n\t\tif s.ticket != 0 {\n\t\t\tthrow(\"corrupted semaphore ticket\")\n\t\t}\n        // 如果handoff为true，则尝试捕获信号量\n\t\tif handoff && cansemacquire(addr) {\n            // 成功，则更新ticket为1\n\t\t\ts.ticket = 1\n\t\t}\n        // 唤醒g\n\t\treadyWithTime(s, 5)\n\t}\n}\n```\n\n\n\n> 新版本的`Mutex`实现，将`Lock`和`Unlock`方法中的只保留了`fastpath`，而`slowpath`部分移到新的子过程中，用于内联优化。\n\n\n\n### RWMutex\n\n现在来看一下`RWMutex`，同样都，先看注释：\n\n```\n// A RWMutex is a reader/writer mutual exclusion lock.\n// The lock can be held by an arbitrary number of readers or a single writer.\n// The zero value for a RWMutex is an unlocked mutex.\n//\n// A RWMutex must not be copied after first use.\n//\n// If a goroutine holds a RWMutex for reading and another goroutine might\n// call Lock, no goroutine should expect to be able to acquire a read lock\n// until the initial read lock is released. In particular, this prohibits\n// recursive read locking. This is to ensure that the lock eventually becomes\n// available; a blocked Lock call excludes new readers from acquiring the\n// lock.\n```\n\n##### 结构声明\n\n```go\ntype RWMutex struct {\n\tw           Mutex  // held if there are pending writers\n\twriterSem   uint32 // semaphore for writers to wait for completing readers\n\treaderSem   uint32 // semaphore for readers to wait for completing writers\n\treaderCount int32  // number of pending readers\n\treaderWait  int32  // number of departing readers\n}\n```\n\n\n\n##### 抢占读锁\n\n```go\nfunc (rw *RWMutex) RLock() {\n   if race.Enabled {\n      _ = rw.w.state\n      race.Disable()\n   }\n    // readCount加1，如果小于0，表明当前有写锁正在等待，则阻塞等待\n   if atomic.AddInt32(&rw.readerCount, 1) < 0 {\n      // A writer is pending, wait for it.\n      runtime_SemacquireMutex(&rw.readerSem, false)\n   }\n   if race.Enabled {\n      race.Enable()\n      race.Acquire(unsafe.Pointer(&rw.readerSem))\n   }\n}\n```\n\n##### 释放读锁\n\n```go\nfunc (rw *RWMutex) RUnlock() {\n   if race.Enabled {\n      _ = rw.w.state\n      race.ReleaseMerge(unsafe.Pointer(&rw.writerSem))\n      race.Disable()\n   }\n    // readerCount减1，小于0说明有等待写\n   if r := atomic.AddInt32(&rw.readerCount, -1); r < 0 {\n      if r+1 == 0 || r+1 == -rwmutexMaxReaders {\n         race.Enable()\n         throw(\"sync: RUnlock of unlocked RWMutex\")\n      }\n      // A writer is pending.\n       // readerWait减1，如果为0表示读锁释放完，唤醒等待读\n      if atomic.AddInt32(&rw.readerWait, -1) == 0 {\n         // The last reader unblocks the writer.\n         runtime_Semrelease(&rw.writerSem, false)\n      }\n   }\n   if race.Enabled {\n      race.Enable()\n   }\n}\n```\n\n##### 抢占写锁\n\n```go\nfunc (rw *RWMutex) Lock() {\n\tif race.Enabled {\n\t\t_ = rw.w.state\n\t\trace.Disable()\n\t}\n\t// First, resolve competition with other writers.\n\trw.w.Lock()\n\t// Announce to readers there is a pending writer.\n\tr := atomic.AddInt32(&rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders\n\t// Wait for active readers.\n\tif r != 0 && atomic.AddInt32(&rw.readerWait, r) != 0 {\n\t\truntime_SemacquireMutex(&rw.writerSem, false)\n\t}\n\tif race.Enabled {\n\t\trace.Enable()\n\t\trace.Acquire(unsafe.Pointer(&rw.readerSem))\n\t\trace.Acquire(unsafe.Pointer(&rw.writerSem))\n\t}\n}\n```\n\n##### 释放写锁\n\n ```go\nfunc (rw *RWMutex) Unlock() {\n\tif race.Enabled {\n\t\t_ = rw.w.state\n\t\trace.Release(unsafe.Pointer(&rw.readerSem))\n\t\trace.Disable()\n\t}\n\n\t// Announce to readers there is no active writer.\n\tr := atomic.AddInt32(&rw.readerCount, rwmutexMaxReaders)\n\tif r >= rwmutexMaxReaders {\n\t\trace.Enable()\n\t\tthrow(\"sync: Unlock of unlocked RWMutex\")\n\t}\n\t// Unblock blocked readers, if any.\n\tfor i := 0; i < int(r); i++ {\n\t\truntime_Semrelease(&rw.readerSem, false)\n\t}\n\t// Allow other writers to proceed.\n\trw.w.Unlock()\n\tif race.Enabled {\n\t\trace.Enable()\n\t}\n}\n ```\n\n\n\n\n\n","tags":["go"]},{"title":"go网络io模型分析","url":"/2019/04/07/go网络io模型分析/","content":"\n在过去，传统的网络编程模型是多线程模型，在主线程中开启一个网络监听，然后每次有一个客户端进行连接，就会单独开启一个线程来处理这个客户端请求。\n\n然而，如果并发量比较大，服务端就会创建大量的线程，而且会有大量的线程阻塞在网络IO上，频繁的线程上下文切换会占用大量的cpu时间片，严重影响服务性能，而且大量的线程也需要占用大量的系统资源\n\n这样就引出著名的`C10K`问题，如何在单台服务器上支持并发`10K`量级的连接\n\n我们知道，虽然同一时间有大量的并发连接，但是同一时刻，只有少数的连接是可读/写的，我们完全可以只使用一个线程来服务提供服务，这也是目前解决`C10K`问题的主要思路，对应的解决方案叫做**IO多路复用**，现在主流的高性能网络服务器/框架都是基于该网络模型，比如`nginx`、`redis`或者`netty`网络库等。\n\n说到这，就不得不提[`epoll`](<http://man7.org/linux/man-pages/man7/epoll.7.html>)，这是`linux`内核提供的用于实现**IO多路复用**的系统调用，其他操作系统上也有类似的接口，关于`epoll`具体内容网上有一大堆的[资料](<http://man7.org/linux/man-pages/man7/epoll.7.html>)，这里就不重复介绍了\n\n**IO多路复用模型**，也可以称作是**事件驱动模型**，虽然能够有效解决`C10K`问题，但是相对传统的多线程模型也带来了一点复杂性。比如说，在多线程模型下，每个连接独占一个线程，而线程本身有自己的上下文；而如果是IO多路复用模型，需要在一个线程中处理多个连接，而每个需要有自己的上下文，需要开发者手动管理。比如服务端还没有接收到一个完整的协议报文时，我们需要把先前接收的部分内容保存到当前连接上下文中，等到下次其余内容到底时再一起处理。\n\n今天，我们主要来看一下`go`中的网络模型。\n\n在`go`中我们可以像传统的多线程模型那样为每个网络连接单独使用一个`goroutine`来提供服务，但是`goroutine`的资源占用相比系统级线程来说非常小，而且其切换在运行在用户态的，并且只需要交换很少的寄存器，因此`goroutine`的上下文切换代价也是极小的，更重要的是，其底层也是基于`epoll`（linux系统下）来实现事件通知的，因此只需要占用很少的系统级线程。\n\n很明显可以看出，`go`中的网络IO模型是传统多线程模型和IO多路复用模型的结合，既有前者的易用性，又有后者的效率，因此使用`go`可以很容易地开发高性能服务器。\n\n今天我们就来看一下，`go`中的网络IO模型是如何实现的。\n\n### 一切从创建Listener开始\n\n我们从创建`Listener`开始说起。\n\n先看下面代码：\n\n```go\nln,_ :=net.Listen(\"tcp\",\":80\")\n```\n\n我们使用`Listen`来创建一个`Listener`，那么底层具体会发生什么呢？让我们一步一步来揭开\n\n首先查看`net.Listen`方法\n\n```go\nfunc Listen(network, address string) (Listener, error) {\n\tvar lc ListenConfig\n\treturn lc.Listen(context.Background(), network, address)\n}\n```\n\n可以看到实际上工作的是`ListenConfig.Listen`,我们继续往下看：\n\n```go\nfunc (lc *ListenConfig) Listen(ctx context.Context, network, address string) (Listener, error) {\n    ...\n\tvar l Listener\n\tla := addrs.first(isIPv4)\n\tswitch la := la.(type) {\n\tcase *TCPAddr:\n\t\tl, err = sl.listenTCP(ctx, la)\n\t...\n\treturn l, nil\n}\n```\n\n因为我们创建的是`tcp`连接，这里我们只关注`sl.listenTCP`方法，继续往下\n\n```go\nfunc (sl *sysListener) listenTCP(ctx context.Context, laddr *TCPAddr) (*TCPListener, error) {\n\tfd, err := internetSocket(ctx, sl.network, laddr, nil, syscall.SOCK_STREAM, 0, \"listen\", sl.ListenConfig.Control)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &TCPListener{fd}, nil\n}\n```\n\n我们看函数第一行，调用了`internetSocket`，很明显里面就是创建实际`socket`的逻辑了，继续往下走\n\n```go\nfunc internetSocket(ctx context.Context, net string, laddr, raddr sockaddr, sotype, proto int, mode string, ctrlFn func(string, string, syscall.RawConn) error) (fd *netFD, err error) {\n\tif (runtime.GOOS == \"windows\" || runtime.GOOS == \"openbsd\" || runtime.GOOS == \"nacl\") && mode == \"dial\" && raddr.isWildcard() {\n\t\traddr = raddr.toLocal(net)\n\t}\n\tfamily, ipv6only := favoriteAddrFamily(net, laddr, raddr, mode)\n\treturn socket(ctx, net, family, sotype, proto, ipv6only, laddr, raddr, ctrlFn)\n}\n```\n\n这里我们只看`linux`的情况，因此继续看`socket`方法：\n\n```go\nfunc socket(ctx context.Context, net string, family, sotype, proto int, ipv6only bool, laddr, raddr sockaddr, ctrlFn func(string, string, syscall.RawConn) error) (fd *netFD, err error) {\n    // 这里是实际创建socket的代码\n\ts, err := sysSocket(family, sotype, proto)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n    // 设置socket选项\n\tif err = setDefaultSockopts(s, family, sotype, ipv6only); err != nil {\n\t\tpoll.CloseFunc(s)\n\t\treturn nil, err\n\t}\n    // 根据socket创建netFD，netFD是net包对底层socket的封装\n\tif fd, err = newFD(s, family, sotype, net); err != nil {\n\t\tpoll.CloseFunc(s)\n\t\treturn nil, err\n\t}\n\n\tif laddr != nil && raddr == nil {\n\t\tswitch sotype {\n        // 看上面的参数，我们传入的sotype是SOCK_STREAM，因此会走这个分支\n\t\tcase syscall.SOCK_STREAM, syscall.SOCK_SEQPACKET:\n\t\t\tif err := fd.listenStream(laddr, listenerBacklog, ctrlFn); err != nil {\n\t\t\t\tfd.Close()\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn fd, nil\n\t\tcase syscall.SOCK_DGRAM:\n\t\t\tif err := fd.listenDatagram(laddr, ctrlFn); err != nil {\n\t\t\t\tfd.Close()\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn fd, nil\n\t\t}\n\t}\n\tif err := fd.dial(ctx, laddr, raddr, ctrlFn); err != nil {\n\t\tfd.Close()\n\t\treturn nil, err\n\t}\n\treturn fd, nil\n}\n```\n\n我们先来看`sysSocket`方法：\n\n```go\nfunc sysSocket(family, sotype, proto int) (int, error) {\n    // 这里的socketFunc实际上是创建socket的系统调用\n    // \tsocketFunc func(int, int, int) (int, error)  = syscall.Socket\n    // 注意这里传入的SOCK_NONBLOCK，表明我们创建的是非阻塞的socket\n    // 这里的SOCK_CLOEXEC表明在执行fork系统调用时，当执行exec时需要关闭从父进程继承的文件设备\n\ts, err := socketFunc(family, sotype|syscall.SOCK_NONBLOCK|syscall.SOCK_CLOEXEC, proto)\n\tswitch err {\n\tcase nil:\n\t\treturn s, nil\n\tdefault:\n\t\treturn -1, os.NewSyscallError(\"socket\", err)\n        // 低版本内核不支持创建时指定SOCK_NONBLOCK或者SOCK_CLOEXEC\n        // 这时候需要分两步，先创建socket，然后再设置flag\n\tcase syscall.EPROTONOSUPPORT, syscall.EINVAL:\n\t}\n\n    // 这里需要加锁，与fork操作互斥，防止在创建socket而没有设置`SOCK_CLOEXEC`时执行了fork和exec\n\tsyscall.ForkLock.RLock()\n    // 创建socket\n\ts, err = socketFunc(family, sotype, proto)\n\tif err == nil {\n        // 设置SOCK_COLEXEC\n\t\tsyscall.CloseOnExec(s)\n\t}\n\tsyscall.ForkLock.RUnlock()\n\tif err != nil {\n\t\treturn -1, os.NewSyscallError(\"socket\", err)\n\t}\n    // 设置非阻塞IO\n\tif err = syscall.SetNonblock(s, true); err != nil {\n\t\tpoll.CloseFunc(s)\n\t\treturn -1, os.NewSyscallError(\"setnonblock\", err)\n\t}\n\treturn s, nil\n}\n```\n\n`sysSocket`主要通过系统调用创建了`socket`，**同时设置了`SOCK_NONBLOCK`标志位**，这点非常重要，这里要明确，我们在`go`中使用的网络连接一般都是非阻塞的。关于阻塞IO和非阻塞IO的区别网上有一大堆的资料，这里就不重复说明了。使用非阻塞IO的主要的原因是，**在go中，当使用阻塞系统调用时，当前goroutine对应的底层系统级线程就会被占用，无法与当前g解绑为其他g提供服务**，这样当需要执行其他`g`时就需要创建新的线程来执行\n\n接着来看`netFd.listenStream`\n\n```go\nfunc (fd *netFD) listenStream(laddr sockaddr, backlog int, ctrlFn func(string, string, syscall.RawConn) error) error {\n\t...\n    // 为socket绑定监听的ip和端口\n\tif err = syscall.Bind(fd.pfd.Sysfd, lsa); err != nil {\n\t\treturn os.NewSyscallError(\"bind\", err)\n\t}\n    // listenFunc func(int, int) error = syscall.Listen\n    // 这里的listenFunc实际上是系统调用Listen\n    // 开始监听\n\tif err = listenFunc(fd.pfd.Sysfd, backlog); err != nil {\n\t\treturn os.NewSyscallError(\"listen\", err)\n\t}\n    // 执行初始化操作\n\tif err = fd.init(); err != nil {\n\t\treturn err\n\t}\n\tlsa, _ = syscall.Getsockname(fd.pfd.Sysfd)\n\tfd.setAddr(fd.addrFunc()(lsa), nil)\n\treturn nil\n}\n```\n\n这里就是常规的绑定监听地址和端口，然后开始监听，这里重要的是`netFD.init`函数，先来看`netFD`的结构：\n\n```go\n// Network file descriptor.\ntype netFD struct {\n\tpfd poll.FD\n\n\t// immutable until Close\n\tfamily      int\n\tsotype      int\n\tisConnected bool // handshake completed or use of association with peer\n\tnet         string\n\tladdr       Addr\n\traddr       Addr\n}\n\n// FD is a file descriptor. The net and os packages use this type as a\n// field of a larger type representing a network connection or OS file.\n// 对应上面的poll.FD\ntype FD struct {\n\t// Lock sysfd and serialize access to Read and Write methods.\n\tfdmu fdMutex // 执行read/write时的互斥锁\n\n\t// System file descriptor. Immutable until Close.\n\tSysfd int // open系统调用返回的文件描述符fd\n\n\t// I/O poller.\n\tpd pollDesc \n\n\t// Writev cache.\n\tiovecs *[]syscall.Iovec \n\n\t// Semaphore signaled when file is closed.\n\tcsema uint32\n\n\t// Non-zero if this file has been set to blocking mode.\n\tisBlocking uint32 \n\n\t// Whether this is a streaming descriptor, as opposed to a\n\t// packet-based descriptor like a UDP socket. Immutable.\n\tIsStream bool\n\n\t// Whether a zero byte read indicates EOF. This is false for a\n\t// message based socket connection.\n\tZeroReadIsEOF bool\n\n\t// Whether this is a file rather than a network socket.\n\tisFile bool\n}\n```\n\n接着看上面的`netFD.init`函数：\n\n```go\nfunc (fd *netFD) init() error {\n    // 这里的pfd实际上就是poll.FD，用来表示一个网络连接或者打开的系统文件\n\treturn fd.pfd.Init(fd.net, true)\n}\n```\n\n我们来看一下`pollFD.Init`：\n\n```go\nfunc (fd *FD) Init(net string, pollable bool) error {\n\t// We don't actually care about the various network types.\n\tif net == \"file\" {\n\t\tfd.isFile = true\n\t}\n \n\tif !pollable {\n\t\tfd.isBlocking = 1\n\t\treturn nil\n\t}\n\t// 这里又有个init，这里的pd是pollDesc类型\n\t// 只有pollable才会调用该方法\n\terr := fd.pd.init(fd)\n\tif err != nil {\n\t\t// If we could not initialize the runtime poller,\n\t\t// assume we are using blocking mode.\n\t\tfd.isBlocking = 1\n\t}\n\treturn err\n}\n```\n\n可以看到上面又有个`init`函数，我们先来看一下`fd.pd`对应的`pollDesc`类型：\n\n```go\ntype pollDesc struct {\n\truntimeCtx uintptr // 这个运行时上下文很重要\n}\n```\n\n我们来看一下`init`函数：\n\n```go\nvar serverInit sync.Once\n\nfunc (pd *pollDesc) init(fd *FD) error {\n\t// 保证runtime_pollServerInit只会执行一次\n\t// 从命名很容易看出来该方法在runtime包中实现\n    serverInit.Do(runtime_pollServerInit)\n\t// 执行runtime_pollOpen\n\t// 只有文件是pollable的时候，才会走到这里\n\t// 该方法实际上是将fd加入到epoll中，该方法在runtime包中实现\n\tctx, errno := runtime_pollOpen(uintptr(fd.Sysfd))\n\tif errno != 0 {\n\t\tif ctx != 0 {\n\t\t\truntime_pollUnblock(ctx)\n\t\t\truntime_pollClose(ctx)\n\t\t}\n\t\treturn syscall.Errno(errno)\n\t}\n    // 保存pollOpen返回的上下文\n\tpd.runtimeCtx = ctx\n\treturn nil\n}\n```\n\n上面这个函数才是关键所在，这里涉及到了`runtime_pollServerInit`和`runtime_pollOpen`两个函数，从命名可以很容易看出这两个函数是在`runtime`包中实现的，然后在链接器链接过来的\n\n先来看一下`runtime_pollServerInit`实现：\n\n```go\nfunc poll_runtime_pollServerInit() {\n\tnetpollinit()\n\tatomic.Store(&netpollInited, 1)\n}\n\nfunc netpollinit() {\n    // 执行系统调用创建epoll\n    // 先尝试使用create1系统调用\n\tepfd = epollcreate1(_EPOLL_CLOEXEC)\n\tif epfd >= 0 {\n\t\treturn\n\t}\n    // 这边的1024是历史原因，只要大于0就好了\n    // 原先epoll底层使用hash表实现，需要传入一个size指定hash表的大小，后面基于rb-tree实现，因此这个参数没有实际意义了，大于0即可\n\tepfd = epollcreate(1024)\n\tif epfd >= 0 {\n\t\tcloseonexec(epfd)\n\t\treturn\n\t}\n\tprintln(\"runtime: epollcreate failed with\", -epfd)\n\tthrow(\"runtime: netpollinit failed\")\n}\n```\n\n很简单，就是创建了一个`epoll`\n\n再来看一下`runtime_pollOpen`的实现：\n\n```go\nfunc poll_runtime_pollOpen(fd uintptr) (*pollDesc, int) {\n\t// 分配一个pollDesc，这个pollDesc是runtime的pollDesc，和上面的pollDesc不是同一个东西，但是他们之间又有关联\n    pd := pollcache.alloc()\n\tlock(&pd.lock)\n\tif pd.wg != 0 && pd.wg != pdReady {\n\t\tthrow(\"runtime: blocked write on free polldesc\")\n\t}\n\tif pd.rg != 0 && pd.rg != pdReady {\n\t\tthrow(\"runtime: blocked read on free polldesc\")\n\t}\n\tpd.fd = fd\n\tpd.closing = false\n\tpd.seq++\n\tpd.rg = 0\n\tpd.rd = 0\n\tpd.wg = 0\n\tpd.wd = 0\n\tunlock(&pd.lock)\n\n\tvar errno int32\n\terrno = netpollopen(fd, pd)\n    // 这里返回了pd的地址，也就是poll.pollDesc中的runtimeCtx实际上保存的就是runtime.pollDesc的地址\n\treturn pd, int(errno)\n}\n\nfunc netpollopen(fd uintptr, pd *pollDesc) int32 {\n\tvar ev epollevent\n\t// 设置需要通知的实际类型，这里设置了边缘触发模式，关于epoll的边缘触发和水平触发模式可以网上有一堆的资料\n\t// 边缘触发和水平触发的本质区别，就是水平触发的话，当事件从epoll的readyList拷贝到用户空间时，会重新加入到readyList，这样下次执行epoll_wait的话，readyList还会有该事件存在（epoll_wait会重新执行file->operations中的poll方法确定是否有事件可以消费）\n\tev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET\n    // 可以看到，这里把pollDesc的地址存到了ev.Data中\n\t*(**pollDesc)(unsafe.Pointer(&ev.data)) = pd\n    // 执行epollctl系统调用，添加socket到epoll中\n\treturn -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &ev)\n}\n```\n\n至此一个`net.Listener`就创建完成了，总结一下主要的逻辑：\n\n1. 创建一个非阻塞`socket`，并执行`bind`和`listen`\n2. 如果没有初始化过`runtime`包的`epoll`，则执行初始化，创建一个`epoll`\n3. 以边缘触发模式将`socket`添加到`epoll`中\n4. 返回封装后的`net.Listener`\n\n### runtime包中的一些注释\n```go\n// Integrated network poller (platform-independent part).\n// A particular implementation (epoll/kqueue) must define the following functions:\n// func netpollinit()\t\t\t// to initialize the poller\n// func netpollopen(fd uintptr, pd *pollDesc) int32\t// to arm edge-triggered notifications\n// and associate fd with pd.\n// An implementation must call the following function to denote that the pd is ready.\n// func netpollready(gpp **g, pd *pollDesc, mode int32)\n\n// pollDesc contains 2 binary semaphores, rg and wg, to park reader and writer\n// goroutines respectively. The semaphore can be in the following states:\n// pdReady - io readiness notification is pending;\n//           a goroutine consumes the notification by changing the state to nil.\n// pdWait - a goroutine prepares to park on the semaphore, but not yet parked;\n//          the goroutine commits to park by changing the state to G pointer,\n//          or, alternatively, concurrent io notification changes the state to READY,\n//          or, alternatively, concurrent timeout/close changes the state to nil.\n// G pointer - the goroutine is blocked on the semaphore;\n//             io notification or timeout/close changes the state to READY or nil respectively\n//             and unparks the goroutine.\n// nil - nothing of the above.\n```\n`pollDesc`的`rg`和`wg`字段，可能的取值情况：\n- `pdReady`：`rg`表示当前有可读事件，`wg`表示可写\n- `pdWait`：表示即将进入等待\n- `G`的指针：需要先进入`pdWait`，然后调用`gopark`，设置等待事件类型，如果是等待读，则设置`rg`，等待写则设置`wg`为当前`G`的指针，然后挂起；当事件到达，`runtime`会将对应的`G`唤醒\n- `0`：其他情况\n\n\n### Accept又是如何执行的呢\n\n接下来我们来看一下执行`Accept`时会发生什么\n\n```go\nfunc (l *TCPListener) Accept() (Conn, error) {\n\tif !l.ok() {\n\t\treturn nil, syscall.EINVAL\n\t}\n\tc, err := l.accept()\n\tif err != nil {\n\t\treturn nil, &OpError{Op: \"accept\", Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err}\n\t}\n\treturn c, nil\n}\n\nfunc (ln *TCPListener) accept() (*TCPConn, error) {\n\tfd, err := ln.fd.accept()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newTCPConn(fd), nil\n}\n```\n\n我们上面创建的是一个`TcpListener`，因此自然是执行对应的`Accept`，可以看到是调用`netFD.Accept`：\n\n```go\nfunc (fd *netFD) accept() (netfd *netFD, err error) {\n    // 执行poll.FD的Accept方法，获取新的客户端连接\n\td, rsa, errcall, err := fd.pfd.Accept()\n\tif err != nil {\n\t\tif errcall != \"\" {\n\t\t\terr = wrapSyscallError(errcall, err)\n\t\t}\n\t\treturn nil, err\n\t}\n\t// 封装netFD\n\tif netfd, err = newFD(d, fd.family, fd.sotype, fd.net); err != nil {\n\t\tpoll.CloseFunc(d)\n\t\treturn nil, err\n\t}\n    // 这里的netFD.init上面分析过了，就是将新的socket加入到epoll中\n\tif err = netfd.init(); err != nil {\n\t\tfd.Close()\n\t\treturn nil, err\n\t}\n\tlsa, _ := syscall.Getsockname(netfd.pfd.Sysfd)\n\tnetfd.setAddr(netfd.addrFunc()(lsa), netfd.addrFunc()(rsa))\n\treturn netfd, nil\n}\n```\n\n接下来看一下`poll.FD`的`Accept`方法：\n\n```go\nfunc (fd *FD) Accept() (int, syscall.Sockaddr, string, error) {\n    // 尝试加锁\n\tif err := fd.readLock(); err != nil {\n\t\treturn -1, nil, \"\", err\n\t}\n\tdefer fd.readUnlock()\n    \n\tif err := fd.pd.prepareRead(fd.isFile); err != nil {\n\t\treturn -1, nil, \"\", err\n\t}\n\tfor {\n        /// 首先尝试直接获取客户端连接\n\t\ts, rsa, errcall, err := accept(fd.Sysfd)\n\t\tif err == nil { // 获取成功，直接返回\n\t\t\treturn s, rsa, \"\", err\n\t\t}\n\t\tswitch err {\n            // 因为我们创建的socket是非阻塞的，当没有新的连接可以accept时会直接返回EAGAIN而不是阻塞\n\t\tcase syscall.EAGAIN:\n            // 如果是可轮询的，表明可以等到epoll事件通知\n\t\t\tif fd.pd.pollable() {\n                // \n\t\t\t\tif err = fd.pd.waitRead(fd.isFile); err == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase syscall.ECONNABORTED:\n\t\t\t// This means that a socket on the listen\n\t\t\t// queue was closed before we Accept()ed it;\n\t\t\t// it's a silly error, so try again.\n\t\t\tcontinue\n\t\t}\n\t\treturn -1, nil, errcall, err\n\t}\n}\n\nfunc accept(s int) (int, syscall.Sockaddr, string, error) {\n    // var Accept4Func func(int, int) (int, syscall.Sockaddr, error) = syscall.Accept4\n    // 首先使用系统调用accept4获取一个非阻塞的socket\n\tns, sa, err := Accept4Func(s, syscall.SOCK_NONBLOCK|syscall.SOCK_CLOEXEC)\n\tswitch err {\n\tcase nil:\n\t\treturn ns, sa, \"\", nil\n\tdefault: // errors other than the ones listed\n\t\treturn -1, sa, \"accept4\", err\n\tcase syscall.ENOSYS: // syscall missing\n\tcase syscall.EINVAL: // some Linux use this instead of ENOSYS\n\tcase syscall.EACCES: // some Linux use this instead of ENOSYS\n\tcase syscall.EFAULT: // some Linux use this instead of ENOSYS\n\t}\n\t// 有些内核不支持accept4\n\tns, sa, err = AcceptFunc(s)\n\tif err == nil {\n\t\tsyscall.CloseOnExec(ns)\n\t}\n\tif err != nil {\n\t\treturn -1, nil, \"accept\", err\n\t}\n    // 设置非阻塞模式\n\tif err = syscall.SetNonblock(ns, true); err != nil {\n\t\tCloseFunc(ns)\n\t\treturn -1, nil, \"setnonblock\", err\n\t}\n\treturn ns, sa, \"\", nil\n}\n```\n\n接着来看`pollDesc.waitRead`实现：\n\n```go\nfunc (pd *pollDesc) waitRead(isFile bool) error {\n\treturn pd.wait('r', isFile)\n}\n\nfunc (pd *pollDesc) wait(mode int, isFile bool) error {\n\tif pd.runtimeCtx == 0 {\n\t\treturn errors.New(\"waiting for unsupported file type\")\n\t}\n    // 又是一个runtime包的方法\n\tres := runtime_pollWait(pd.runtimeCtx, mode)\n\treturn convertErr(res, isFile)\n}\n```\n\n接着看一下`runtime_pollWait`实现：\n\n```go\nfunc poll_runtime_pollWait(pd *pollDesc, mode int) int {\n\terr := netpollcheckerr(pd, int32(mode))\n\tif err != 0 {\n\t\treturn err\n\t}\n\t// As for now only Solaris uses level-triggered IO.\n\tif GOOS == \"solaris\" {\n\t\tnetpollarm(pd, mode)\n\t}\n    // 实际干活的是netpollblock\n\tfor !netpollblock(pd, int32(mode), false) {\n\t\terr = netpollcheckerr(pd, int32(mode))\n\t\tif err != 0 {\n\t\t\treturn err\n\t\t}\n\t\t// Can happen if timeout has fired and unblocked us,\n\t\t// but before we had a chance to run, timeout has been reset.\n\t\t// Pretend it has not happened and retry.\n\t}\n\treturn 0\n}\n\nfunc netpollblock(pd *pollDesc, mode int32, waitio bool) bool {\n    // 这里如果是'r'模式，则gpp是&pd.rg\n    // 'w'模式则是'&pd.wg'\n\tgpp := &pd.rg\n\tif mode == 'w' {\n\t\tgpp = &pd.wg\n\t}\n\n\t// cas操作，设置gpp为pdwait\n\tfor {\n\t\told := *gpp\n\t\tif old == pdReady {\n\t\t\t*gpp = 0\n\t\t\treturn true\n\t\t}\n\t\tif old != 0 {\n\t\t\tthrow(\"runtime: double wait\")\n\t\t}\n\t\tif atomic.Casuintptr(gpp, 0, pdWait) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// 这里直接执行gopark，将当前协程挂起 ^-^\n\tif waitio || netpollcheckerr(pd, mode) == 0 {\n        // 这里netpollblockcommit会被调用，把当前g的引用保存到gpp中，也就是pollDesc的rg或者wg中\n\t\tgopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5)\n\t}\n\t// be careful to not lose concurrent READY notification\n\told := atomic.Xchguintptr(gpp, 0)\n\tif old > pdWait {\n\t\tthrow(\"runtime: corrupted polldesc\")\n\t}\n\treturn old == pdReady\n}\n```\n\n至此，`Accept`的流程也很清晰了：\n\n1. 首先直接尝试通过`socket`执行`accept`来获取可能的客户端连接\n2. 如果此时客户端没有连接，因为`socket`是非阻塞模式，会直接返回`EAGAIN`\n3. 调用`runtime.poll_runtime_pollWait`将当前协程挂起，并且根据是等待读还是等待写将当前`g`的引用保存到`pollDesc`中的`rg`或者`wg`中\n4. 当有新的客户端连接到来时，`epoll`会通知将当前阻塞的协程恢复，然后重新执行第一步\n\n### 那么epoll的wait又是什么时候调用的呢\n\n我们可以在协程的调度逻辑中看到这样一段代码段：\n\n```go\n\tif netpollinited() && atomic.Load(&netpollWaiters) > 0 && atomic.Load64(&sched.lastpoll) != 0 {\n        // 这里的netpoll的参数false表示不阻塞\n\t\tif gp := netpoll(false); gp != nil { \n            // 这里获取的可能是一个列表，将后面多余的g加入调度队列，这里调度一次只能调度一个\n\t\t\tinjectglist(gp.schedlink.ptr())\n            // 设置g为runnable\n\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n\t\t\tif trace.enabled {\n\t\t\t\ttraceGoUnpark(gp, 0)\n\t\t\t}\n\t\t\treturn gp, false\n\t\t}\n\t}\n```\n\n我们来看一下`netpoll`的执行：\n\n```go\nfunc netpoll(block bool) *g {\n\tif epfd == -1 {\n\t\treturn nil\n\t}\n\twaitms := int32(-1)\n    // 调度逻辑中传入的是0\n\tif !block {\n\t\twaitms = 0\n\t}\n\tvar events [128]epollevent\nretry:\n    // 执行epoll_wait系统调用\n\tn := epollwait(epfd, &events[0], int32(len(events)), waitms)\n\tif n < 0 {\n\t\tif n != -_EINTR {\n\t\t\tprintln(\"runtime: epollwait on fd\", epfd, \"failed with\", -n)\n\t\t\tthrow(\"runtime: netpoll failed\")\n\t\t}\n\t\tgoto retry\n\t}\n    // 这里gp是一个链表\n\tvar gp guintptr\n\tfor i := int32(0); i < n; i++ {\n\t\tev := &events[i]\n\t\tif ev.events == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tvar mode int32\n\t\tif ev.events&(_EPOLLIN|_EPOLLRDHUP|_EPOLLHUP|_EPOLLERR) != 0 {\n\t\t\tmode += 'r'\n\t\t}\n\t\tif ev.events&(_EPOLLOUT|_EPOLLHUP|_EPOLLERR) != 0 {\n\t\t\tmode += 'w'\n\t\t}\n\t\tif mode != 0 {\n            // 从ev.data取出pollDesc，还记得上面分析过，在加入epoll时会把对应的pollDesc保存到ev.Data中，而协程阻塞时会把g指针保存在pollDesc中的rg或者wg中\n\t\t\tpd := *(**pollDesc)(unsafe.Pointer(&ev.data))\n\t\t\t// 这里执行netpollready，把对应阻塞的g加到gp链表头部\n\t\t\tnetpollready(&gp, pd, mode)\n\t\t}\n\t}\n\tif block && gp == 0 {\n\t\tgoto retry\n\t}\n\treturn gp.ptr()\n}\n\nfunc netpollready(gpp *guintptr, pd *pollDesc, mode int32) {\n\tvar rg, wg guintptr\n\tif mode == 'r' || mode == 'r'+'w' {\n        // 这里调用了netpollunblock，获取对应的g\n\t\trg.set(netpollunblock(pd, 'r', true))\n\t}\n\tif mode == 'w' || mode == 'r'+'w' {\n\t\twg.set(netpollunblock(pd, 'w', true))\n\t}\n    // 链表设置，将新的g添加到链表头部\n\tif rg != 0 {\n\t\trg.ptr().schedlink = *gpp\n\t\t*gpp = rg\n\t}\n\tif wg != 0 {\n\t\twg.ptr().schedlink = *gpp\n\t\t*gpp = wg\n\t}\n}\n\nfunc netpollunblock(pd *pollDesc, mode int32, ioready bool) *g {\n    // 如果是等待读则rg是阻塞的g的引用\n    // 如果是等待写则wg是阻塞的g的引用\n\tgpp := &pd.rg\n\tif mode == 'w' {\n\t\tgpp = &pd.wg\n\t}\n\n\tfor {\n\t\told := *gpp\n\t\tif old == pdReady {\n\t\t\treturn nil\n\t\t}\n\t\tif old == 0 && !ioready {\n\t\t\t// Only set READY for ioready. runtime_pollWait\n\t\t\t// will check for timeout/cancel before waiting.\n\t\t\treturn nil\n\t\t}\n\t\tvar new uintptr\n\t\tif ioready {\n\t\t\tnew = pdReady\n\t\t}\n        // 状态为ready\n\t\tif atomic.Casuintptr(gpp, old, new) {\n\t\t\tif old == pdReady || old == pdWait {\n\t\t\t\told = 0\n\t\t\t}\n\t\t\treturn (*g)(unsafe.Pointer(old))\n\t\t}\n\t}\n}\n```\n\n可以看到，在执行协程的调度时，会执行`epoll_wait`系统调用，获取已经准备好的`socket`，并唤醒对应的`goroutine`\n\n除了在调度时会执行`epoll_wait`，在后台线程`sysmon`中也会定时执行`epoll_wait`：\n\n```go\nfunc sysmon() {\n\t...\n\tfor {\n\t\t...\n\t\tif netpollinited() && lastpoll != 0 && lastpoll+10*1000*1000 < now {\n\t\t\tatomic.Cas64(&sched.lastpoll, uint64(lastpoll), uint64(now))\n\t\t\tgp := netpoll(false) // non-blocking - returns list of goroutines\n\t\t\tif gp != nil {\n\t\t\t\tincidlelocked(-1)\n\t\t\t\tinjectglist(gp)\n\t\t\t\tincidlelocked(1)\n\t\t\t}\n\t\t}\n\t\t...\n\t}\n}\n```\n\n### 大同小异的读写操作\n\n那么接下来，我们来看一下`Read`操作，实际上`Read`最后会执行\n\n```go\nfunc (c *conn) Read(b []byte) (int, error) {\n   if !c.ok() {\n      return 0, syscall.EINVAL\n   }\n   n, err := c.fd.Read(b)\n   if err != nil && err != io.EOF {\n      err = &OpError{Op: \"read\", Net: c.fd.net, Source: c.fd.laddr, Addr: c.fd.raddr, Err: err}\n   }\n   return n, err\n}\n\nfunc (fd *netFD) Read(p []byte) (n int, err error) {\n\tn, err = fd.pfd.Read(p)\n\truntime.KeepAlive(fd)\n\treturn n, wrapSyscallError(\"read\", err)\n}\n```\n\n最后到了`poll.FD`的`Read`方法：\n\n```go\nfunc (fd *FD) Read(p []byte) (int, error) {\n    // 这里执行对应的加锁操作\n\t...\n\tfor {\n        // 首先尝试直接读，如果无可读内容，因为是非阻塞模式，会返回EAGAIN\n\t\tn, err := syscall.Read(fd.Sysfd, p)\n\t\tif err != nil {\n\t\t\tn = 0\n\t\t\tif err == syscall.EAGAIN && fd.pd.pollable() {\n                // 这里的waitRead有没有似曾相识？这个方法在accept流程的时候已经分析过了，最后会将当前协程挂起\n\t\t\t\tif err = fd.pd.waitRead(fd.isFile); err == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// On MacOS we can see EINTR here if the user\n\t\t\t// pressed ^Z.  See issue #22838.\n\t\t\tif runtime.GOOS == \"darwin\" && err == syscall.EINTR {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\terr = fd.eofError(n, err)\n\t\treturn n, err\n\t}\n}\n```\n\n再来看一下写过程，最后会执行：\n\n```go\nfunc (fd *FD) Write(p []byte) (int, error) {\n    // 这里执行对应的加锁操作\n\t...\n    // 记录已经写入字节数\n\tvar nn int\n\tfor {\n\t\tmax := len(p)\n\t\tif fd.IsStream && max-nn > maxRW {\n\t\t\tmax = nn + maxRW\n\t\t}\n\t\tn, err := syscall.Write(fd.Sysfd, p[nn:max])\n\t\tif n > 0 {\n\t\t\tnn += n\n\t\t}\n        // 写入方法与读方法的区别在于，读方法只要读取到内容就会返回\n        // 而写入需要将传入的字节切片全部写入才返回\n\t\tif nn == len(p) {\n\t\t\treturn nn, err\n\t\t}\n        // 这里的waitWrite和上面的waitRead类似\n\t\tif err == syscall.EAGAIN && fd.pd.pollable() {\n\t\t\tif err = fd.pd.waitWrite(fd.isFile); err == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nn, err\n\t\t}\n\t\tif n == 0 {\n\t\t\treturn nn, io.ErrUnexpectedEOF\n\t\t}\n\t}\n}\n\n// 其实最后都是调用的pd.wait\nfunc (pd *pollDesc) waitWrite(isFile bool) error {\n\treturn pd.wait('w', isFile)\n}\n\n// 最终调用runtime_pollWait将当前协程挂起\nfunc (pd *pollDesc) wait(mode int, isFile bool) error {\n\tif pd.runtimeCtx == 0 {\n\t\treturn errors.New(\"waiting for unsupported file type\")\n\t}\n\tres := runtime_pollWait(pd.runtimeCtx, mode)\n\treturn convertErr(res, isFile)\n}\n```\n\n### 差点被遗忘的close\n\n接着来看一下`Close`方法，实际执行的是：\n\n```go\nfunc (c *conn) Close() error {\n\tif !c.ok() {\n\t\treturn syscall.EINVAL\n\t}\n    // 这里执行netFD.Close\n\terr := c.fd.Close()\n\tif err != nil {\n\t\terr = &OpError{Op: \"close\", Net: c.fd.net, Source: c.fd.laddr, Addr: c.fd.raddr, Err: err}\n\t}\n\treturn err\n}\n\nfunc (fd *netFD) Close() error {\n    // 清除finalizer\n\truntime.SetFinalizer(fd, nil)\n    // 调用poll.FD的Close方法\n\treturn fd.pfd.Close()\n}\n\n\nfunc (fd *FD) Close() error {\n\tif !fd.fdmu.increfAndClose() {\n\t\treturn errClosing(fd.isFile)\n\t}\n\n\t// 这里evict方法唤醒所有阻塞读写的g\n\tfd.pd.evict()\n\t// 减少引用，如果引用为0则关闭\n\terr := fd.decref()\n\n\tif fd.isBlocking == 0 {\n\t\truntime_Semacquire(&fd.csema)\n\t}\n\n\treturn err\n}\n\nfunc (pd *pollDesc) evict() {\n\tif pd.runtimeCtx == 0 {\n\t\treturn\n\t}\n\truntime_pollUnblock(pd.runtimeCtx)\n}\n\nfunc poll_runtime_pollUnblock(pd *pollDesc) {\n\tlock(&pd.lock)\n\tif pd.closing {\n\t\tthrow(\"runtime: unblock on closing polldesc\")\n\t}\n\tpd.closing = true\n\tpd.seq++\n\tvar rg, wg *g\n\tatomicstorep(unsafe.Pointer(&rg), nil)\n    // 获取阻塞的g\n\trg = netpollunblock(pd, 'r', false)\n\twg = netpollunblock(pd, 'w', false)\n\tif pd.rt.f != nil {\n\t\tdeltimer(&pd.rt)\n\t\tpd.rt.f = nil\n\t}\n\tif pd.wt.f != nil {\n\t\tdeltimer(&pd.wt)\n\t\tpd.wt.f = nil\n\t}\n\tunlock(&pd.lock)\n\tif rg != nil {\n        // 调用goready唤醒g\n\t\tnetpollgoready(rg, 3)\n\t}\n\tif wg != nil {\n        // 唤醒g\n\t\tnetpollgoready(wg, 3)\n\t}\n}\n\n\nfunc (fd *FD) decref() error {\n\t// 减少引用，如果引用为0，则返回true\n    if fd.fdmu.decref() {\n        // 关闭连接\n\t\treturn fd.destroy()\n\t}\n\treturn nil\n}\n\nfunc (fd *FD) destroy() error {\n\t// 调用runtime_pollClose方法\n\tfd.pd.close()\n    // var CloseFunc func(int) error = syscall.Close\n    // 这里的CloseFunc就是系统调用close\n\terr := CloseFunc(fd.Sysfd)\n\tfd.Sysfd = -1\n\truntime_Semrelease(&fd.csema)\n\treturn err\n}\n\nfunc (pd *pollDesc) close() {\n\tif pd.runtimeCtx == 0 {\n\t\treturn\n\t}\n\truntime_pollClose(pd.runtimeCtx)\n\tpd.runtimeCtx = 0\n}\n\nfunc poll_runtime_pollClose(pd *pollDesc) {\n\tif !pd.closing {\n\t\tthrow(\"runtime: close polldesc w/o unblock\")\n\t}\n\tif pd.wg != 0 && pd.wg != pdReady {\n\t\tthrow(\"runtime: blocked write on closing polldesc\")\n\t}\n\tif pd.rg != 0 && pd.rg != pdReady {\n\t\tthrow(\"runtime: blocked read on closing polldesc\")\n\t}\n    // 从epoll中删除fd\n\tnetpollclose(pd.fd)\n    // 释放pollDesc\n\tpollcache.free(pd)\n}\n\nfunc netpollclose(fd uintptr) int32 {\n\tvar ev epollevent\n    // 系统调用epoll_ctl删除对应的fd\n\treturn -epollctl(epfd, _EPOLL_CTL_DEL, int32(fd), &ev)\n}\n```\n\n综上，关闭一个连接时：\n\n1. 设置pollDesc相关flag为已关闭，唤醒该连接上阻塞的协程\n2. 减少对应poll.FD的引用，如果引用为0，则只需真正的关闭\n3. 执行关闭操作，先从epoll删除对应的fd，然后执行close系统调用关闭\n\n### 最后\n\n可以看到，`go`使用非阻塞IO来防止大量系统线程阻塞带来的上下文切换，取而代之的是让轻量级的协程阻塞在IO事件上，然后通过`epoll`来实现IO事件通知，唤醒阻塞的协程。","tags":["go"]},{"title":"golang内存逃逸","url":"/2019/03/31/golang内存逃逸/","content":"\n`golang`中，编译器在编译时会通过内存逃逸分析确定变量分配在堆上还是栈上。\n\n```go\ntype Dog struct {\n}\n\nfunc (d *Dog) Eat() {\n}\n\ntype Animal interface {\n\tEat()\n}\n\nfunc main() {\n\tdog1 := new(Dog)\n\tnoneEscape(dog1)\n\tnop(dog1)\n\tdog2 := new(Dog)\n\tescape(dog2)\n\tdog3 := Dog{}\n\tfmt.Println(&dog3)\n}\n\n//go:noinline\nfunc nop( a Animal){\n}\n\n//go:noinline\nfunc noneEscape(d *Dog) {\n\td.Eat()\n}\n\n//go:noinline\nfunc escape(a Animal) {\n\ta.Eat()\n}\n\n```\n\n可以通过`--gcflags=\"-m -m\"`参数，在编译时打印出内存逃逸分析信息，`-m`最多可以指定四个，越多打印的信息越详细。上面代码中的`go:noinline`用于告诉编译器禁止对该函数进行内联优化。\n\n运行：\n\n```sh\n$ go build -gcflags=\"-m -m\" .\n```\n\n查看打印结果：\n\n```\n# just-for-fun/escape\n.\\main.go:8:6: can inline (*Dog).Eat as: method(*Dog) func() {  }\n.\\main.go:30:6: cannot inline noneEscape: marked go:noinline\n.\\main.go:31:7: inlining call to (*Dog).Eat method(*Dog) func() {  }\n.\\main.go:26:6: cannot inline nop: marked go:noinline\n.\\main.go:35:6: cannot inline escape: marked go:noinline\n.\\main.go:15:6: cannot inline main: function too complex: cost 355 exceeds budget 80\n.\\main.go:8:7: (*Dog).Eat d does not escape\n.\\main.go:30:17: noneEscape d does not escape\n.\\main.go:26:11: nop a does not escape\n.\\main.go:35:13: leaking param: a\n.\\main.go:35:13:        from a.Eat() (receiver in indirect call) at .\\main.go:36:7\n.\\main.go:20:8: dog2 escapes to heap\n.\\main.go:20:8:         from dog2 (passed to call[argument escapes]) at .\\main.go:20:8\n.\\main.go:19:13: new(Dog) escapes to heap\n.\\main.go:19:13:        from dog2 (assigned) at .\\main.go:19:7\n.\\main.go:19:13:        from dog2 (interface-converted) at .\\main.go:20:8\n.\\main.go:19:13:        from dog2 (passed to call[argument escapes]) at .\\main.go:20:8\n.\\main.go:22:14: &dog3 escapes to heap\n.\\main.go:22:14:        from ... argument (arg to ...) at .\\main.go:22:13\n.\\main.go:22:14:        from *(... argument) (indirection) at .\\main.go:22:13\n.\\main.go:22:14:        from ... argument (passed to call[argument content escapes]) at .\\main.go:22:13\n.\\main.go:22:14: &dog3 escapes to heap\n.\\main.go:22:14:        from &dog3 (interface-converted) at .\\main.go:22:14\n.\\main.go:22:14:        from ... argument (arg to ...) at .\\main.go:22:13\n.\\main.go:22:14:        from *(... argument) (indirection) at .\\main.go:22:13\n.\\main.go:22:14:        from ... argument (passed to call[argument content escapes]) at .\\main.go:22:13\n.\\main.go:21:2: moved to heap: dog3\n.\\main.go:16:13: main new(Dog) does not escape\n.\\main.go:18:5: main dog1 does not escape\n.\\main.go:22:13: main ... argument does not escape\n<autogenerated>:1: leaking param: .this\n<autogenerated>:1:      from .this.Eat() (receiver in indirect call) at <autogenerated>:1\n```\n\n从上面的信息中可以看到，`dog1`分配在栈上，而`dog2`和`dog3`都分配在堆上。\n\n`dog1`在方法`noneEscape`中，调用了`Eat`方法，因为`Eat`方法并没有发生内存逃逸，因此`dog1`在`noneEscape`中没有内存逃逸。而`nop`方法内`dog1`没有执行任何操作，也不会发生内存逃逸。可见，即使是使用`new`分配的变量，也不一定是分配在堆上。\n\n而`dog2`在方法`escape`中，调用了`Eat`方法，因为这时候`dog2`是`Animal`接口类型，`golang`中接口类型的方法是动态派发的，编译器并不知道具体调用的是哪个`Eat`方法，从而无法确定`dog2`在`Eat`是否有发生内存逃逸。在这种情况下，编译器会认为`dog2`发生了内存逃逸，并将其分配在堆上。如果编译器能够在编译时就对接口的实际类型进行分析，对`Eat`方法进行静态派发，就可以发现`dog2`并没有内存逃逸。\n\n\n\n### fun thing\n\n代码1：\n\n```go\nfunc main() {\n   byts := []byte(\"\")\n   s1 := append(byts,'a')\n   s2:= append(byts,'b')\n   fmt.Println(string(s1),string(s2)) // b b\n}\n```\n\n代码2：\n\n```go\nfunc main() {\n   byts := []byte(\"\")\n   s1 := append(byts,'a')\n   s2:= append(byts,'b')\n   fmt.Println(byts)\n   fmt.Println(string(s1),string(s2)) // a b\n}\n```\n\n观察上面两个程序，只是添加了一行代码，两次执行结构却完全不同。导致结果完全不同的原因在于，`fmt.Println(byts)`导致`byts`逃脱到堆上。\n\n查看两份代码编译后的`[]byte(\"\")`对应的汇编代码片段：\n\n代码1：\n\n```asm\n0x0036 00054 (main.go:6)\tLEAQ\t\"\"..autotmp_6+120(SP), AX\n0x003b 00059 (main.go:6)\tPCDATA\t$2, $0\n0x003b 00059 (main.go:6)\tMOVQ\tAX, (SP)\n0x003f 00063 (main.go:6)\tXORPS\tX0, X0\n0x0042 00066 (main.go:6)\tMOVUPS\tX0, 8(SP)\n0x0047 00071 (main.go:6)\tCALL\truntime.stringtoslicebyte(SB)\n```\n\n代码2：\n\n```asm\n0x0036 00054 (main.go:6)\tMOVQ\t$0, (SP)\n0x003e 00062 (main.go:6)\tXORPS\tX0, X0\n0x0041 00065 (main.go:6)\tMOVUPS\tX0, 8(SP)\n0x0046 00070 (main.go:6)\tCALL\truntime.stringtoslicebyte(SB)\n```\n\n可以看到，两次调用`runtime.stringtoslicebyte`时传递的参数不同，第一次的第一个参数是非空的，而第二次的第一次参数的空的，查看该函数实现：\n\n```go\nconst tmpStringBufSize = 32\n\ntype tmpBuf [tmpStringBufSize]byte\n\nfunc stringtoslicebyte(buf *tmpBuf, s string) []byte {\n\tvar b []byte\n    // 如果buf不为空，并且len(buf)小于len(s)，会直接使用buf作为底层数组，而buf的长度为32\n\tif buf != nil && len(s) <= len(buf) {\n\t\t*buf = tmpBuf{}\n\t\tb = buf[:len(s)]\n\t} else {\n\t\tb = rawbyteslice(len(s))\n\t}\n\tcopy(b, s)\n\treturn b\n}\n\n// rawbyteslice allocates a new byte slice. The byte slice is not zeroed.\nfunc rawbyteslice(size int) (b []byte) {\n\tcap := roundupsize(uintptr(size))\n\tp := mallocgc(cap, nil, false)\n\tif cap != uintptr(size) {\n\t\tmemclrNoHeapPointers(add(p, uintptr(size)), cap-uintptr(size))\n\t}\n\n\t*(*slice)(unsafe.Pointer(&b)) = slice{p, size, int(cap)}\n\treturn\n}\n```\n\n通过上面的代码，我们可以得知，代码1中的`byts`，`cap`为`32`，而代码2中的`byts`，`cap`为0。\n\n因为在代码1中，`cap`为`32`，则两次`append`都不会对底层数组发送重分配，而且都是修改第一个元素，因此第二次操作会覆盖第一次操作。而在代码2中，每次`append`都会重新分配一个底层数组。\n\n\n\n","tags":["go"]},{"title":"go map分析","url":"/2019/03/30/go-map分析/","content":"\n`map`其实就是一个`hash table`，今天我们来看一下`go`中`map`的实现，相关代码位于`runtime/map.go`中。\n\n### 结构定义\n\n我们首先来看一下`map`的相关结构定义\n\n```go\ntype hmap struct {\n\tcount     int // 当前map中存放的元素，我们可以通过内置函数`len`来获取\n\tflags     uint8\n\tB         uint8  // 当前map的backet数量为2^B，其中最大能够存放loadFactor * 2^B个元素，当超过这个阈值时就会进行扩容，loadFactor默认为13/2，这个值是全局定义的常量\n\tnoverflow uint16 // 总的overflow的数量\n\thash0     uint32 // hash seed\n\n\tbuckets    unsafe.Pointer // bucket数组，长度为2^B，这里bucket实际是bmap\n\toldbuckets unsafe.Pointer // 如果发生扩容，旧的buckets就会保存到oldbuckets，在后续的操作中会慢慢迁移到新的buckets中\n\tnevacuate  uintptr        // 扩容时需要从原来的buckets将数据迁移到新的buckets中，该字段表示小于该数值的buckets当前都已经迁移完成\n\n\textra *mapextra // 如果map中保存的key和value都没有包含指针，那么gc时就不需要对buckets里面的内容进行扫描，但是每个bucket本质上是一个链表，buckets头部保存的是每个bucket链表的头节点，这时候会将每个链表的后续节点保存到该字段内，从而gc时可以对这些后续节点进行扫描，防止被回收\n}\n```\n**`map`的key和value，如果都没有包含指针，那么会对其进行优化，`gc`的时候就不需要去扫描每个键值对了**\n\n上面`extra`字段对应的`mapextra`类型：\n\n```go\ntype mapextra struct {\n\toverflow    *[]*bmap // 对应buckets\n\toldoverflow *[]*bmap // 对应oldbuckets\n\n\t// nextOverflow holds a pointer to a free overflow bucket.\n\tnextOverflow *bmap // 分配bmap时，可能会预先分配一些，当需要时可以直接从这里获取\n}\n```\n\n上面说的，每个`bucket`实际上是一个`bmap`链表，而`hmap`中的`buckets`是一个`bmap`链表数组，这实际上就是[开散列](https://en.wikipedia.org/wiki/Hash_table#Open_addressing)。和普通的开散列不同的是，**一个`bmap`中保存了8个键值对**，下面来看一下`bmap`的定义：\n\n```go\n// A bucket for a Go map.\n// bucket实际上是一个bmap\ntype bmap struct {\n\t// 这里bucketCnt是一个全局声明的常量，大小为8，也就是限制每个bmap中保存8个键值对\n    // 当判断一个key是否在当前bmap中时，会先获取这个key的hash的高8位，然后在tophash中查找是否有匹配的索引，如果有再进一步比较key是否相同，如果当前bmap中没有找到，则到下一个bmap中查找\n\ttophash [bucketCnt]uint8\n\t// tophash后面紧跟着保存在当前bmap中的8个key和8个value\n    // 因为不同类型的key和value内存大小是不同的，这里需要在运行时根据指针运算来访问\n    // 在bmap中是按照 `k1,k2,..,k8,v1,v2,...,v8` 这样来排列的，而不是按照直观上的`k1,v1,k2,v2,...,k8,v8`这样来排列，主要是为了减少内存对齐时额外的内存开销\n    // 8个键值对之后还有一个overflow指针，用来链接下一个bmap，正如上面说的，每个bucket实际上是一个bmap链表，这里通过overflow链接的这些bmap被称为`overflow bucket`\n}\n```\n> 因为bmap链接下一个bmap的overflow指针在末尾，而不同类型的key和value的内存大小又不同，因此无法直接获取到下一个bmap的地址，前面说过，当key和value不包含指针的时候，gc时不需要扫描这些键值对；但是又需要扫描这些bmap，因此这种情况下会把这些bmap存到mapextra字段中的`overflow和oldoverflow中，这样gc时直接遍历这两个切片就好了。\n那为什么不把overflow放到bmap头部呢？个人觉得可能是为了让内存访问更友好吧，连续的内存访问肯定更加高效。\n\n\n如上面`bmap`所见，键值对并没有显示声明出来，而是需要在运行时根据指针运算来访问，这里来看一下一个全局声明的常量`dataOffset`，这个常量在后续会经常看到：\n\n```go\nconst(\n    ...\n\tdataOffset = unsafe.Offsetof(struct {\n\t\tb bmap\n\t\tv int64\n\t}{}.v)\n    ...\n)\n```\n\n因为`bmap`实际上只声明了一个`uint8`类型的数组，因此这里`v`字段的偏移量就是实际`bmap`中第一个`key`值的偏移量\n\n这里假设`bmap`的地址是`bptr`，`key`的类型大小是`ksize`，`val`的类型大小是`vsize`，那么访问第`i`个`key`和`val`的地址可以通过下面公式计算（`i`从`0`开始计算）：\n\n```\nith key: bptr + dataOffset + i * ksize\nith val: bptr + dataOffset + bucketCnt * ksize + i * vsize\n```\n\n至此，我们对`map`的结构有了一个大体的了解，其键值对的存储结构大致可以用下图来描述：\n\n```\n    bucket数组         bmap中的overflow指向下一个bmap \n  实际上是bmap数组    这些由overflow引用的bucket被称作overflow bucket\n|-----------------|  |-----------------|\n|k1,..,k8,v1,..,v8|->|k1,..,k8,v1,..,v8|-> ...\t\t  \n|-----------------|  |-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n```\n\n\n\n> 从上面可以看到，一个`bucket`实际上是一个`bmap`，而且`bmap`可以通过`overflow`指针来形成链表，这些通过`overflow`引用的`bmap`被称作`overflow bucket`，而`buckets`中`bmap`的称为`bucket`。`bucket`实际上也可以认为是整条`bmap`链表，比如扩容时的`bucket`迁移，实际上就是迁移整条`bmap`链表。\n\n\n### map创建\n\n当我们使用`make`创建`map`时，可以指定一个`size`参数，当我们没有指定`size`时，或者`size`在编译时已知是一个不大于`8`的数值时（也就是size是一个常量），会通过`makemap_small`来创建一个`map`：\n\n```go\n// makehmap_small implements Go map creation for make(map[k]v) and\n// make(map[k]v, hint) when hint is known to be at most bucketCnt\n// at compile time and the map needs to be allocated on the heap.\nfunc makemap_small() *hmap {\n\th := new(hmap)\n\th.hash0 = fastrand() // 初始化hash seed\n\treturn h\n}\n```\n\n可以看到，通过`makemap_small`创建的`map`，此时的`B`是0，`buckets`是`nil`，在第一次写入的时候才会去创建具体的`buckets`\n\n对应的，当我们指定了一个合适的`size`时，会通过另一个函数来创建`map`：\n\n```go\n// makemap implements Go map creation for make(map[k]v, hint).\n// If the compiler has determined that the map or the first bucket\n// can be created on the stack, h and/or bucket may be non-nil.\n// If h != nil, the map can be created directly in h.\n// If h.buckets != nil, bucket pointed to can be used as the first bucket.\nfunc makemap(t *maptype, hint int, h *hmap) *hmap {\n    // 如果小于0或者过大，则设为0\n\tif hint < 0 || hint > int(maxSliceCap(t.bucket.size)) {\n\t\thint = 0\n\t}\n\n\t// initialize Hmap\n    // 编译器可能会进行优化，在栈上创建map，因此h可能不为空\n\tif h == nil {\n\t\th = new(hmap)\n\t}\n\th.hash0 = fastrand()\n\n\t// find size parameter which will hold the requested # of elements\n\tB := uint8(0)\n    // 根据传入的size来计算B的大小，这里的B是buckets的数量，前文说过，map最多可以容纳 0.65*2^B 个键值对，当超过这个阈值时，会执行扩容\n\tfor overLoadFactor(hint, B) { \n\t\tB++\n\t}\n\th.B = B\n\n\t// allocate initial hash table\n\t// if B == 0, the buckets field is allocated lazily later (in mapassign)\n\t// If hint is large zeroing this memory could take a while.\n\tif h.B != 0 {\n\t\tvar nextOverflow *bmap\n        // 创建bucket数组，这里可能会预先分配几个bmap，后续可以直接使用而不需要执行内存分配\n\t\th.buckets, nextOverflow = makeBucketArray(t, h.B, nil)\n\t\tif nextOverflow != nil {\n\t\t\th.extra = new(mapextra)\n\t\t\th.extra.nextOverflow = nextOverflow\n\t\t}\n\t}\n\n\treturn h\n}\n```\n\n当我们创建一个`map`时，如果`map`中要存储的键值对数据可以估计的话，最好在创建的时候指定好`size`，这样可以预先分配好`bucket`的数量。\n\n\n\n### map的访问\n\n通常我们对`map`的访问有`get`、`set`、`delete`、和`for range`操作，在分析具体的实现下时，我们要先来了解一下如果定位一个`key`的位置，这里先不涉及在执行扩容时的场景。\n\n我们再回顾一下图：\n\n```\n    bucket数组         bmap中的overflow指向下一个bmap \n  实际上是bmap数组    这些由overflow引用的bucket被称作overflow bucket\n|-----------------|  |-----------------|\n|k1,..,k8,v1,..,v8|->|k1,..,k8,v1,..,v8|-> ...\t\t  \n|-----------------|  |-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n|k1,..,k8,v1,..,v8|-> ...\n|-----------------|\n```\n\n要定位一个`key`，我们需要先确定这个`key`是落在哪个`bucket`中，也就是哪一条`bmap`链表中，通过计算`key`的哈希值，然后通过`hash%bucket数量`的来定位 ，具体的逻辑如下：\n\n```go\nalg := t.key.alg // 这里的t是map的类型描述\nhash := alg.hash(key, uintptr(h.hash0)) // 计算hash值\nm := bucketMask(h.B) // 这里的m为 1<<B-1\n// 这里的hash&m等价于求模操作\nb := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + (hash&m)*uintptr(t.bucketsize))) \n```\n\n当定位到具体的`bucket`后，只需要从头开始遍历该`bmap`链表，因为`bmap`中有个`tophash`数组，保存了每个`key`对应的哈希值的高八位，因此我们不需要挨个对每个`key`进行比较，只需要先比较哈希值的高八位是否相同就行了，如果相同才接着比较`key`是否相等，具体代码如下：\n\n```go\ntop := tophash(hash) // 取key的hash值的高八位\n\t// 这里b是一个bmap指针，如果当前bmap没有，则查找下一个bmap\n\tfor ; b != nil; b = b.overflow(t) { \n        // 遍历当前bmap，这里bucketCnt是常量8，限制一个bmap中最多只有8个键值对\n\t\tfor i := uintptr(0); i < bucketCnt; i++ {\n            // 先比较哈希值高八位，如果不相等则continue\n\t\t\tif b.tophash[i] != top {\n\t\t\t\tcontinue\n\t\t\t}\n            // 计算当前的key的位置\n\t\t\tk := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))\n            // 是否间接存储（存储的是实际key的地址）\n\t\t\tif t.indirectkey {\n\t\t\t\tk = *((*unsafe.Pointer)(k))\n\t\t\t}\n            // 比较key是否相等\n\t\t\tif alg.equal(key, k) {\n\t\t\t\t......\n\t\t\t}\n\t\t}\n\t}\n```\n\n##### get\n\n与`get`相关的方法有多个：\n\n```go\n// val := m[key]\nfunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer\n// val,has := m[key]\nfunc mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) \n// for-range时，用于获取key和val\nfunc mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)\n```\n\n这三个方法大同小异，这里只分析第一个方法：\n\n先看该函数的注释，如果`key`不存在时，返回的是对应的`val`类型的空值。\n\n```go\n// mapaccess1 returns a pointer to h[key].  Never returns nil, instead\n// it will return a reference to the zero object for the value type if\n// the key is not in the map.\n// NOTE: The returned pointer may keep the whole map live, so don't\n// hold onto it for very long.\nfunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer {\n    // 如果没有内容，则返回对应类型的零值\n\tif h == nil || h.count == 0 {\n\t\treturn unsafe.Pointer(&zeroVal[0])\n\t}\n    // map不是并发安全的，如果存在写操作则panic，因此我们需要在多个goroutine对map进行并发读写时，需要加锁保护\n\tif h.flags&hashWriting != 0 {\n\t\tthrow(\"concurrent map read and map write\")\n\t}\n\talg := t.key.alg\n    // 计算key的哈希值\n\thash := alg.hash(key, uintptr(h.hash0))\n    // 获取buckets数组长度的掩码\n\tm := bucketMask(h.B)\n    // 定位key所在的bucket\n\tb := (*bmap)(add(h.buckets, (hash&m)*uintptr(t.bucketsize)))\n    // 当前是否处于扩容操作，当发生扩容操作时，或重新分配buckets数组，并且需要将旧的buckets内的键值对迁移到新的buckets数组中，这个迁移过程不是一次性完成的而是分批完成的\n\tif c := h.oldbuckets; c != nil {\n       \t// 扩容操作有两种：第一种，如果是因为当前的键值对数量已经达到了阈值0.65*2^B，则会触发扩容，这时候新的buckets数组的数量为原来的2倍；第二种是当前键值对数量并没有达到阈值，但是当前存在太多的overflow bucket，可能是因为之前存储了太多的键值对，但是后面又被删除掉了，这时候有的bucket链表会比较长，但是实际上存储的键值对比较稀疏，这样会影响查找效率，这种情况会触发sameSizeGrow，即扩容时buckets数组长度与原来一致\n        // 如果是前一种，则原先的buckets数量的掩码是m>>1\n\t\tif !h.sameSizeGrow() {\n\t\t\t// There used to be half as many buckets; mask down one more power of two.\n\t\t\tm >>= 1\n\t\t}\n        // 计算该key落在的oldbuckets中的哪个bucket中\n\t\toldb := (*bmap)(add(c, (hash&m)*uintptr(t.bucketsize)))\n        // 如果当前bucket还没有迁移，则在该bucket中查询，迁移操作是按照bucket为单位进行的\n\t\tif !evacuated(oldb) {\n\t\t\tb = oldb\n\t\t}\n\t}\n    // 获取哈希值高八位\n\ttop := tophash(hash)\n    // 遍历bmap链表\n\tfor ; b != nil; b = b.overflow(t) {\n\t\tfor i := uintptr(0); i < bucketCnt; i++ {\n            // 比较哈希值高八位\n\t\t\tif b.tophash[i] != top {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tk := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))\n\t\t\tif t.indirectkey {\n\t\t\t\tk = *((*unsafe.Pointer)(k))\n\t\t\t}\n            // 比较key是否相等\n\t\t\tif alg.equal(key, k) {\n                // 获取key对应的val，这里的计算公式在前面结构定义一节中已经说明过\n\t\t\t\tv := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))\n                // 如果这里存储的是val的地址\n\t\t\t\tif t.indirectvalue {\n\t\t\t\t\tv = *((*unsafe.Pointer)(v))\n\t\t\t\t}\n\t\t\t\treturn v\n\t\t\t}\n\t\t}\n\t}\n\treturn unsafe.Pointer(&zeroVal[0])\n}\n```\n\n##### set \n\n`set`操作对应的方法是`mapassign`，这个方法为指定的`key`分配一个用于存放`val`的槽，或者返回已经存在的槽，新的`val`直接写入这个槽中，即完成了`map`的`set`操作。\n\n在写入时，因为可能该`key`已经存在，因此需要先遍历目标`bucket`的`bmap`链表，具体代码如下：\n\n```go\n// Like mapaccess, but allocates a slot for the key if it is not present in the map.\nfunc mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer {\n\t// 不允许写入空map\n    if h == nil {\n\t\tpanic(plainError(\"assignment to entry in nil map\"))\n\t}\n    // 如果有并发写操作，直接panic\n\tif h.flags&hashWriting != 0 {\n\t\tthrow(\"concurrent map writes\")\n\t}\n\talg := t.key.alg\n    // 计算key的哈希值\n\thash := alg.hash(key, uintptr(h.hash0))\n\n\t// Set hashWriting after calling alg.hash, since alg.hash may panic,\n\t// in which case we have not actually done a write.\n    // 设置写标志\n\th.flags |= hashWriting\n\t// 如果buckets数组为空，则会创建一个长度为1的buckets数组\n\tif h.buckets == nil {\n\t\th.buckets = newobject(t.bucket) // newarray(t.bucket, 1)\n\t}\n\nagain:\n    // 计算目标bucket在buckets数组中的索引\n\tbucket := hash & bucketMask(h.B)\n    // 如果当前正在执行扩容操作，则会执行迁移操作，前面说过扩容时的迁移操作是分批进行的，而迁移是按照bucket为单位进行的，而触发迁移的时机是执行写操作\n\tif h.growing() {\n        // 该函数会执行迁移操作，迁移的目标是bucket%len_of_oldbuckets\n\t\tgrowWork(t, h, bucket) \n\t}\n    // 计算要写入的bucket\n\tb := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize)))\n    // 计算哈希值高八位\n\ttop := tophash(hash)\n\t// 在bmap中要插入的索引，如果前文所说，一个bmap最多可以有8个键值对\n\tvar inserti *uint8\n    // 要插入key的地址\n\tvar insertk unsafe.Pointer\n    // 对应的val的地址\n\tvar val unsafe.Pointer\n    // 因为前面已经执行过扩容操作，因此写入只需要对新的bucket进行操作\n\tfor {\n        // 遍历当前bmap\n\t\tfor i := uintptr(0); i < bucketCnt; i++ {\n            // 高八位哈希值不相同\n\t\t\tif b.tophash[i] != top {\n                // 对第一个遇到的空槽，设置为待定槽，因为可能在后续遍历中发现当前key已经存在，则直接返回对应的val就好了\n\t\t\t\tif b.tophash[i] == empty && inserti == nil {\n\t\t\t\t\tinserti = &b.tophash[i]\n\t\t\t\t\tinsertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))\n\t\t\t\t\tval = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n            // 这里说明哈希值高八位一致，因此需要比较key是否相等\n\t\t\tk := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))\n\t\t\tif t.indirectkey {\n\t\t\t\tk = *((*unsafe.Pointer)(k))\n\t\t\t}\n            // 如果不相等，继续遍历下一个\n\t\t\tif !alg.equal(key, k) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// 如果key已经存在，直接返回对应的val地址就好了\n\t\t\tif t.needkeyupdate {\n\t\t\t\ttypedmemmove(t.key, k, key)\n\t\t\t}\n\t\t\tval = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))\n            // goto大法好\n\t\t\tgoto done\n\t\t}\n        // 当前bmap没有找到对应的key，则继续遍历下一个bmap\n\t\tovf := b.overflow(t)\n\t\tif ovf == nil { // 如果当前bucket已经遍历完成，则跳出\n\t\t\tbreak\n\t\t}\n\t\tb = ovf\n\t}\n\t\n    // 首先判断是否需要进行扩容：如果当前没有扩容操作正在执行并且数据达到阈值或者存在太多的bmap，则会触发扩容操作\n\tif !h.growing() && (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) {\n        // 触发扩容操作，然后跳到开头重新执行上面流程\n\t\thashGrow(t, h)\n\t\tgoto again // Growing the table invalidates everything, so try again\n\t}\n\t// 如果不需要扩容操作，并且没有可以使用的空槽，则需要分配一个新的overflow bucket，这里是分配在链表尾部，当前的b指向的就是整条bmap链表的最后一个节点\n\tif inserti == nil {\n\t\t// all current buckets are full, allocate a new one.\n\t\tnewb := h.newoverflow(t, b)\n        // 插入第一个槽中\n\t\tinserti = &newb.tophash[0]\n\t\tinsertk = add(unsafe.Pointer(newb), dataOffset)\n\t\tval = add(insertk, bucketCnt*uintptr(t.keysize))\n\t}\n\n\t// store new key/value at insert position\n    // 设置key\n    // 这里针对是否需要对key或者val进行间接存储的处理\n\tif t.indirectkey {\n\t\tkmem := newobject(t.key)\n\t\t*(*unsafe.Pointer)(insertk) = kmem\n\t\tinsertk = kmem\n\t}\n\tif t.indirectvalue {\n\t\tvmem := newobject(t.elem)\n\t\t*(*unsafe.Pointer)(val) = vmem\n\t}\n\ttypedmemmove(t.key, insertk, key)\n\t*inserti = top\n\th.count++\t// map中元素数据加1\n\ndone:\n    // 检查写操作标志位\n\tif h.flags&hashWriting == 0 {\n\t\tthrow(\"concurrent map writes\")\n\t}\n    // 清除写操作\n\th.flags &^= hashWriting\n\tif t.indirectvalue {\n\t\tval = *((*unsafe.Pointer)(val))\n\t}\n\treturn val\n}\n```\n\n##### delete\n\n删除操作流程如下：\n\n```go\nfunc mapdelete(t *maptype, h *hmap, key unsafe.Pointer) {\n   if h == nil || h.count == 0 {\n      return\n   }\n    // 设置写操作标志位\n   if h.flags&hashWriting != 0 {\n      throw(\"concurrent map writes\")\n   }\n\n   alg := t.key.alg\n   hash := alg.hash(key, uintptr(h.hash0))\n\n   // Set hashWriting after calling alg.hash, since alg.hash may panic,\n   // in which case we have not actually done a write (delete).\n   h.flags |= hashWriting\n\n   bucket := hash & bucketMask(h.B)\n    // 如果正在扩容，先执行迁移操作\n   if h.growing() {\n      growWork(t, h, bucket)\n   }\n   b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize)))\n   top := tophash(hash)\nsearch:\n    // 查找要删除的可以，因为前面已经执行了迁移操作，因此这里只需要在新的bucket中查找即可\n   for ; b != nil; b = b.overflow(t) {\n      for i := uintptr(0); i < bucketCnt; i++ {\n         if b.tophash[i] != top {\n            continue\n         }\n         k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize))\n         k2 := k\n         if t.indirectkey {\n            k2 = *((*unsafe.Pointer)(k2))\n         }\n         if !alg.equal(key, k2) {\n            continue\n         }\n         // 执行清除操作\n         // Only clear key if there are pointers in it.\n         if t.indirectkey {\n            *(*unsafe.Pointer)(k) = nil\n         } else if t.key.kind&kindNoPointers == 0 {\n            memclrHasPointers(k, t.key.size)\n         }\n         v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))\n         if t.indirectvalue {\n            *(*unsafe.Pointer)(v) = nil\n         } else if t.elem.kind&kindNoPointers == 0 {\n            memclrHasPointers(v, t.elem.size)\n         } else {\n            memclrNoHeapPointers(v, t.elem.size)\n         }\n         b.tophash[i] = empty // 将tophash中的值设为empty，表示空槽\n         h.count-- // 数量减一\n         break search\n      }\n   }\n\n   if h.flags&hashWriting == 0 {\n      throw(\"concurrent map writes\")\n   }\n   h.flags &^= hashWriting\n}\n```\n\n##### for-range\n\n执行`for-range`操作时，需要一个迭代器，我们先来看迭代器的声明：\n\n```go\n// A hash iteration structure.\ntype hiter struct {\n\tkey         unsafe.Pointer // key值，nil表示迭代结束\n\tvalue       unsafe.Pointer // val值\n\tt           *maptype\n\th           *hmap\n\tbuckets     unsafe.Pointer // bucket ptr at hash_iter initialization time\n\tbptr        *bmap          // 当前正在迭代的buckete的指针\n\toverflow    *[]*bmap       // keeps overflow buckets of hmap.buckets alive\n\toldoverflow *[]*bmap       // keeps overflow buckets of hmap.oldbuckets alive\n\tstartBucket uintptr        // 记录这次迭代从哪个bucket开始\n\toffset      uint8          // 记录这次迭代从hmap中的哪个offset开始\n\twrapped     bool           // already wrapped around from end of bucket array to beginning\n\tB           uint8\n\ti           uint8\n\tbucket      uintptr\n\tcheckBucket uintptr\n}\n```\n\n`hiter`的创建：\n\n```go\nfunc mapiterinit(t *maptype, h *hmap, it *hiter) {\n\tif h == nil || h.count == 0 {\n\t\treturn\n\t}\n\n\tit.t = t // 记录map类型信息\n\tit.h = h // 引用map\n\n\t// grab snapshot of bucket state\n\tit.B = h.B // 记录当前的B\n\tit.buckets = h.buckets // 记录当前的buckets\n    // 如果当前map中的键值对没有指针\n\tif t.bucket.kind&kindNoPointers != 0 {\n\t\t// Allocate the current slice and remember pointers to both current and old.\n\t\t// This preserves all relevant overflow buckets alive even if\n\t\t// the table grows and/or overflow buckets are added to the table\n\t\t// while we are iterating.\n\t\th.createOverflow()\n\t\tit.overflow = h.extra.overflow\n\t\tit.oldoverflow = h.extra.oldoverflow\n\t}\n\n\t// decide where to start\n    // 这里随机选举一个开始迭代的位置，因此我们对map进行for-range操作，每次输出的序列都是不同的\n\tr := uintptr(fastrand())\n\tif h.B > 31-bucketCntBits {\n\t\tr += uintptr(fastrand()) << 31\n\t}\n    // 设置开始的bucket和offset\n\tit.startBucket = r & bucketMask(h.B)\n\tit.offset = uint8(r >> h.B & (bucketCnt - 1))\n\n\t// 当前迭代的bucket索引\n\tit.bucket = it.startBucket\n\n\t// 设置迭代标志位\n\tif old := h.flags; old&(iterator|oldIterator) != iterator|oldIterator {\n\t\tatomic.Or8(&h.flags, iterator|oldIterator)\n\t}\n\t// 执行next操作\n\tmapiternext(it)\n}\n```\n\n`mapiternext`方法用于推进迭代器前进到下一个键值对，对应逻辑：\n\n```go\n\nfunc mapiternext(it *hiter) {\n\th := it.h\n\tif h.flags&hashWriting != 0 {\n\t\tthrow(\"concurrent map iteration and map write\")\n\t}\n\tt := it.t\n\tbucket := it.bucket\n\tb := it.bptr // 当前正在遍历的bucket\n\ti := it.i\n\tcheckBucket := it.checkBucket\n\talg := t.key.alg\n\nnext:\n\tif b == nil { // 如果it.bptr还没有初始化，需要根据it.bucket进行初始化，第一次执行或者每次遍历完一个bucket都会清空bptr\n\t\tif bucket == it.startBucket && it.wrapped {\n\t\t\t// end of iteration\n\t\t\tit.key = nil\n\t\t\tit.value = nil\n\t\t\treturn\n\t\t}\n        // 如果正在执行扩容，并且迭代是在扩容之后开始的，这时候如果旧的bucket还没有迁移到新的bucket中，那么需要到旧的bucket中遍历\n\t\tif h.growing() && it.B == h.B {\n            // 获取当前要迭代的bucket对应的oldbucket\n\t\t\toldbucket := bucket & it.h.oldbucketmask()\n\t\t\tb = (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)))\n            // 如果oldbucket中的元素还没有迁移到新的bucket中\n\t\t\tif !evacuated(b) {\n\t\t\t\tcheckBucket = bucket // oldbucket中的键值对迁移到新的bucket中时，可能会迁移到两个bucket中，比如原来长度是4，hash%4等于3，限制长度是8，hash%8可能为4也可能为7，这里的checkBucket记录当前正在遍历h.buckets数组中的哪个bucket，用于后面的判断\n\t\t\t} else {\n                // 已经迁移完成，直接在新的bucket中遍历\n\t\t\t\tb = (*bmap)(add(it.buckets, bucket*uintptr(t.bucketsize)))\n\t\t\t\tcheckBucket = noCheck\n\t\t\t}\n\t\t} else { // 当前没有扩容操作，直接在新的bucket中遍历\n\t\t\tb = (*bmap)(add(it.buckets, bucket*uintptr(t.bucketsize)))\n\t\t\tcheckBucket = noCheck\n\t\t}\n\t\tbucket++ // 下一次遍历的bucket的索引\n        // 因为开始迭代的bucket位置是随机的，如果越界了，从第一个bucket开始\n\t\tif bucket == bucketShift(it.B) {\n\t\t\tbucket = 0\n\t\t\tit.wrapped = true\n\t\t}\n\t\ti = 0\n\t}\n    // 遍历bmap中键值对\n\tfor ; i < bucketCnt; i++ {\n        // 计算开始遍历的偏移位置\n\t\toffi := (i + it.offset) & (bucketCnt - 1)\n        // 如果空槽，则跳过\n\t\tif b.tophash[offi] == empty || b.tophash[offi] == evacuatedEmpty {\n\t\t\tcontinue\n\t\t}\n        // 获取key\n\t\tk := add(unsafe.Pointer(b), dataOffset+uintptr(offi)*uintptr(t.keysize))\n\t\tif t.indirectkey {\n\t\t\tk = *((*unsafe.Pointer)(k))\n\t\t}\n        // 获取val\n\t\tv := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+uintptr(offi)*uintptr(t.valuesize))\n        // 如果正在执行扩容，并且正在遍历的bucket还没有迁移完成，并且扩容是由于键值对达到阈值触发的，这时候扩容后的buckets数组的长度为原来的两倍，原来的一个bucket中的键值对迁移时会迁移到新的两个bucket中，因为目标bucket是通过哈希取模计算的，而这时候bucket数组长度扩大了两倍，比如原理数组长度是4，取模后是3，现在数组长度是8，取模后可能为3也可能为7\n\t\tif checkBucket != noCheck && !h.sameSizeGrow() {\n            // 如果key==key，正常都是走这个逻辑\n\t\t\tif t.reflexivekey || alg.equal(k, k) { \n\t\t\t\t// 如果当前的key不是落在当前的bucket中的，比如现在正在遍历的bucket为3，但是key现在的hash是7，以后迁移时将迁移到索引为7的bucket中，因此这个时候应该跳过这个key\n\t\t\t\thash := alg.hash(k, uintptr(h.hash0))\n\t\t\t\tif hash&bucketMask(it.B) != checkBucket {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else { // key != key的情况，比如math.NaN() != math.NaN()\n\t\t\t\tif checkBucket>>(it.B-1) != uintptr(b.tophash[offi]&1) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} \n\t\t}\n        // 走到这里，可能有一种情况，在迭代开始之后发生了扩容（比如我们在for-range里面插入新的键值对，这时候触发了迁移），这时候迭代的是扩容之前的键值对，即只会迭代当前的oldbuckets里面键值对，这时候如果这些键值对还没有发生迁移或者说key是不可比较的，比如key为math.NAN()，因为key!=key，因此这个key不会被删除或者更新，可以直接返回\n\t\tif (b.tophash[offi] != evacuatedX && b.tophash[offi] != evacuatedY) ||\n\t\t\t!(t.reflexivekey || alg.equal(k, k)) {\n\t\t\tit.key = k\n\t\t\tif t.indirectvalue {\n\t\t\t\tv = *((*unsafe.Pointer)(v))\n\t\t\t}\n\t\t\tit.value = v\n\t\t} else { // 走到这里说明发生了迁移，使用mapaccessK来获取\n\t\t\trk, rv := mapaccessK(t, h, k)\n\t\t\tif rk == nil { // 已经被删除了\n\t\t\t\tcontinue // key has been deleted\n\t\t\t}\n\t\t\tit.key = rk\n\t\t\tit.value = rv\n\t\t}\n        // 更新迭代器状态\n\t\tit.bucket = bucket\n\t\tif it.bptr != b { // avoid unnecessary write barrier; see issue 14921\n\t\t\tit.bptr = b\n\t\t}\n\t\tit.i = i + 1\n\t\tit.checkBucket = checkBucket\n\t\treturn\n\t}\n    // 当前bmap遍历完成，遍历下一个bmap，如果已经是bmap链表的最后一个节点，则返回的b为nil，这时候会触发遍历下一个bucket\n\tb = b.overflow(t)\n\ti = 0\n\tgoto next\n}\n```\n\n\n\n### 扩容\n\n当写入时，如果当前`bucket`已经满了，则会触发扩容检查，如果当前不处于扩容状态并且满足：\n\n- 当前键值对个数已经达到 `0.65*2^B`\n- 当前`overflow bucket`数量达到`1<<(B&15)`，这里的`B`如果大于15，按照15计算\n\n下面看一下开始扩容的逻辑：\n\n```go\nfunc hashGrow(t *maptype, h *hmap) {\n\t// If we've hit the load factor, get bigger.\n\t// Otherwise, there are too many overflow buckets,\n\t// so keep the same number of buckets and \"grow\" laterally.\n\tbigger := uint8(1)\n    // 如果键值对数量没有达到阈值，则说明是overflow bucket数量过多触发的\n\tif !overLoadFactor(h.count+1, h.B) {\n\t\tbigger = 0 // 这里将bigger设置成了0\n\t\t// 设置标志位，用于后续区分是哪种情况触发的扩容\n\t\t// 如果是overflow bucket数量过多触发的，实际上并不会增加bucket的数量\n\t\t// 因此称作sameSizeGrow\n\t\th.flags |= sameSizeGrow\n\t}\n\n\t// 保存原来的buckets到oldbuckets\n\toldbuckets := h.buckets\n\t// 分配新的buckets\n\t// 如果是键值对达到阈值触发的扩容，bucket数量为原来的2倍，也就是扩展buckets数组\n\t// 否则维持原来的数量\n\tnewbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil)\n\t// 设置更新迭代标志位\n\tflags := h.flags &^ (iterator | oldIterator)\n\tif h.flags&iterator != 0 {\n\t\tflags |= oldIterator\n\t}\n\t// commit the grow (atomic wrt gc)\n    // 更新hmap字段\n\th.B += bigger\n\th.flags = flags\n\th.oldbuckets = oldbuckets\n\th.buckets = newbuckets\n\th.nevacuate = 0\n\th.noverflow = 0\n\n\tif h.extra != nil && h.extra.overflow != nil {\n\t\t// Promote current overflow buckets to the old generation.\n\t\tif h.extra.oldoverflow != nil {\n\t\t\tthrow(\"oldoverflow is not nil\")\n\t\t}\n\t\th.extra.oldoverflow = h.extra.overflow\n\t\th.extra.overflow = nil\n\t}\n\tif nextOverflow != nil {\n\t\tif h.extra == nil {\n\t\t\th.extra = new(mapextra)\n\t\t}\n\t\th.extra.nextOverflow = nextOverflow\n\t}\n\n\t// the actual copying of the hash table data is done incrementally\n\t// by growWork() and evacuate().\n}\n```\n\n再来看一下`bucket`的迁移操作：\n\n```go\nfunc growWork(t *maptype, h *hmap, bucket uintptr) {\n\t// 迁移正在使用的bucket对应的oldbucket的数据\n\tevacuate(t, h, bucket&h.oldbucketmask())\n\n\t// 继续对h.nevacuate对应的bucket进行迁移，让迁移尽快完成\n\tif h.growing() {\n\t\tevacuate(t, h, h.nevacuate)\n\t}\n}\n```\n\n从上面可以看到，一次`growwork`最多可以迁移两个`bucket`，这样可以尽早完成扩容之后的迁移\n\n接着看一下`evacuate`的逻辑：\n\n```go\nfunc evacuate(t *maptype, h *hmap, oldbucket uintptr) {\n    // 计算要进行迁移的bucket\n\tb := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)))\n\tnewbit := h.noldbuckets() // 获取oldbuckets数组的长度\n    // 如果还没有执行过迁移\n\tif !evacuated(b) {\n\t\t// 一个oldbucket中的key可能迁移到两个新的bucket中，这里使用x来表示低位目标bucket，y表示高位目标bucket\n\t\tvar xy [2]evacDst\n\t\tx := &xy[0]\n        // 低位目标bucket的索引和当前要迁移的oldbucket的索引一致\n\t\tx.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize)))\n\t\tx.k = add(unsafe.Pointer(x.b), dataOffset)\n\t\tx.v = add(x.k, bucketCnt*uintptr(t.keysize))\n\t\t// 如果是sameSizeGrow，buckets长度没有变化，则不会有高位目标bucket\n\t\tif !h.sameSizeGrow() {\n\t\t\t// Only calculate y pointers if we're growing bigger.\n\t\t\t// Otherwise GC can see bad pointers.\n\t\t\ty := &xy[1]\n            // 计算高位目标bucket\n\t\t\ty.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize)))\n\t\t\ty.k = add(unsafe.Pointer(y.b), dataOffset)\n\t\t\ty.v = add(y.k, bucketCnt*uintptr(t.keysize))\n\t\t}\n\t\t// 遍历bucket对应的bmap链表\n\t\tfor ; b != nil; b = b.overflow(t) {\n\t\t\tk := add(unsafe.Pointer(b), dataOffset)\n\t\t\tv := add(k, bucketCnt*uintptr(t.keysize))\n\t\t\tfor i := 0; i < bucketCnt; i, k, v = i+1, add(k, uintptr(t.keysize)), add(v, uintptr(t.valuesize)) {\n\t\t\t\ttop := b.tophash[i]\n\t\t\t\tif top == empty { // 空槽，没有键值对\n\t\t\t\t\tb.tophash[i] = evacuatedEmpty\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif top < minTopHash {\n\t\t\t\t\tthrow(\"bad map state\")\n\t\t\t\t}\n\t\t\t\tk2 := k\n\t\t\t\tif t.indirectkey {\n\t\t\t\t\tk2 = *((*unsafe.Pointer)(k2))\n\t\t\t\t}\n\t\t\t\tvar useY uint8\n                // 不是sameSizeGrow，说明可能会迁移到高位目标bucket\n\t\t\t\tif !h.sameSizeGrow() {\n\t\t\t\t\thash := t.key.alg.hash(k2, uintptr(h.hash0))\n                    // 存在迭代器，并且key!=key(NaNs)，一般不会走这个分支\n\t\t\t\t\tif h.flags&iterator != 0 && !t.reflexivekey && !t.key.alg.equal(k2, k2) {\n\t\t\t\t\t\tuseY = top & 1\n\t\t\t\t\t\ttop = tophash(hash)\n\t\t\t\t\t} else {\n                        // 比如hash是7，oldbuckets长度是4，现在是8，原来bucket是3，现在应该迁移到7这个bucket，这种情况hash&newbit=newbit    \n\t\t\t\t\t\tif hash&newbit != 0 {\n\t\t\t\t\t\t\tuseY = 1\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n                // 这里是对全局常量进行检查：evacuatedY=evacuatedX+1\n                // evacuatedX标记迁移到低位目标bucket\n                // evacuatedY标记迁移到高位目标bucket\n\t\t\t\tif evacuatedX+1 != evacuatedY {\n\t\t\t\t\tthrow(\"bad evacuatedN\")\n\t\t\t\t}\n\t\t\t\t// 对当前槽进行标记，表示已经完成迁移\n\t\t\t\tb.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY\n                // 根据是否迁移到高位bucket选择目标bucket\n\t\t\t\tdst := &xy[useY]                 // evacuation destination\n\t\t\t\t// 如果当前bmap满了，创建新的bmap\n\t\t\t\tif dst.i == bucketCnt {\n\t\t\t\t\tdst.b = h.newoverflow(t, dst.b)\n\t\t\t\t\tdst.i = 0\n\t\t\t\t\tdst.k = add(unsafe.Pointer(dst.b), dataOffset)\n\t\t\t\t\tdst.v = add(dst.k, bucketCnt*uintptr(t.keysize))\n\t\t\t\t}\n                // 执行键值对迁移\n\t\t\t\tdst.b.tophash[dst.i&(bucketCnt-1)] = top // mask dst.i as an optimization, to avoid a bounds check\n\t\t\t\tif t.indirectkey {\n\t\t\t\t\t*(*unsafe.Pointer)(dst.k) = k2 // copy pointer\n\t\t\t\t} else {\n\t\t\t\t\ttypedmemmove(t.key, dst.k, k) // copy value\n\t\t\t\t}\n\t\t\t\tif t.indirectvalue {\n\t\t\t\t\t*(*unsafe.Pointer)(dst.v) = *(*unsafe.Pointer)(v)\n\t\t\t\t} else {\n\t\t\t\t\ttypedmemmove(t.elem, dst.v, v)\n\t\t\t\t}\n              \t// 更新迁移目标状态\n\t\t\t\tdst.i++\n\t\t\t\t// These updates might push these pointers past the end of the\n\t\t\t\t// key or value arrays.  That's ok, as we have the overflow pointer\n\t\t\t\t// at the end of the bucket to protect against pointing past the\n\t\t\t\t// end of the bucket.\n\t\t\t\tdst.k = add(dst.k, uintptr(t.keysize))\n\t\t\t\tdst.v = add(dst.v, uintptr(t.valuesize))\n\t\t\t}\n\t\t}\n\t\t// Unlink the overflow buckets & clear key/value to help GC.\n        // 如果不存在对oldbuckets的迭代器并且键值对中包含指针，在oldbuckets清除当前迁移的bucket，帮助尽快gc掉这些没有用的内存\n\t\tif h.flags&oldIterator == 0 && t.bucket.kind&kindNoPointers == 0 {\n\t\t\tb := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))\n\t\t\t// Preserve b.tophash because the evacuation\n\t\t\t// state is maintained there.\n\t\t\tptr := add(b, dataOffset)\n\t\t\tn := uintptr(t.bucketsize) - dataOffset\n\t\t\tmemclrHasPointers(ptr, n)\n\t\t}\n\t}\n\t// 如果当前迁移的bucket等于h.nevacuate则更新h.nevacuate的值\n    // 如果所有bucket都已经迁移完成，则消除扩容状态\n\tif oldbucket == h.nevacuate {\n\t\tadvanceEvacuationMark(h, t, newbit)\n\t}\n}\n```\n\n\n\n\n\n\n\n","tags":["go"]},{"title":"go中的猴子补丁","url":"/2019/03/06/go中的猴子补丁/","content":"\n### 函数值\n\n首先查看下面代码：\n\n```go \nfunc a()int {return 1}\n\nfunc main() {\n\tfmt.Printf(\"%p\\n\", a) // 0x48f950\n\tfn := a\n\tfmt.Printf(\"0x%x\\n\",*(*uintptr)(unsafe.Pointer(&fn))) // 0x4c8680\n\tfmt.Printf(\"0x%x\\n\", **(**uintptr)(unsafe.Pointer(&fn))) // 0x48f950\n}\n```\n\n根据上面的输出我们可以发现，函数值`fn`并没有直接持有函数`a`的地址，这是因为**`Go`的函数值可以包含一些额外的上下文信息**，这是实现闭包和绑定实例方法的关键，我们可以在[源码](https://github.com/golang/go/blob/e9d9d0befc634f6e9f906b5ef7476fbd7ebd25e3/src/runtime/runtime2.go#L75-L78)中找到点函数值类型的线索：\n\n```go\ntype funcval struct {\n\tfn uintptr\n\t// variable-size, fn-specific data here\n}\n```\n\n我们代码中的函数变量，实际上应该`*funcval`类型\n\n##### 闭包实现原理\n\n接下来，我们探究一下`golang`中闭包的实现原理，我们首先写一个简单的闭包demo，然后从编译后的汇编代码来探究其实现\n\n```go\nfunc main() {\n\tf := fn() // 这里的f是一个函数值\n\tf()\n}\n\nfunc fn() func() {\n\tvar a = 10\n\treturn func() {\n\t\tfmt.Println(a) // 捕获局部变量a\n\t}\n}\n```\n\n接下来将上面代码编译成汇编：\n\n```sh\n$ go tool compile -S -N main.go > asm.s\n```\n\n下面是生成的汇编，只保留主要的内容：\n\n```assembly\n\"\".main STEXT size=72 args=0x0 locals=0x18\n\t0x0000 00000 (demo.go:7)\tTEXT\t\"\".main(SB), $24-0\n\t0x0024 00036 (demo.go:8)\tCALL\t\"\".fn(SB) # 调用fn函数获取\n\t0x0029 00041 (demo.go:8)\tMOVQ\t(SP), DX # 保存返回的函数值指针到DX，这里的DX是关键\n\t0x002d 00045 (demo.go:8)\tMOVQ\tDX, \"\".f+8(SP) # 把DX的值赋给局部变量f \n\t0x0032 00050 (demo.go:9)\tMOVQ\t(DX), AX # 从上面funcval结构可知，(DX)为实际函数地址，也就是下面的\"\".fn.func1\n\t0x0035 00053 (demo.go:9)\tCALL\tAX # 调用实际的函数\n\t0x0040 00064 (demo.go:10)\tRET\n\n\"\".fn STEXT size=136 args=0x8 locals=0x28\n\t0x0000 00000 (demo.go:12)\tTEXT\t\"\".fn(SB), $40-8\n\t0x0036 00054 (demo.go:14)\tLEAQ\ttype.noalg.struct { F uintptr; \"\".a int }(SB), AX # 这里表示实际funcval的类型\n\t0x003d 00061 (demo.go:14)\tMOVQ\tAX, (SP)\n\t0x0041 00065 (demo.go:14)\tCALL\truntime.newobject(SB) # new一个funcval\n\t0x0046 00070 (demo.go:14)\tMOVQ\t8(SP), AX # 返回值\n\t0x0050 00080 (demo.go:14)\tLEAQ\t\"\".fn.func1(SB), CX # 取实际函数地址\n\t0x0057 00087 (demo.go:14)\tMOVQ\tCX, (AX) # 保存实际地址\n\t0x0061 00097 (demo.go:14)\tMOVQ\t\"\".a+16(SP), CX # 保存变量a到funcval\n\t0x0066 00102 (demo.go:14)\tMOVQ\tCX, 8(AX) \n\t0x006f 00111 (demo.go:14)\tMOVQ\tAX, \"\".~r0+48(SP) # 设置返回值\n\t0x007d 00125 (demo.go:14)\tRET\n\n\"\".fn.func1 STEXT size=258 args=0x0 locals=0x88\n\t0x0000 00000 (demo.go:14)\tTEXT\t\"\".fn.func1(SB), NEEDCTXT, $136-0\n\t0x0036 00054 (demo.go:14)\tMOVQ\t8(DX), AX\t# [DX+8]实际上存储的就是闭包引用外部的变量a\n\t0x003a 00058 (demo.go:14)\tMOVQ\tAX, \"\".a+48(SP) # 将AX赋值给变量a\n\t0x003f 00063 (demo.go:15)\tMOVQ\tAX, \"\"..autotmp_2+56(SP)\n\t0x0056 00086 (demo.go:15)\tLEAQ\ttype.int(SB), AX # fmt.Println函数实际接收的是[]interface{}，这里需要先将a转换成interface{}类型\n\t0x005d 00093 (demo.go:15)\tMOVQ\tAX, (SP)\n\t0x0061 00097 (demo.go:15)\tMOVQ\t\"\"..autotmp_2+56(SP), AX\n\t0x0066 00102 (demo.go:15)\tMOVQ\tAX, 8(SP)\n\t0x006b 00107 (demo.go:15)\tCALL\truntime.convT2E64(SB)\n```\n\n从上面我们可以看到，`go`的闭包是通过`funcval`携带额外的上下文信息来实现的。\n\n当创建闭包函数时，将被闭包捕获的变量的地址保存到`funcval`，当调用闭包函数时，会将`funcval`的地址保存到`DX`寄存器，执行闭包函数时，可以通过`DX`寄存器来访问这些变量。\n\n\n\n### 实现猴子补丁\n\n现在，我们要在`go`中实现猴子补丁，所想要实现的效果是：\n\n```go\nfunc a() {\n\tfmt.Println(\"run a\")\n}\nfunc b() {\n\tfmt.Println(\"run b\")\n}\n\nfunc main() {\n\ta() // run a\n\treplace(a, b)\n\ta() // run b\n}\n```\n\n我们要在`replace`方法中，将对函数`a`的调用替换成对函数`b`的调用。\n\n具体实现：\n\n```go\nfunc replace(a, b func()) {\n\treplaceFunction(**(**uintptr)(unsafe.Pointer(&a)), *(*uintptr)(unsafe.Pointer(&b)))\n}\n\n// from is a pointer to the actual function\n// to is a pointer to a go funcvalue\nfunc replaceFunction(from, to uintptr) {\n    // demo只支持64bit\n\tif unsafe.Sizeof(uintptr(1)) != 8 {\n\t\tpanic(\"only support amd64\")\n\t}\n    // jmpToFunctionValue生成跳转到to代表的函数的机器码\n\tjumpData := jmpToFunctionValue(to)\n    // 使用生成的机器码替换from函数\n\tcopyToLocation(from, jumpData)\n\treturn\n}\n\n// movabs rdx,to # to是一个*funcval，需要将其存储到DX寄存器，rdx是64bit的DX寄存器\n// jmp QWORD PTR [rdx] # 跳转到to对应的实际函数的开始处执行\nfunc jmpToFunctionValue(to uintptr) []byte {\n\treturn []byte{\n\t\t0x48, 0xBA,\n\t\tbyte(to),\n\t\tbyte(to >> 8),\n\t\tbyte(to >> 16),\n\t\tbyte(to >> 24),\n\t\tbyte(to >> 32),\n\t\tbyte(to >> 40),\n\t\tbyte(to >> 48),\n\t\tbyte(to >> 56), // movabs rdx,to\n\t\t0xFF, 0x22,     // jmp QWORD PTR [rdx]\n\t}\n}\n\n// 内存替换，因为code所在的代码段默认是只读的，因此需要使用系统调用mprotect将其更改为可写的\nfunc copyToLocation(location uintptr, data []byte) {\n\tf := rawMemoryAccess(location, len(data))\n\tmprotectCrossPage(location, len(data), syscall.PROT_READ|syscall.PROT_WRITE|syscall.PROT_EXEC)\n\tcopy(f, data[:])\n\tmprotectCrossPage(location, len(data), syscall.PROT_READ|syscall.PROT_EXEC)\n}\n\n// 将指定内存地址转换成一个slice\nfunc rawMemoryAccess(p uintptr, length int) []byte {\n\treturn *(*[]byte)(unsafe.Pointer(&reflect.SliceHeader{\n\t\tData: p,\n\t\tLen:  length,\n\t\tCap:  length,\n\t}))\n}\n\n// 使用系统调用mprotect修改指定内存的访问权限\nfunc mprotectCrossPage(addr uintptr, length int, prot int) {\n\tpageSize := syscall.Getpagesize()\n\tfor p := pageStart(addr); p < addr+uintptr(length); p += uintptr(pageSize) {\n\t\tpage := rawMemoryAccess(p, pageSize)\n\t\terr := syscall.Mprotect(page, prot)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\n// 内存页对齐\nfunc pageStart(ptr uintptr) uintptr {\n\treturn ptr & ^(uintptr(syscall.Getpagesize() - 1))\n}\n```\n\n当执行`replace(a,b)`时，会动态将函数`a`的指令替换成`movabs rdx,to; jmp QWORD PTR [rdx]`\n\n之后调用函数`a`时，会执行`call`指令，这时候会把传递给函数`a`的参数保存到栈上，并且将返回地址保存到指定的寄存器`RA`中；\n\n因为函数`a`被替换成上诉两条指令，因此会跳转到函数`b`执行，这时候函数`b`可以直接使用栈上的参数（这就要求两个函数要有相同的函数签名）；因为`morestack`操作是在函数开始执行的时候进行检查的，因此不会有栈溢出的问题。\n\n当函数`b`执行完成时，会执行`ret`指令，这时候会把返回值保存到栈上，同时将`RA`中的返回地址弹出到`PC`寄存器中；\n\n对于函数调用者来说，整个过程是透明的。\n\n\n\n### refer\n\n[monkey patching in Go](https://bou.ke/blog/monkey-patching-in-go/)\n\n[bouk/monkey](https://github.com/bouk/monkey)","tags":["go"]},{"title":"net/rpc分析","url":"/2019/02/28/net-rpc分析/","content":"\n### rpc\n\n`golang`本身提供了`net/rpc`标准库，用于提供`rpc`服务。\n\n`rpc`通过将网络传输和数据序列化/反序列化屏蔽在接口背后，提供一种简洁的调用接口，已达到调用远程服务方法在执行本地方法一样。\n\n### server \n\n##### service\n\n`service`代表每个注册的服务\n\n```go\ntype methodType struct {\n\tsync.Mutex \t\t\t\t  // protects counters\n\tmethod     reflect.Method // 方法信息\n\tArgType    reflect.Type   // 第一个参数类型\n\tReplyType  reflect.Type   // 第二个参数类型，该参数用来返回结果\n\tnumCalls   uint           // 统计调用次数\n}\n\ntype service struct {\n\tname   string                 // 服务名\n\trcvr   reflect.Value          // 服务对象的值\n\ttyp    reflect.Type           // 服务对象的类型\n\tmethod map[string]*methodType // 该服务对外提供的方法\n}\n```\n\n##### server\n\n`server`代表一个`rpc server`\n\n```go\ntype Server struct {\n\tserviceMap sync.Map     // 注册的服务：map[string]*service\n\treqLock    sync.Mutex   // protects freeReq\n\tfreeReq    *Request     // 缓存Request列表，避免每次请求都要重新创建一个\n\trespLock   sync.Mutex   // protects freeResp\n\tfreeResp   *Response\n}\n```\n\n##### Request & Response\n\n```go \n// Request is a header written before every RPC call. It is used internally\n// but documented here as an aid to debugging, such as when analyzing\n// network traffic.\ntype Request struct {\n\tServiceMethod string   // format: \"Service.Method\"\n\tSeq           uint64   // 请求的Seq，客户端会对请求进行编号，用于区分不同的请求\n\tnext          *Request // for free list in Server\n}\n\n// Response is a header written before every RPC return. It is used internally\n// but documented here as an aid to debugging, such as when analyzing\n// network traffic.\ntype Response struct {\n\tServiceMethod string    // echoes that of the Request\n\tSeq           uint64    // echoes that of the request\n\tError         string    // error, if any.\n\tnext          *Response // for free list in Server\n}\n```\n\n##### Register\n\n```go\nfunc (server *Server) Register(rcvr interface{}) error {\n\t// 使用反射名作为服务名称\n    return server.register(rcvr, \"\", false)\n}\n\nfunc (server *Server) RegisterName(name string, rcvr interface{}) error {\n    // 自定义服务名称\n   return server.register(rcvr, name, true)\n}\n\nfunc (server *Server) register(rcvr interface{}, name string, useName bool) error {\n   s := new(service) \n   s.typ = reflect.TypeOf(rcvr)   // 设置类型\n   s.rcvr = reflect.ValueOf(rcvr) // 设置值\n   // 默认取类型名\n   sname := reflect.Indirect(s.rcvr).Type().Name()\n   if useName { // 如果需要使用自定义名称\n      sname = name\n   }\n   if sname == \"\" {\n      s := \"rpc.Register: no service name for type \" + s.typ.String()\n      log.Print(s)\n      return errors.New(s)\n   }\n    // 如果该service不是导出类型，保错\n   if !isExported(sname) && !useName {\n      s := \"rpc.Register: type \" + sname + \" is not exported\"\n      log.Print(s)\n      return errors.New(s)\n   }\n   s.name = sname\n\n   // 存找该services用于提供对外服务的方法\n   s.method = suitableMethods(s.typ, true)\n\t// 方法数必须大于0\n   if len(s.method) == 0 {\n      str := \"\"\n\n      // To help the user, see if a pointer receiver would work.\n      method := suitableMethods(reflect.PtrTo(s.typ), false)\n      if len(method) != 0 {\n         str = \"rpc.Register: type \" + sname + \" has no exported methods of suitable type (hint: pass a pointer to value of that type)\"\n      } else {\n         str = \"rpc.Register: type \" + sname + \" has no exported methods of suitable type\"\n      }\n      log.Print(str)\n      return errors.New(str)\n   }\n\t// 不允许同一个服务名称重复注册\n   if _, dup := server.serviceMap.LoadOrStore(sname, s); dup {\n      return errors.New(\"rpc: service already defined: \" + sname)\n   }\n   return nil\n}\n```\n\n`suitableMethods`用来查找`service`中需要暴露的方法，实现就是遍历该`service`的所有方法，并返回其中符合条件的方法\n\n```go \nfunc suitableMethods(typ reflect.Type, reportErr bool) map[string]*methodType {\n\tmethods := make(map[string]*methodType)\n    // 遍历方法\n\tfor m := 0; m < typ.NumMethod(); m++ {\n\t\tmethod := typ.Method(m)\n\t\tmtype := method.Type\n\t\tmname := method.Name\n\t\t// Method must be exported.\n        // 如果method是导出的（方法名首字母大写），PkgPath为空\n\t\tif method.PkgPath != \"\" { \n\t\t\tcontinue\n\t\t}\n\t\t// Method needs three ins: receiver, *args, *reply.\n        // 参数个数必须为3，其中第一个参数为service对象\n\t\tif mtype.NumIn() != 3 {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: method %q has %d input parameters; needs exactly three\\n\", mname, mtype.NumIn())\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// First arg need not be a pointer.\n\t\t// 第二个参数必须是内置类型或者自定义的导出类型，不需要是指针类型\n        argType := mtype.In(1)\n\t\tif !isExportedOrBuiltinType(argType) {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: argument type of method %q is not exported: %q\\n\", mname, argType)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// Second arg must be a pointer.\n\t\treplyType := mtype.In(2)\n        // 第三个参数必须是指针类型，该参数用来向客户端返回请求结果\n\t\tif replyType.Kind() != reflect.Ptr {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: reply type of method %q is not a pointer: %q\\n\", mname, replyType)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// Reply type must be exported.\n        // 该参数也必须是内置类型或者导出类型\n\t\tif !isExportedOrBuiltinType(replyType) {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: reply type of method %q is not exported: %q\\n\", mname, replyType)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// Method needs one out.\n        // 方法必须有且只有一个error类型的返回值\n\t\tif mtype.NumOut() != 1 {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: method %q has %d output parameters; needs exactly one\\n\", mname, mtype.NumOut())\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// The return type of the method must be error.\n\t\tif returnType := mtype.Out(0); returnType != typeOfError {\n\t\t\tif reportErr {\n\t\t\t\tlog.Printf(\"rpc.Register: return type of method %q is %q, must be error\\n\", mname, returnType)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n        // 符合条件，添加\n\t\tmethods[mname] = &methodType{method: method, ArgType: argType, ReplyType: replyType}\n\t}\n\treturn methods\n}\n```\n\n##### 启动服务\n\n```go \nfunc (server *Server) Accept(lis net.Listener) {\n   for {\n      conn, err := lis.Accept()\n      if err != nil {\n         log.Print(\"rpc.Serve: accept:\", err.Error())\n         return\n      }\n      // 每个客户启用一个goroutine进行处理\n      go server.ServeConn(conn)\n   }\n}\n```\n\n##### 处理请求\n\n```go\nfunc (server *Server) ServeConn(conn io.ReadWriteCloser) {\n\tbuf := bufio.NewWriter(conn)\n    // 默认使用gob编解码\n\tsrv := &gobServerCodec{\n\t\trwc:    conn,\n\t\tdec:    gob.NewDecoder(conn),\n\t\tenc:    gob.NewEncoder(buf),\n\t\tencBuf: buf,\n\t}\n\tserver.ServeCodec(srv)\n}\n\nfunc (server *Server) ServeCodec(codec ServerCodec) {\n\tsending := new(sync.Mutex) // 写response内容时需要加锁\n\twg := new(sync.WaitGroup)\n\tfor {\n        // 从连接中读取请求，主要是通过gobEncoder实现\n\t\tservice, mtype, req, argv, replyv, keepReading, err := server.readRequest(codec)\n\t\tif err != nil {\n\t\t\tif debugLog && err != io.EOF {\n\t\t\t\tlog.Println(\"rpc:\", err)\n\t\t\t}\n\t\t\tif !keepReading {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// send a response if we actually managed to read a header.\n\t\t\tif req != nil {\n\t\t\t\tserver.sendResponse(sending, req, invalidRequest, codec, err.Error())\n\t\t\t\tserver.freeRequest(req)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\twg.Add(1)\n        // 每个rpc都使用一个goroutine进行处理\n\t\tgo service.call(server, sending, wg, mtype, req, argv, replyv, codec)\n\t}\n\t// 优雅关闭\n\twg.Wait()\n\tcodec.Close()\n}\n```\n\n```go\nfunc (s *service) call(server *Server, sending *sync.Mutex, wg *sync.WaitGroup, mtype *methodType, req *Request, argv, replyv reflect.Value, codec ServerCodec) {\n\tif wg != nil {\n\t\tdefer wg.Done()\n\t}\n\tmtype.Lock()\n\tmtype.numCalls++ // 统计调用次数\n\tmtype.Unlock()\n\tfunction := mtype.method.Func\n\t// 调用具体的请求方法\n\treturnValues := function.Call([]reflect.Value{s.rcvr, argv, replyv})\n\t// The return value for the method is an error.\n\terrInter := returnValues[0].Interface()\n\terrmsg := \"\"\n    // 如果请求方法返回error\n\tif errInter != nil {\n\t\terrmsg = errInter.(error).Error()\n\t}\n    // 写入响应结果，这里主要通过gob.Decoder实现\n\tserver.sendResponse(sending, req, replyv.Interface(), codec, errmsg)\n\t// 释放req\n    server.freeRequest(req)\n}\n```\n\n### client\n\n##### client\n\n```go\ntype Client struct {\n\tcodec ClientCodec // codec\n\n\treqMutex sync.Mutex // protects following\n\trequest  Request\n\n\tmutex    sync.Mutex // protects following\n\tseq      uint64 // 下一次请求的seq\n\tpending  map[uint64]*Call // 正在执行的请求\n\tclosing  bool // user has called Close\n\tshutdown bool // server has told us to stop\n}\n```\n\n##### Call\n\n```go\ntype Call struct {\n   ServiceMethod string      // 调用的远程方法\n   Args          interface{} // 方法第一个参数\n   Reply         interface{} // 第二个参数，用于接收返回值\n   Error         error       // 保存请求的错误信息\n   Done          chan *Call  // 用于通知请求结束\n}\n```\n\n\n\n##### NewClient\n\n```go\nfunc NewClient(conn io.ReadWriteCloser) *Client {\n\tencBuf := bufio.NewWriter(conn)\n\tclient := &gobClientCodec{conn, gob.NewDecoder(conn), gob.NewEncoder(encBuf), encBuf}\n\treturn NewClientWithCodec(client)\n}\n\nfunc NewClientWithCodec(codec ClientCodec) *Client {\n\tclient := &Client{\n\t\tcodec:   codec,\n\t\tpending: make(map[uint64]*Call),\n\t}\n\tgo client.input() // input用来处理server的响应\n\treturn client\n}\n\n```\n\n\n\n##### Call\n\n使用方法`Call`和方法`Go`调用远程方法，其中`Call`会同步等待请求结束，`Go`是异步执行\n\n```go \nfunc (client *Client) Call(serviceMethod string, args interface{}, reply interface{}) error {\n    // Call方法内部也是调用Go，然后等待调用完成后返回\n\tcall := <-client.Go(serviceMethod, args, reply, make(chan *Call, 1)).Done\n\treturn call.Error\n}\n\n// Go方法返回一个channel用来通知调用结束\nfunc (client *Client) Go(serviceMethod string, args interface{}, reply interface{}, done chan *Call) *Call {\n\tcall := new(Call)\n\tcall.ServiceMethod = serviceMethod\n\tcall.Args = args\n\tcall.Reply = reply\n    \n\tif done == nil {\n\t\tdone = make(chan *Call, 10) // buffered.\n\t} else {\n\t\t// If caller passes done != nil, it must arrange that\n\t\t// done has enough buffer for the number of simultaneous\n\t\t// RPCs that will be using that channel. If the channel\n\t\t// is totally unbuffered, it's best not to run at all.\n        if cap(done) == 0 {\n\t\t\tlog.Panic(\"rpc: done channel is unbuffered\")\n\t\t}\n\t}\n\tcall.Done = done\n\tclient.send(call)\n\treturn call\n}\n\n// send执行实际的请求发送\nfunc (client *Client) send(call *Call) {\n\tclient.reqMutex.Lock()\n\tdefer client.reqMutex.Unlock()\n\n\t// Register this call.\n\tclient.mutex.Lock()\n\tif client.shutdown || client.closing {\n\t\tclient.mutex.Unlock()\n\t\tcall.Error = ErrShutdown\n\t\tcall.done()\n\t\treturn\n\t}\n\tseq := client.seq // 获取此次请求seq\n\tclient.seq++ // 计算下一次请求seq\n\tclient.pending[seq] = call // 加入pending列表中\n\tclient.mutex.Unlock() \n\n\t// Encode and send the request.\n\tclient.request.Seq = seq\n\tclient.request.ServiceMethod = call.ServiceMethod\n    // 发送请求\n\terr := client.codec.WriteRequest(&client.request, call.Args)\n\tif err != nil {\n\t\tclient.mutex.Lock()\n\t\tcall = client.pending[seq]\n\t\tdelete(client.pending, seq)\n\t\tclient.mutex.Unlock()\n\t\tif call != nil {\n\t\t\tcall.Error = err\n\t\t\tcall.done()\n\t\t}\n\t}\n}\n```\n\n分析上面的`send`方法，当请求发送出去之后就返回了，那么如何处理请求的响应呢？我们可以看到每次新的请求都会加入到`client.pending`中，那么对应的应该有一个幕后的协程来处理，当收到`server`的响应时，根据`seq`获取对应的`call`，然后通过`call.Done`通知请求完成，\n\n对应的方法为`input`：\n\n```go\nfunc (client *Client) input() {\n\tvar err error\n\tvar response Response\n\tfor err == nil {\n\t\tresponse = Response{}\n        // 读取server的返回结果\n\t\terr = client.codec.ReadResponseHeader(&response)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n        // 获取这次响应对应的请求的seq\n\t\tseq := response.Seq\n\t\tclient.mutex.Lock()\n        // 获取对应的请求\n\t\tcall := client.pending[seq]\n\t\tdelete(client.pending, seq)\n\t\tclient.mutex.Unlock()\n\t\t// 处理响应结果\n\t\tswitch {\n\t\tcase call == nil:\n\t\t\t// We've got no pending call. That usually means that\n\t\t\t// WriteRequest partially failed, and call was already\n\t\t\t// removed; response is a server telling us about an\n\t\t\t// error reading request body. We should still attempt\n\t\t\t// to read error body, but there's no one to give it to.\n\t\t\terr = client.codec.ReadResponseBody(nil)\n\t\t\tif err != nil {\n\t\t\t\terr = errors.New(\"reading error body: \" + err.Error())\n\t\t\t}\n\t\tcase response.Error != \"\":\n\t\t\t// We've got an error response. Give this to the request;\n\t\t\t// any subsequent requests will get the ReadResponseBody\n\t\t\t// error if there is one.\n\t\t\tcall.Error = ServerError(response.Error)\n\t\t\terr = client.codec.ReadResponseBody(nil)\n\t\t\tif err != nil {\n\t\t\t\terr = errors.New(\"reading error body: \" + err.Error())\n\t\t\t}\n\t\t\tcall.done()\n\t\tdefault:\n\t\t\terr = client.codec.ReadResponseBody(call.Reply)\n\t\t\tif err != nil {\n\t\t\t\tcall.Error = errors.New(\"reading body \" + err.Error())\n\t\t\t}\n\t\t\tcall.done()\n\t\t}\n\t}\n    // 发生错误退出之后，停止所有等待的请求\n\t// Terminate pending calls.\n\tclient.reqMutex.Lock()\n\tclient.mutex.Lock()\n\tclient.shutdown = true\n\tclosing := client.closing\n\tif err == io.EOF {\n\t\tif closing {\n\t\t\terr = ErrShutdown\n\t\t} else {\n\t\t\terr = io.ErrUnexpectedEOF\n\t\t}\n\t}\n    // 停止所有等待的请求\n\tfor _, call := range client.pending {\n\t\tcall.Error = err\n\t\tcall.done()\n\t}\n\tclient.mutex.Unlock()\n\tclient.reqMutex.Unlock()\n\tif debugLog && err != io.EOF && !closing {\n\t\tlog.Println(\"rpc: client protocol error:\", err)\n\t}\n}\n\n// 通知调用结束\nfunc (call *Call) done() {\n\tselect {\n\tcase call.Done <- call:\n\t\t// ok\n\tdefault:\n\t\t// We don't want to block here. It is the caller's responsibility to make\n\t\t// sure the channel has enough buffer space. See comment in Go().\n\t\tif debugLog {\n\t\t\tlog.Println(\"rpc: discarding Call reply due to insufficient Done chan capacity\")\n\t\t}\n\t}\n}\n```\n\n","tags":["go"]},{"title":"slice扩容","url":"/2019/02/26/slice扩容/","content":"\n### slice header \n\ngo中的`slice`声明如下：\n\n```go\ntype slice struct {\n\tarray unsafe.Pointer // 指向底层数组\n\tlen   int // 长度，当前存储的元素个数\n\tcap   int // 容量，底层数组的长度\n}\n```\n\n### grow\n\n`slice`中的`len`表示当前切片中存在的元素个数，而`cap`表示切边底层数组总共可以存放的元素个数，\n\n当我们使用`append`函数为切片追加元素时，如果底层数组剩余容量`cap-len`不足以容纳新的元素，则会发生扩容，具体的扩容逻辑如下：\n\n```go \n// @params et: slice元素类型\n// @params old: 老的slice\n// @params cap: 期待的最小cap值，这里的cap等于(old.len + append的元素个数)\n// @return: 新的slice，并且拷贝老的数据到新的slice\nfunc growslice(et *_type, old slice, cap int) slice {\n    // 如果元素不需要存储空间，比如类型struct{}\n\tif et.size == 0 {\n\t\tif cap < old.cap {\n\t\t\tpanic(errorString(\"growslice: cap out of range\"))\n\t\t}\n        // 直接创建一个新的slice，不需要内存分配\n        // 这里zerobase是一个值为0的uintptr\n\t\treturn slice{unsafe.Pointer(&zerobase), old.len, cap}\n\t}\n\t\n\tnewcap := old.cap\n\tdoublecap := newcap + newcap\n\t// 如果x2不能满足，则使用期待值\n    if cap > doublecap {\n\t\tnewcap = cap\n\t} else {\n        // 否则，如果元素个数小于1024，直接x2\n\t\tif old.len < 1024 {\n\t\t\tnewcap = doublecap\n\t\t} else {\n            // 持续1.25倍直到满足\n\t\t\tfor 0 < newcap && newcap < cap {\n\t\t\t\tnewcap += newcap / 4\n\t\t\t}\n\t\t\t// 如果溢出了，直接使用期待值\n\t\t\tif newcap <= 0 {\n\t\t\t\tnewcap = cap\n\t\t\t}\n\t\t}\n\t}\n\n\tvar overflow bool\n    // 原来元素占用内存大小，现在元素占用内存大小，新的底层数组容量大小\n\tvar lenmem, newlenmem, capmem uintptr\n\t// 计算上面声明的变量值，这里根据et.size进行优化\n\tswitch {\n\tcase et.size == 1: // 不需要乘除法\n\t\tlenmem = uintptr(old.len)\n\t\tnewlenmem = uintptr(cap)\n\t\tcapmem = roundupsize(uintptr(newcap)) \n\t\toverflow = uintptr(newcap) > maxAlloc\n\t\tnewcap = int(capmem)\n\tcase et.size == sys.PtrSize: // 会别优化成移位运算\n\t\tlenmem = uintptr(old.len) * sys.PtrSize\n\t\tnewlenmem = uintptr(cap) * sys.PtrSize\n\t\tcapmem = roundupsize(uintptr(newcap) * sys.PtrSize)\n\t\toverflow = uintptr(newcap) > maxAlloc/sys.PtrSize\n\t\tnewcap = int(capmem / sys.PtrSize)\n\tcase isPowerOfTwo(et.size): // 位运算\n\t\tvar shift uintptr\n\t\tif sys.PtrSize == 8 {\n\t\t\t// Mask shift for better code generation.\n\t\t\tshift = uintptr(sys.Ctz64(uint64(et.size))) & 63\n\t\t} else {\n\t\t\tshift = uintptr(sys.Ctz32(uint32(et.size))) & 31\n\t\t}\n\t\tlenmem = uintptr(old.len) << shift\n\t\tnewlenmem = uintptr(cap) << shift\n\t\tcapmem = roundupsize(uintptr(newcap) << shift)\n\t\toverflow = uintptr(newcap) > (maxAlloc >> shift)\n\t\tnewcap = int(capmem >> shift)\n\tdefault:\n\t\tlenmem = uintptr(old.len) * et.size\n\t\tnewlenmem = uintptr(cap) * et.size\n\t\tcapmem = roundupsize(uintptr(newcap) * et.size)\n\t\toverflow = uintptr(newcap) > maxSliceCap(et.size)\n\t\tnewcap = int(capmem / et.size)\n\t}\n\n\t// The check of overflow (uintptr(newcap) > maxSliceCap(et.size))\n\t// in addition to capmem > _MaxMem is needed to prevent an overflow\n\t// which can be used to trigger a segfault on 32bit architectures\n\t// with this example program:\n\t//\n\t// type T [1<<27 + 1]int64\n\t//\n\t// var d T\n\t// var s []T\n\t//\n\t// func main() {\n\t//   s = append(s, d, d, d, d)\n\t//   print(len(s), \"\\n\")\n\t// }\n\tif cap < old.cap || overflow || capmem > maxAlloc {\n\t\tpanic(errorString(\"growslice: cap out of range\"))\n\t}\n\n\tvar p unsafe.Pointer\n    // 切片元素内不包含指针\n\tif et.kind&kindNoPointers != 0 {\n        // 分配新的底层数组，这里false指示不需要内存清零\n\t\tp = mallocgc(capmem, nil, false)\n        // 不包含指针，内存拷贝\n\t\tmemmove(p, old.array, lenmem)\n\t\t// The append() that calls growslice is going to overwrite from old.len to cap (which will be the new length).\n\t\t// Only clear the part that will not be overwritten.\n        // 新数组中未被使用的内存清零\n\t\tmemclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem)\n\t} else {\n\t\t// Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory.\n        // 因为元素中包含指针，垃圾收集器需要跟踪指针，因此分配内存时需要在位图中标记指针的位置\n\t\tp = mallocgc(capmem, et, true)\n\t\t// 没有开启写屏障，直接拷贝内存\n        if !writeBarrier.enabled {\n\t\t\tmemmove(p, old.array, lenmem)\n\t\t} else { // gc中，开启了写屏障\n\t\t\tfor i := uintptr(0); i < lenmem; i += et.size {\n\t\t\t\ttypedmemmove(et, add(p, i), add(old.array, i))\n\t\t\t}\n\t\t}\n\t}\n\t// 返回新的slice\n\treturn slice{p, old.len, newcap}\n}\n\n```\n\n\n\n### turn string to []byte\n\n当执行强制类型转换，将`string`类型转换成`[]byte`时，会执行`stringtoslicebyte`：\n\n```go\n// The constant is known to the compiler.\n// There is no fundamental theory behind this number.\nconst tmpStringBufSize = 32\n\ntype tmpBuf [tmpStringBufSize]byte\n// 这里的tmpBuf是一个长度为32的数组\nfunc stringtoslicebyte(buf *tmpBuf, s string) []byte {\n   var b []byte\n   // 如果buf不为空并且字符串长度小于32，直接使用buf\n   if buf != nil && len(s) <= len(buf) { \n      *buf = tmpBuf{} // 清零\n      b = buf[:len(s)]\n   } else {\n      b = rawbyteslice(len(s))\n   }\n   copy(b, s)\n   return b\n}\n```\n\n根据上面的逻辑，当对长度小于32的小字符串进行强制类型转换时，会返回一个`cap`为32的`slice`","tags":["go"]},{"title":"channel源码分析","url":"/2019/01/31/channel源码分析/","content":"\n`channel`是`go`中的一种数据结构，可以用来实现并发控制、协程间通信、...\n\n### 结构定义\n\n`channel`结构在`runtime`中的定义如下：\n\n```go\ntype hchan struct {\n\tqcount   uint           // total data in the queue\n\tdataqsiz uint           // size of the circular queue，缓冲区是一个环形队列\n\tbuf      unsafe.Pointer // points to an array of dataqsiz elements\n\telemsize uint16\n\tclosed   uint32 // 标记该channel是否已经关闭\n\telemtype *_type // element type，该channel内数据元素的类型\n\tsendx    uint   // send index\n\trecvx    uint   // receive index\n\trecvq    waitq  // list of recv waiters，\n\tsendq    waitq  // list of send waiters\n\n\t// lock protects all fields in hchan, as well as several\n\t// fields in sudogs blocked on this channel.\n\t//\n\t// Do not change another G's status while holding this lock\n\t// (in particular, do not ready a G), as this can deadlock\n\t// with stack shrinking.\n\tlock mutex\n}\n\ntype waitq struct {\n\tfirst *sudog // sudog封装了阻塞的g\n\tlast  *sudog\n}\n```\n\n### 创建channel\n\n当我们要创建channel时，需要使用`make`接口，对应的创建逻辑如下：\n\n```go \ntype chantype struct {\n\ttyp  _type\n\telem *_type\n\tdir  uintptr\n}\n// 可以看到这里返回的是*hchan，也说明我们代码中的channel实际上就是一个指针类型 \nfunc makechan(t *chantype, size int) *hchan {\n\telem := t.elem // channel的元素类型\n\n\t// compiler checks this but be safe.\n\tif elem.size >= 1<<16 { \n\t\tthrow(\"makechan: invalid channel element type\")\n\t}\n   \t// 内存对齐校验\n\tif hchanSize%maxAlign != 0 || elem.align > maxAlign {\n\t\tthrow(\"makechan: bad alignment\")\n\t}\n\t// buf size校验\n\tif size < 0 || uintptr(size) > maxSliceCap(elem.size) || uintptr(size)*elem.size > maxAlloc-hchanSize {\n\t\tpanic(plainError(\"makechan: size out of range\"))\n\t}\n\n\t// Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers.\n\t// buf points into the same allocation, elemtype is persistent.\n\t// SudoG's are referenced from their owning thread so they can't be collected.\n\t// TODO(dvyukov,rlh): Rethink when collector can move allocated objects.\n\tvar c *hchan\n\tswitch {\n\tcase size == 0 || elem.size == 0: // 缓冲区为0\n\t\t// Queue or element size is zero.\n\t\tc = (*hchan)(mallocgc(hchanSize, nil, true))\n\t\t// Race detector uses this location for synchronization.\n\t\tc.buf = unsafe.Pointer(c)\n\tcase elem.kind&kindNoPointers != 0: // 不包含指针\n\t\t// Elements do not contain pointers.\n\t\t// Allocate hchan and buf in one call.\n\t\tc = (*hchan)(mallocgc(hchanSize+uintptr(size)*elem.size, nil, true))\n\t\tc.buf = add(unsafe.Pointer(c), hchanSize)\n\tdefault:\n\t\t// Elements contain pointers.\n        // 包含指针，hchan和buf要分开分配内存\n\t\tc = new(hchan)\n\t\tc.buf = mallocgc(uintptr(size)*elem.size, elem, true)\n\t}\n\t// 初始化状态\n\tc.elemsize = uint16(elem.size)\n\tc.elemtype = elem\n\tc.dataqsiz = uint(size)\n\n\tif debugChan {\n\t\tprint(\"makechan: chan=\", c, \"; elemsize=\", elem.size, \"; elemalg=\", elem.alg, \"; dataqsiz=\", size, \"\\n\")\n\t}\n\treturn c\n}\n```\n\n### 写入channel\n\n```go\nfunc chansend1(c *hchan, elem unsafe.Pointer) {\n\tchansend(c, elem, true, getcallerpc())\n}\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {\n\t// 往空channel写入，会导致阻塞\n    if c == nil {\n\t\tif !block {\n\t\t\treturn false\n\t\t}\n\t\tgopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2)\n\t\tthrow(\"unreachable\")\n\t}\n\n\tif debugChan {\n\t\tprint(\"chansend: chan=\", c, \"\\n\")\n\t}\n\n\tif raceenabled {\n\t\tracereadpc(unsafe.Pointer(c), callerpc, funcPC(chansend))\n\t}\n\n\t// Fast path: check for failed non-blocking operation without acquiring the lock.\n\t//\n\t// After observing that the channel is not closed, we observe that the channel is\n\t// not ready for sending. Each of these observations is a single word-sized read\n\t// (first c.closed and second c.recvq.first or c.qcount depending on kind of channel).\n\t// Because a closed channel cannot transition from 'ready for sending' to\n\t// 'not ready for sending', even if the channel is closed between the two observations,\n\t// they imply a moment between the two when the channel was both not yet closed\n\t// and not ready for sending. We behave as if we observed the channel at that moment,\n\t// and report that the send cannot proceed.\n\t//\n\t// It is okay if the reads are reordered here: if we observe that the channel is not\n\t// ready for sending and then observe that it is not closed, that implies that the\n\t// channel wasn't closed during the first observation.\n\tif !block && c.closed == 0 && ((c.dataqsiz == 0 && c.recvq.first == nil) ||\n\t\t(c.dataqsiz > 0 && c.qcount == c.dataqsiz)) {\n\t\treturn false\n\t}\n\n\tvar t0 int64\n\tif blockprofilerate > 0 {\n\t\tt0 = cputicks()\n\t}\n\n\tlock(&c.lock)\n\t// channel已经关闭，panic\n\tif c.closed != 0 {\n\t\tunlock(&c.lock)\n\t\tpanic(plainError(\"send on closed channel\"))\n\t}\n\t// 如果有阻塞在写的g，直接发送给这个g\n\tif sg := c.recvq.dequeue(); sg != nil {\n\t\t// Found a waiting receiver. We pass the value we want to send\n\t\t// directly to the receiver, bypassing the channel buffer (if any).\n\t\tsend(c, sg, ep, func() { unlock(&c.lock) }, 3)\n\t\treturn true\n\t}\n\t// buf channel并且有足够buf，写入buf\n\tif c.qcount < c.dataqsiz {\n\t\t// Space is available in the channel buffer. Enqueue the element to send.\n\t\tqp := chanbuf(c, c.sendx)\n\t\tif raceenabled {\n\t\t\traceacquire(qp)\n\t\t\tracerelease(qp)\n\t\t}\n\t\ttypedmemmove(c.elemtype, qp, ep)\n\t\tc.sendx++\n\t\tif c.sendx == c.dataqsiz {\n\t\t\tc.sendx = 0\n\t\t}\n\t\tc.qcount++\n\t\tunlock(&c.lock)\n\t\treturn true\n\t}\n\n\tif !block {\n\t\tunlock(&c.lock)\n\t\treturn false\n\t}\n\n\t// Block on the channel. Some receiver will complete our operation for us.\n    // 写阻塞\n\tgp := getg()\n\tmysg := acquireSudog()\n\tmysg.releasetime = 0\n\tif t0 != 0 {\n\t\tmysg.releasetime = -1\n\t}\n\t// No stack splits between assigning elem and enqueuing mysg\n\t// on gp.waiting where copystack can find it.\n\tmysg.elem = ep\n\tmysg.waitlink = nil\n\tmysg.g = gp\n\tmysg.isSelect = false\n\tmysg.c = c\n\tgp.waiting = mysg\n\tgp.param = nil\n\tc.sendq.enqueue(mysg)\n    // 挂起\n\tgoparkunlock(&c.lock, waitReasonChanSend, traceEvGoBlockSend, 3)\n\t// 这里是阻塞被唤醒之后的逻辑\n\t// someone woke us up.\n\tif mysg != gp.waiting {\n\t\tthrow(\"G waiting list is corrupted\")\n\t}\n\tgp.waiting = nil\n    // channel被关闭时，会设置gp.param=nil\n\tif gp.param == nil {\n\t\tif c.closed == 0 {\n\t\t\tthrow(\"chansend: spurious wakeup\")\n\t\t}\n        // 该channel已经关闭，panic\n\t\tpanic(plainError(\"send on closed channel\"))\n\t}\n\tgp.param = nil\n\tif mysg.releasetime > 0 {\n\t\tblockevent(mysg.releasetime-t0, 2)\n\t}\n\tmysg.c = nil\n\treleaseSudog(mysg)\n\treturn true\n}\n```\n\n\n\n### 从channel读取\n\n```go \n// 单返回值版本\nfunc chanrecv1(c *hchan, elem unsafe.Pointer) {\n\tchanrecv(c, elem, true)\n}\n\n// 两返回值版本\nfunc chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) {\n\t_, received = chanrecv(c, elem, true)\n\treturn\n}\n\nfunc chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) {\n\t// raceenabled: don't need to check ep, as it is always on the stack\n\t// or is new memory allocated by reflect.\n\n\tif debugChan {\n\t\tprint(\"chanrecv: chan=\", c, \"\\n\")\n\t}\n\t// 从空channel读会导致阻塞\n\tif c == nil {\n\t\tif !block {\n\t\t\treturn\n\t\t}\n\t\tgopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2)\n\t\tthrow(\"unreachable\")\n\t}\n\n\t// Fast path: check for failed non-blocking operation without acquiring the lock.\n\t//\n\t// After observing that the channel is not ready for receiving, we observe that the\n\t// channel is not closed. Each of these observations is a single word-sized read\n\t// (first c.sendq.first or c.qcount, and second c.closed).\n\t// Because a channel cannot be reopened, the later observation of the channel\n\t// being not closed implies that it was also not closed at the moment of the\n\t// first observation. We behave as if we observed the channel at that moment\n\t// and report that the receive cannot proceed.\n\t//\n\t// The order of operations is important here: reversing the operations can lead to\n\t// incorrect behavior when racing with a close.\n\tif !block && (c.dataqsiz == 0 && c.sendq.first == nil ||\n\t\tc.dataqsiz > 0 && atomic.Loaduint(&c.qcount) == 0) &&\n\t\tatomic.Load(&c.closed) == 0 {\n\t\treturn\n\t}\n\n\tvar t0 int64\n\tif blockprofilerate > 0 {\n\t\tt0 = cputicks()\n\t}\n\n\tlock(&c.lock)\n\t// 如果channel已经关闭，并且缓冲区内元素数量为0，则返回空元素\n\tif c.closed != 0 && c.qcount == 0 {\n\t\tif raceenabled {\n\t\t\traceacquire(unsafe.Pointer(c))\n\t\t}\n\t\tunlock(&c.lock)\n\t\tif ep != nil {\n\t\t\ttypedmemclr(c.elemtype, ep)\n\t\t}\n\t\treturn true, false // 第二个返回值false表示没有读取到真正的内容\n\t}\n\t// 如果存在等待写的g，直接从这个g读取\n\tif sg := c.sendq.dequeue(); sg != nil {\n\t\t// Found a waiting sender. If buffer is size 0, receive value\n\t\t// directly from sender. Otherwise, receive from head of queue\n\t\t// and add sender's value to the tail of the queue (both map to\n\t\t// the same buffer slot because the queue is full).\n\t\trecv(c, sg, ep, func() { unlock(&c.lock) }, 3)\n\t\treturn true, true\n\t}\n\t// 如果缓冲区存在元素，则读取缓冲区元素\n    // channel即使已经被关闭，如果缓冲区还有元素，仍可以读取\n\tif c.qcount > 0 {\n\t\t// Receive directly from queue\n\t\tqp := chanbuf(c, c.recvx)\n\t\tif raceenabled {\n\t\t\traceacquire(qp)\n\t\t\tracerelease(qp)\n\t\t}\n\t\tif ep != nil {\n\t\t\ttypedmemmove(c.elemtype, ep, qp)\n\t\t}\n\t\ttypedmemclr(c.elemtype, qp)\n\t\tc.recvx++\n\t\tif c.recvx == c.dataqsiz {\n\t\t\tc.recvx = 0\n\t\t}\n\t\tc.qcount--\n\t\tunlock(&c.lock)\n\t\treturn true, true\n\t}\n\n\tif !block {\n\t\tunlock(&c.lock)\n\t\treturn false, false\n\t}\n\n\t// no sender available: block on this channel.\n    // 阻塞在写操作\n\tgp := getg()\n\tmysg := acquireSudog()\n\tmysg.releasetime = 0\n\tif t0 != 0 {\n\t\tmysg.releasetime = -1\n\t}\n\t// No stack splits between assigning elem and enqueuing mysg\n\t// on gp.waiting where copystack can find it.\n\tmysg.elem = ep\n\tmysg.waitlink = nil\n\tgp.waiting = mysg\n\tmysg.g = gp\n\tmysg.isSelect = false\n\tmysg.c = c\n\tgp.param = nil\n\tc.recvq.enqueue(mysg)\n    // 挂起\n\tgoparkunlock(&c.lock, waitReasonChanReceive, traceEvGoBlockRecv, 3)\n\n    // 这之后是唤醒之后的逻辑\n\t// someone woke us up\n\tif mysg != gp.waiting {\n\t\tthrow(\"G waiting list is corrupted\")\n\t}\n\tgp.waiting = nil\n\tif mysg.releasetime > 0 {\n\t\tblockevent(mysg.releasetime-t0, 2)\n\t}\n\tclosed := gp.param == nil\n\tgp.param = nil\n\tmysg.c = nil\n\treleaseSudog(mysg)\n\t// 如果已经关闭就返回false\n    return true, !closed\n}\n```\n\n\n\n### 关闭channel\n\n```go \n\nfunc closechan(c *hchan) {\n    // 如果关闭nil channel，则panic\n\tif c == nil {\n\t\tpanic(plainError(\"close of nil channel\"))\n\t}\n\n\tlock(&c.lock)\n   \t// 如果channel已经关闭，则panic\n\tif c.closed != 0 {\n\t\tunlock(&c.lock)\n\t\tpanic(plainError(\"close of closed channel\"))\n\t}\n\t// 竞争检测\n\tif raceenabled {\n\t\tcallerpc := getcallerpc()\n\t\tracewritepc(unsafe.Pointer(c), callerpc, funcPC(closechan))\n\t\tracerelease(unsafe.Pointer(c))\n\t}\n\t// 设置关闭标志位\n\tc.closed = 1\n\t// glist用于收集释放的g\n\tvar glist *g\n\n\t// release all readers\n    // 关闭时，要释放所有阻塞在读操作的协程\n\tfor {\n\t\tsg := c.recvq.dequeue()\n\t\tif sg == nil {\n\t\t\tbreak\n\t\t}\n        // 从已关闭的channel中读取的都是空内容，因此这里将元素内存置零\n\t\tif sg.elem != nil {\n\t\t\ttypedmemclr(c.elemtype, sg.elem)\n\t\t\tsg.elem = nil\n\t\t}\n\t\tif sg.releasetime != 0 {\n\t\t\tsg.releasetime = cputicks()\n\t\t}\n        // 从sudog中获取阻塞的g\n\t\tgp := sg.g\n\t\tgp.param = nil\n\t\tif raceenabled {\n\t\t\traceacquireg(gp, unsafe.Pointer(c))\n\t\t}\n        // 链表设置\n\t\tgp.schedlink.set(glist)\n\t\tglist = gp\n\t}\n\n\t// release all writers (they will panic)\n    // 释放等待写的channel\n\tfor {\n\t\tsg := c.sendq.dequeue()\n\t\tif sg == nil {\n\t\t\tbreak\n\t\t}\n\t\tsg.elem = nil\n\t\tif sg.releasetime != 0 {\n\t\t\tsg.releasetime = cputicks()\n\t\t}\n\t\tgp := sg.g\n\t\tgp.param = nil\n\t\tif raceenabled {\n\t\t\traceacquireg(gp, unsafe.Pointer(c))\n\t\t}\n\t\tgp.schedlink.set(glist)\n\t\tglist = gp\n\t}\n\tunlock(&c.lock)\n\n\t// Ready all Gs now that we've dropped the channel lock.\n    // 将上面释放的所有g设置为就绪状态，等待调度\n\tfor glist != nil {\n\t\tgp := glist\n\t\tglist = glist.schedlink.ptr()\n\t\tgp.schedlink = 0\n\t\tgoready(gp, 3)\n\t}\n}\n\n```\n\n","tags":["go","channel"]},{"title":"golang执行command","url":"/2018/12/27/golang执行command/","content":"\n### 在golang中使用cmd\n\n在日常开发中，我们有时候需要在程序中调用系统的其他指令来完成任务，比如通过调用`mysqldump`来执行数据库备份。\n\n`golang`提供了`Cmd`，可以很方便的帮助我们来完成这些内容。\n\n> Package exec runs external commands. It wraps os.StartProcess to make it\n> easier to remap stdin and stdout, connect I/O with pipes, and do other\n> adjustments.\n>\n> Unlike the \"system\" library call from C and other languages, the\n> os/exec package intentionally does not invoke the system shell and\n> does not expand any glob patterns or handle other expansions,\n> pipelines, or redirections typically done by shells. The package\n> behaves more like C's \"exec\" family of functions. To expand glob\n> patterns, either call the shell directly, taking care to escape any\n> dangerous input, or use the path/filepath package's Glob function.\n> To expand environment variables, use package os's ExpandEnv.\n\n### demo\n\n```go\nfunc Backup(p string){\n    cmd := exec.Cmd{}\n\tcmd.Path = \"/usr/bin/mysqldump\"\n    cmd.Args = []string{\"-uuname\", \"-ppasswd\", `db_name`}\n\tfd, err := os.OpenFile(p, os.O_CREATE|os.O_TRUNC|os.O_RDWR, 0666)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer fd.Close()\n\tcmd.Stderr = os.Stderr // 重定向错误输出，可以在控制台中看到子进程的错误信息，方便排查\n\tcmd.Stdout = fd // 重定向cmd的输出，保存到目标文件中\n\terr = cmd.Run() // Run实际上就是Start和Wait的组合，会等待子进程结束才返回，如果需要异步直接使用Start\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc Restore(p string) {\n\tcmd := exec.Cmd{}\n\tcmd.Path = \"/usr/bin/mysql\"\n    cmd.Args = []string{\"-uuname\", \"-ppasswd\", \"-Ddb_name\"}\n\tfd, err := os.Open(p)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer fd.Close()\n\tcmd.Stdin = fd // 重定向标准输出为打开文件\n\tcmd.Stdout = os.Stdout\n\tcmd.Stderr = os.Stderr\n\tif err := cmd.Run(); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n如同上面`demo`所示，我们可以通过重定向子进程的`stdin`，`stdout`，`stderr`等\n\n此外，`Cmd`也提供了`Pipe`接口，看一下实现：\n\n```go\nfunc (c *Cmd) StdinPipe() (io.WriteCloser, error) {\n\tif c.Stdin != nil {\n\t\treturn nil, errors.New(\"exec: Stdin already set\")\n\t}\n\tif c.Process != nil {\n\t\treturn nil, errors.New(\"exec: StdinPipe after process started\")\n\t}\n\tpr, pw, err := os.Pipe() // 创建一条管道\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tc.Stdin = pr // 重定向标准输出为读取端\n\tc.closeAfterStart = append(c.closeAfterStart, pr)\n\twc := &closeOnce{File: pw} // 包装管道的写出端\n\tc.closeAfterWait = append(c.closeAfterWait, wc)\n\treturn wc, nil // 返回\n}\n```\n\n管道可以用于两个进程之间单向传输数据，一端用于写入，一端用于读取。\n\n\n\n### Cmd不是Shell\n\n**当我们在控制台执行命令的时候，实际上我们输入的命令会先通过`shell`进行预处理，然后才会被实际的程序执行**\n\n比如，当我们在控制台执行：\n\n```sh\n$ rm -rf *\n```\n\n`shell`会先将`*`替换成所有匹配的文件列表，然后再把`-rf`和待删除的文件列表传给`rm`命令执行\n\n而如果通过`Cmd`进行调用，并不会执行这些预处理。\n\n比如：\n\n```go\nfunc main(){\n    cmd := exec.Cmd{}\n\tcmd.Path = \"/bin/rm\"\n    cmd.Args = []string{\"-r\", \"-f\", \"*\"}\n\tcmd.Stdout = os.Stdout\n\tcmd.Stderr = os.Stderr\n\tif err := cmd.Run(); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n`*`会直接作为参数传递给`rm`，而`rm`本身并不会执行模糊匹配，而是把`*`当做普通的文件名对待，如果当前目录没有存在文件名为`*`的文件，则会报错：`No such file or directory`\n\n解决的方法一：\n\n```go\nfunc main(){\n    cmd := exec.Cmd{}\n\tcmd.Path = \"/bin/bash\"\n    cmd.Args = []string{\"-c\", \"rm -rf *\"} // 使用 bash -c \"rm -rf *\"\n\tcmd.Stdout = os.Stdout\n\tcmd.Stderr = os.Stderr\n\tif err := cmd.Run(); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n解决方法二：\n\n```go\nfunc main(){\n    fs, _ := filepath.Glob(\"*\") // 获取匹配`*`的文件列表\n    cmd := exec.Cmd{}\n\tcmd.Path = \"/bin/rm\"\n    cmd.Args = []string{\"-r\", \"-f\"}\n    cmd.Args = append(cmd.Args, fs...)\n\tcmd.Stdout = os.Stdout\n\tcmd.Stderr = os.Stderr\n\tif err := cmd.Run(); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n","tags":["go"]},{"title":"grpc上手使用","url":"/2018/12/21/grpc上手使用/","content":"\n# grpc上手使用\n\n### 安装\n\n`golang`版本的`grpc`要求`go`版本要在`1.6`以上\n\n##### install gRPC\n\n使用`go get`命令安装`grpc`包\n\n```sh\n$ go get -u google.golang.org/grpc\n```\n\n> 由于某些不可逆原因，上面命令会报连接超时，可以到`github`上将项目`clone`到`$GOPATH/src/google.golang.org/`下\n>\n> ```sh\n> $ cd $GOPATH/src/google.golang.org\n> $ git clone git@github.com:grpc/grpc-go.git grpc\n> ```\n\n##### install Protocol Buffers  v3\n\n`grpc`默认使用`protobuf`作为序列化工具。\n\n1. 打开[Releases](https://github.com/protocolbuffers/protobuf/releases)页面，下载对应平台的`.zip`包`protoc-<version>-<platform>.zip`\n2. 解压\n3. 添加二进制文件路径导`PATH`环境变量\n\n##### install protoc plugin\n\n安装`golang`版本对应的`protobuf`生成工具\n\n```sh\n$ go get -u github.com/golang/protobuf/protoc-gen-go\n$ export PATH=$PATH:$GOPATH/bin\n```\n\n### 运行demo\n\n进入`example`目录\n\n```sh\n$ cd $GOPATH/src/google.golang.org/grpc/examples/helloworld\n```\n\n删除原来的`helloworld.pb.go`文件，并使用`protoc`生成自己生成一个\n\n```sh\n$ rm helloworld/helloworld.pb.go // 删除原来的helloworld.pb.go文件\n$ protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld // 根据 .proto 文件生成对应的.go文件\n```\n\n编写`grpc`接口时，在`.proto`文件定义接口通信数据格式和接口信息，然后通过`protoc`自动生成对应的`go`代码，大大方便了开发\n\n- `-I PATH`：specify the directory in which to search for imports.  May be specified multiple times; directories will be searched in order.  If not given, the current working directory is used.\n- `--go_out`：指定输出`go`代码\n- `plugins=grpc`：`.proto`中的`service `是`grpc`扩展的功能，需要使用`grpc`插件进行解析才能生成对应的接口定义代码。\n\n运行 `grpc server `和 `grpc client`\n\n```sh\n$ go run greeter_server/main.go // 启动grpc server\n$ go run greeter_client/main.go // 启动grpc client\n```\n\n\n\n### 实践\n\n使用`grpc`开发一个简单的求和服务。\n\n##### 定义.proto文件\n\n在项目下创建`proto/sum.proto`文件：\n\n```protobuf\nsyntax = \"proto3\"; // 使用 proto3\n\n// java生成选项\noption java_multiple_files = true;\noption java_package = \"io.grpc.examples.helloworld\";\noption java_outer_classname = \"HelloWorldProto\";\n\npackage proto; // 生成的go所属的package\n\nmessage SumResp {\n    int64 sum = 1;\n}\n\nmessage SumReq {\n    int64 a = 1;\n    int64 b = 2;\n}\n\n\nservice CalcSvc {\n    // 每个rpc接口声明都必须有且一个参数和一个返回值\n    rpc Sum(SumReq) returns (SumResp) {}\n}\n```\n\n##### 根据接口描述文件生成源码\n\n进入`proto`目录，执行\n\n```sh\n$ protoc sum.proto --go_out=plugins=grpc:.\n```\n\n可以看到，在本目录下生成`sum.pb.go`文件，且`package`为`proto`\n\n##### 开发服务端接口\n\n首先查看生成的`sum.pb.go`文件，可以看到根据`sum.proto`文件中的`CalcSvc`接口定义生成了对应的接口：\n\n```go\n// CalcSvcServer is the server API for CalcSvc service.\ntype CalcSvcServer interface {\n\t// 每个rpc接口声明都必须有且一个参数和一个返回值\n\tSum(context.Context, *SumReq) (*SumResp, error)\n}\n```\n\n开发服务端接口只要就是根据这些接口定义实现具体的业务逻辑\n\n在项目下创建`service/main.go`：\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/reflection\"\n\t\"grpc-demo/proto\"\n\t\"log\"\n\t\"net\"\n)\n\n// 类型断言\nvar _ proto.CalcSvcServer = new(CalcSvc)\n\ntype CalcSvc struct{}\n\nfunc (CalcSvc) Sum(ctx context.Context, req *proto.SumReq) (resp *proto.SumResp, err error) {\n    // 建议使用GetA，不要直接使用req.A，可能存在req=nil的情况\n\ta := req.GetA() \n\tb := req.GetB()\n\tlog.Println(\"request coming ...\")\n\treturn &proto.SumResp{\n\t\tSum: a + b,\n\t}, err\n}\n\nfunc main() {\n\tlis, err := net.Listen(\"tcp\", \":8888\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n    // 注册服务到gRPC\n\ts := grpc.NewServer()\n\tproto.RegisterCalcSvcServer(s, &CalcSvc{})\n    // 启用Server Reflection，可以使用gRPC CLI去检查services\n    // https://github.com/grpc/grpc-go/blob/master/Documentation/server-reflection-tutorial.md\n\treflection.Register(s)\n    // 启动服务\n\tif err := s.Serve(lis); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n```\n\n##### 客户端访问\n\n在项目下创建`client/main.go`：\n\n```go \npackage main\n\nimport (\n\t\"context\"\n\t\"google.golang.org/grpc\"\n\t\"grpc-demo/proto\"\n\t\"log\"\n)\n\nfunc main() {\n    // 创建gRPC连接\n    // WithInsecure option 指定不启用认证功能\n\tconn, err := grpc.Dial(\":8888\", grpc.WithInsecure())\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n    // 创建gRPC client\n\tclient := proto.NewCalcSvcClient(conn)\n    // 请求gRPC server\n\tresp, err := client.Sum(context.Background(), &proto.SumReq{\n\t\tA: 5,\n\t\tB: 10,\n\t})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tlog.Printf(\"5 + 10 = %d\", resp.GetSum())\n}\n```\n\n##### 运行\n\n```go \n$ go run service/main.go\n$ go run client/main.go\n```\n\n\n\n### grpc连接复用\n\n首先修改服务端代码，**添加 `1s` 的睡眠时间**，模拟复杂业务处理场景：\n\n```go\nfunc (CalcSvc) Sum(ctx context.Context, req *proto.SumReq) (resp *proto.SumResp, err error) {\n\ta := req.GetA()\n\tb := req.GetB()\n\tlog.Println(\"request coming ...\")\n    // 添加 1s 睡眠，模拟接口执行业务逻辑\n\ttime.Sleep(time.Second)\n\treturn &proto.SumResp{\n\t\tSum: a + b,\n\t}, err\n}\n```\n\n##### http2多路复用\n\n`grpc`底层使用`http2`协议进行通信，因此单条连接支持多路复用\n\n修改客户端代码：\n\n```go\n\nfunc main() {\n\tconn ,err:=grpc.Dial(\":8888\", grpc.WithInsecure())\n\tif err!=nil {\n\t\tlog.Fatal(err)\n\t}\n\tclient :=proto.NewCalcSvcClient(conn)\n\n\twg := sync.WaitGroup{}\n\tbegin := time.Now()\n\tconcurrentNum := 1000\n\twg.Add(concurrentNum)\n    \n\tfor i := 0; i < concurrentNum; i++ {\n\t\tgo func() {\n\t\t\tresp, err := client.Sum(context.Background(), &proto.SumReq{\n\t\t\t\tA: 5,\n\t\t\t\tB: 10,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tlog.Printf(\"5 + 10 = %d\", resp.GetSum())\n\t\t\twg.Done()\n\t\t}()\n\t}\n\twg.Wait()\n\tlog.Printf(\"用时：%v\", time.Now().Sub(begin))\n}\n```\n\n在上面代码中，服务端每次都睡眠`1s`，客户端使用单条连接进行通信，**1000个并发请求总共执行时间为`1.1s`左右**\n\n如果是`2000`个请求，平均在`1.2s`左右，`10000`个请求是`2`s左右。\n\n可见`grpc`本身单条连接可用提供的并发效果足以满足大部分业务场景。\n\n**注意：**上面的`1000`个并发请求并不是单条连接可以同时发起`1000`个请求，而是其内部支持类似`pipeline`的机制。\n\n##### 连接池\n\n接下来不使用`http2`的多路复用，采用连接池的方式来创建请求\n\n首先实现一个连接池：\n\n```go\npackage main\n\nimport (\n\t\"google.golang.org/grpc\"\n\t\"sync\"\n\t\"time\"\n)\n\n// 连接池选项\ntype Options struct {\n\tDial        Dialer\n\tMaxConn     int\n\tMaxIdle     int\n\tWaitTimeout time.Duration\n}\n\n// 创建连接\ntype Dialer func() (*grpc.ClientConn, error)\n\ntype Pool struct {\n\tdial    Dialer\n\tmaxConn int // 最大打开连接数\n\tmaxIdle int // 最大空闲连接数\n\n\twaitTimeout time.Duration // 等待连接超时时间\n    // 等待连接时通过connCh来传输可用连接\n\tconnCh      chan *grpc.ClientConn\n\n\tcurConnNum int // 记录当前打开的连接数\n    // 保存空闲连接\n\tfreeConn   []*grpc.ClientConn\n\tsync.Mutex\n}\n\n// 创建连接池\nfunc NewPool(opts Options) *Pool {\n\tif opts.MaxConn <= 0 {\n\t\topts.MaxConn = 10\n\t}\n\tif opts.MaxIdle <= 0 {\n\t\topts.MaxIdle = 5\n\t}\n\tif opts.MaxIdle > opts.MaxConn {\n\t\topts.MaxIdle = opts.MaxIdle\n\t}\n\n\treturn &Pool{\n\t\tdial:        opts.Dial,\n\t\tmaxConn:     opts.MaxConn,\n\t\tmaxIdle:     opts.MaxIdle,\n\t\twaitTimeout: opts.WaitTimeout,\n\t\tconnCh:      make(chan *grpc.ClientConn),\n\t\tfreeConn:    make([]*grpc.ClientConn, 0, opts.MaxIdle),\n\t}\n\n}\n\n// 获取连接\nfunc (p *Pool) Get() (conn *grpc.ClientConn) {\n\tp.Lock()\n\t// 已经到达最大连接数\n\tif p.curConnNum >= p.maxConn {\n        // 如果等待超时时间为0，直接返回\n\t\tif p.waitTimeout == 0 {\n\t\t\tp.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\tvar tm <-chan time.Time\n        // 如果等待超时时间小于0，表示无限等待\n\t\tif p.waitTimeout > 0 {\n\t\t\ttm = time.After(p.waitTimeout)\n\t\t}\n\t\tp.Unlock()\n        // 等待可用连接或者超时\n\t\tselect {\n\t\tcase <-tm:\n\t\tcase conn = <-p.connCh:\n\t\t}\n\t\treturn\n\t}\n\t// 如果存在空闲连接\n\tif ln := len(p.freeConn); ln > 0 {\n\t\tconn = p.freeConn[0]\n\t\tp.freeConn[0] = p.freeConn[ln-1]\n\t\tp.freeConn = p.freeConn[:ln-1]\n\t} else { // 创建新的连接\n\t\tc, err := p.dial()\n\t\tif err != nil {\n\t\t\tconn = nil\n\t\t} else {\n\t\t\tp.curConnNum++\n\t\t\tconn = c\n\t\t}\n\t}\n\tp.Unlock()\n\treturn\n}\n\n// 释放连接\nfunc (p *Pool) Put(conn *grpc.ClientConn) error {\n\tif conn == nil {\n\t\treturn nil\n\t}\n    // 首先判断是否有其他协程在等待连接\n\tselect {\n\tcase p.connCh <- conn:\n\t\treturn nil\n\tdefault:\n\t}\n\tp.Lock()\n\tdefer p.Unlock()\n    // 放回空闲连接\n\tif len(p.freeConn) < p.maxIdle {\n\t\tp.freeConn = append(p.freeConn, conn)\n\t\treturn nil\n\t}\n    // 再次判断是否有等待可用连接\n\tselect {\n\tcase p.connCh <- conn:\n\t\treturn nil\n\tdefault:\n        // 关闭连接\n\t\tp.curConnNum--\n\t\treturn conn.Close()\n\t}\n}\n\n// 统计连接池状态\nfunc (p *Pool) Stat() PoolStat {\n\tp.Lock()\n\tp.Unlock()\n\treturn PoolStat{\n\t\tConnNum:     p.curConnNum,\n\t\tIdleConnNum: len(p.freeConn),\n\t}\n}\n\ntype PoolStat struct {\n\tConnNum     int\n\tIdleConnNum int\n}\n\n```\n\n接下来，使用该连接池进行测试：\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"google.golang.org/grpc\"\n\t\"grpc-demo/proto\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n)\n\nfunc main() {\n\topts := Options{\n\t\tDial: func() (*grpc.ClientConn, error) {\n\t\t\treturn grpc.Dial(\":8888\", grpc.WithInsecure())\n\t\t},\n\t\tWaitTimeout: time.Second * 10,\n\t\tMaxConn:     100, // 设置最大连接数为100\n\t\tMaxIdle:     50,\n\t}\n\tpool := NewPool(opts)\n\tif pool == nil {\n\t\tpanic(\"nil pool\")\n\t}\n\n\twg := sync.WaitGroup{}\n\tbegin := time.Now()\n\tconcurrentNum := 1000\n\twg.Add(concurrentNum)\n\tfor i := 0; i < concurrentNum; i++ {\n\t\tgo func() {\n\n\t\t\tconn := pool.Get()\n\t\t\tif conn == nil {\n\t\t\t\tpanic(\"nil conn\")\n\t\t\t}\n\t\t\tdefer pool.Put(conn)\n\t\t\tclient := proto.NewCalcSvcClient(conn)\n\n\t\t\tresp, err := client.Sum(context.Background(), &proto.SumReq{\n\t\t\t\tA: 5,\n\t\t\t\tB: 10,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\tlog.Printf(\"5 + 10 = %d\", resp.GetSum())\n\t\t\twg.Done()\n\t\t}()\n\t}\n\twg.Wait()\n\tlog.Printf(\"用时：%v\", time.Now().Sub(begin))\n\tlog.Println(pool.Stat())\n}\n```\n\n在上面的代码中，每次请求时都从连接池中获取一个连接，请求完成后将其释放。\n\n运行上面代码，**`1000`个并发请求总共需要花费`10.15s`左右**。\n\n\n\n### 负载均衡\n\n这里使用`dns`来进行负载均衡进行演示。\n\n我实验机器上面的本机`IP`是`127.0.0.1`，虚拟机`IP`是`192.168.50.12`\n\n首先，修改系统的`hosts`文件，添加：\n\n```\n192.168.50.12 www.grpc.com\n127.0.0.1 www.grpc.com\n```\n\n然后，同时在本地和虚拟机中启动`grpc server`\n\n最后，修改`grpc client`代码：\n\n```go\nconn, err := grpc.Dial(\"dns:///www.grpc.com:8888\", grpc.WithInsecure(), grpc.WithBalancerName(roundrobin.Name))\nif err != nil {\n\tlog.Fatal(err)\n}\nclient := proto.NewCalcSvcClient(conn)\n```\n\n在创建`grpc`连接的时候，使用`dns:///www.grpc.com:8888`，同时指定负载策略为`roundrobin`。\n\n执行`grpc client`，可用看到**两边的`grpc server`都有打印出请求日志**。\n\n`grpc`提供的负载均衡测试是在**请求级别上进行负载均衡**。\n\n`grpc`会同时为每个`grpc server`创建一条连接；每次要发起一个请求的时候，都会根据负载策略选择一条连接来发起请求。","tags":["go","grpc"]},{"title":"go编译共享库给c调用","url":"/2018/12/19/go编译共享库给c调用/","content":"\n### introduce\n\n使用` golang`开发`httpServer`非常方便，如果我们需要在`c`程序中内嵌`httpServer`，可以考虑使用`go`来开发服务模块，然后编译成共享库供`c`调用\n\n### code by go \n\n##### code\n\n```go\npackage main\n\nimport (\n\t\"net/http\"\n\t\"time\"\n\t\"log\"\n)\n\nimport \"C\" // 需要导入`C`才可以生成`.h`文件\n\n\n// 使用`export`导出函数\n//export ServerRun\nfunc ServerRun(_addr *C.char) int {\n    // 转换c字符串为golang字符串\n    addr :=C.GoString(_addr)\n\tmux := http.NewServeMux()\n\tmux.HandleFunc(\"/\", func(resp http.ResponseWriter, req *http.Request) {\n\t\tresp.Write([]byte{'H', 'i', '!'})\n\t})\n\tif err := http.ListenAndServe(addr, mux); err != nil {\n\t\tlog.Println(err.Error())\n\t\treturn -1\n\t}\n\treturn 0\n}\n\n// 内部函数也可以导出\n//export wait\nfunc wait() {\n\ttime.Sleep(time.Hour * 1)\n}\n\nfunc main() {}\n```\n\n注意点：\n\n- 需要引入`C`包，可以使用`C`包中的`GoString`将`c`的字符串转换为`go`的字符串\n\n- 需要导出的函数，需要使用`//export funcName`标识\n\n- 包内函数也可以导出\n\n- `go`和`c`两者的字符串内存布局不同，如果`go`函数参数声明为`go`的字符串类型，在`c`中相当于一个结构体：\n\n  ```c\n  typedef struct { const char *p; ptrdiff_t n; } _GoString_;\n  typedef _GoString_ GoString;\n  ```\n\n  当要在`c`中调用`go`函数时，需要手动构造字符串，而且还有内存安全的问题。\n\n##### compile\n\n- 动态共享库：运行时动态加载；如果运行时加载失败则报错\n\n  ```sh\n  $ go build -buildmode=c-shared -o libtest.so main.go\n  ```\n\n  编译完成之后将生成`libtest.so`和`libtest.h`文件\n\n- 静态共享库：编译时静态链接到程序中；生成二进制文件较大\n\n  ```sh\n  $ go build -buildmode=c-archive -o test.a main.go\n  ```\n\n  编译完成之后将生成`test.h`和`test.a`文件\n\n\n\n### use in c \n\n##### code \n\n在开始写代码之前，我们要先看一下生成的`test.h`里面的内容：\n\n```c\ntypedef struct { const char *p; ptrdiff_t n; } _GoString_;\ntypedef _GoString_ GoString;\n\ntypedef long long GoInt64;\ntypedef GoInt64 GoInt;\n\nextern GoInt ServerRun(char* p0);\n\nextern void wait();\n```\n\n可以看到，`.h`文件中包含了外部函数`ServerRun`和`wait`的声明\n\n```c\n#include<stdio.h>\n#include \"test.h\"\n\nint main(void){\n\t// 执行 ServerRun\n    if (ServerRun(\":8080\") != 0){\n        printf(\"failed to start server!\");\n        return -1;\n    }\n    return 0;\n}\n```\n\n\n\n##### compile \n\n- 静态共享库\n\n  ```sh\n  $ gcc -pthread -o test main.c test.a \n  ```\n\n  使用静态链接时，需要指定`-pthread`选项 \n\n  > Link with the POSIX threads library.  This option is supported on GNU/Linux targets, most other Unix derivatives, and also on x86 Cygwin and MinGW targets.  On some targets this option also sets flags for the preprocessor, so it should be used consistently for both compilation and linking.\n\n  也可以动态加载`pthread`库\n\n  ```sh\n  $ gcc -lpthread -o test main.c test.a\n  ```\n\n- 动态共享库\n\n  ```sh\n  $ gcc main.c -ltest -L. -I. -o test\n  ```\n\n  - `-l`：声明使用到的动态共享库，比如`libtest.so`，则这里传入`test`\n  - `-L`：在指定路径中查找共享库；也可以将`.so`文件拷贝到默认共享库目录下\n  - `-I`：在指定路径中查找`.h`头部文件\n\n\n\n编译之后生成`test`文件，执行`./test`，然后在访问`http://localhost:8080`可以看到返回了`Hi!`内容。\n\n如果使用动态加载，运行前需要先将`libtest.so`文件拷贝到动态加载库默认的加载路径中，或者将当前路径加到`LD_LIBRARY_PATH `环境变量中。\n","tags":["go"]},{"title":"goland 中获取 goid","url":"/2018/08/18/goland-中获取-goid/","content":"\n### introduce\n\n目前网上有很多获取goroutine id的方法，主要分为两种：\n\n- 通过runtime.Stack方法获取栈的信息，而栈信息以`goroutine {goid}` 开头，再通过字符串处理就可以提取出goid\n- go中通过g来表示goroutine，而在tls中保存了当前执行的g的地址。可以通过汇编获取到g的地址，然后加上goid在g中的偏移量就可以获取到goid的值了\n\n第一种方法实现方便，只要通过简单的字符串处理就可以获取到goid，但是性能开销较大；\n\n第二种方法，需要结合汇编来获取当前执行的g的地址，而且需要获取到goid在g中的偏移量；而不同的版本中g的结构都不一样，因此该方法需要为每个版本都提供一种实现\n\n### code\n\n下面将基于go1.10实现上面两种获取goid的方案。\n\n##### 方法一：\n\n```go\n\tstack := make([]byte, 20) //读取前二十个字节\n\truntime.Stack(stack, false)\n\tgoid,_ :=strconv.Atoi(strings.Split(string(stack),\" \")[1])\n```\n\n在上面的实现中，读取栈的前20个字节，其内容为`goroutine 6 ...`，我们这里只需要关注goid在字符串数组第二的位置，然后通过简单的字符串切割和类型转换就可以获取到goid了\n\n##### 方法二：\n\n因为g的定义在runtime.runtime2.go中，我们需要将其拷贝出来\n\n```go\ntype g struct {\n\tstack       stack\n\tstackguard0 uintptr\n    _defer      uintptr\n\t...\n\tgoid           int64\n\t...\n}\n\ntype stack struct {\n\tlo uintptr\n\thi uintptr\n}\n\ntype gobuf struct {\n\tsp   uintptr\n\tpc   uintptr\n\tg    uintptr\n\tctxt unsafe.Pointer\n\tret  uint64\n\tlr   uintptr\n\tbp   uintptr\n}\n```\n\n拷贝的时候，因为g中还引用了其他类型，也需要一起拷贝出来。这里有个小技巧，因为我们只是需要使用g来计算goid的偏移量，因此如果有的字段是指针类型的，那么可以将其换成`uintptr`类型。比如说`_defer`是`*_defer`类型，那么可以将其换成`uintptr`类型，这样我们就不需要在自己的代码中声明`_defer`结构了。\n\n然后，声明全局变量`offset`\n\n```go\nvar offset =unsafe.Offsetof((*g)(nil).goid)\n```\n\n我们通过`unsafe.Offsetof`方法来计算goid在g中的偏移。\n\n接着，在go文件中声明Goid方法的stub\n\n```go\nfunc Goid()int64\n```\n\n并在goid.s中实现该函数\n\n```assembly\nTEXT ·Goid(SB),NOSPLIT,$0-8\n    MOVQ ·offset(SB),AX\t//获取到全局变量offset\n    MOVQ (TLS),BX\t\t//获取当前g的地址\n    ADDQ BX,AX\t\t\t//计算goid的地址\n    MOVQ (AX),BX\t\t//获取goid的值\n    MOVQ BX,ret+0(FP)\n    RET\n\t//最后的空行必须保留，否则编译报错\n```\n\n在上述实现中，我直接在汇编中计算goid在内存中的地址。还有一种实现是在汇编中获取g的地址，然后将其转换成*g类型并获取goid的值，这样就不需要计算offset的值了，但是在实际测试中，前者的执行速度是后者的两倍。\n\n\n\n以上代码可以在[github](https://github.com/ymcvalu/goid)上查看\n\n\n\n","tags":["go","goid"]},{"title":"go自定义类型的序列化过程","url":"/2018/08/13/go自定义类型的序列化过程/","content":"\n\n\n### 问题引入\n当某个struct存在某个字段为string或者[]byte类型但是实际上保存的内容是json格式的数据时，对其进行json序列化，比如\n```go\ntype Message struct {\n\tFrom string     `json:\"from\"`\n\tTo   string     `json:\"to\"`\n\tData string `json:\"data\"`\n}\n\nfunc main() {\n\tmsg := Message{\n\t\tFrom: \"XiaoMing\",\n\t\tTo:   \"LiGang\",\n\t\tData: `{\"title\":\"test\",\"body\":\"something\"}`,\n\t}\n\tjsonData, err := json.Marshal(msg)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfmt.Println(string(jsonData))\n}\n```\n在上面的例子中，Data字段是string类型，但是保存的内容是json格式的数据，这个时候，程序输出：\n```json\n{\"from\":\"XiaoMing\",\"to\":\"LiGang\",\"data\":\"{\\\"title\\\":\\\"test\\\",\\\"body\\\":\\\"something\\\"}\"}\n```\n可以看到，序列化之后的data是一个字符串。\n如果Message对应的是数据库中的一张表，而data字段在数据库中是json类型，当我们需要一个接口，查询Message表中的记录返回给客户端。如果直接执行序列化，那么客户端获取到的Data实际上是一个字符串，客户端还需要自行对这个字符串进行json反序列化。\n>这时候我们就会想，有没有什么办法能够在服务端序列化Message时，将data字段序列化成json对象而不是字符串呢？\n\n### 自定义序列化\n因为data字段的值本身就是json类型，为什么不能在序列化时直接使用呢？\n查看json包的官方文档，我们可以发现关于 [自定义序列化](https://godoc.org/encoding/json#ex-package--CustomMarshalJSON)的例子\n当执行json序列化时，如果对应的类型实现了`Marshaler`接口：\n```go\ntype Marshaler interface {\n\tMarshalJSON() ([]byte, error)\n}\n```\n那么就会执行其`MarshalJSON`方法，并将返回的字节数组作为该值的序列化值。\n那么回到上面的例子，我们就很容易实现目标：\n```go\ntype JsonString string\n\nfunc (j JsonString) MarshalJSON() ([]byte, error) {\n\tfmt.Println(\"marshal...\")\n\treturn []byte(j), nil\n}\n\ntype Message struct {\n\tFrom string     `json:\"from\"`\n\tTo   string     `json:\"to\"`\n\tData JsonString `json:\"data\"`\n}\n```\n在上面的代码中基于`string`类型声明了`JsonString`，代表json格式的字符串，并实现了Marshaler接口。因为JsonString代表的就是json字符串，直接将其转换成字节数组返回。\n然后将Message中的Data字段换成JsonString类型。\n再次执行程序，可以看到：\n```json\n{\"from\":\"XiaoMing\",\"to\":\"LiGang\",\"data\":{\"title\":\"test\",\"body\":\"something\"}}\n```\n**Perfect!**","tags":["go"]},{"title":"golang中的defer实现","url":"/2018/04/15/golang中的defer实现/","content":"\n\n\n`defer`是go独有的关键字，可以说是go的一大特色。\n\n被`defer`修饰的函数调用，会在函数返回时被执行，因此常常被用于执行锁或者资源释放等。\n\n在每次获得资源时，都紧接`defer`语句对其进行释放，可以防止在后续的操作中忘记释放资源。\n\n在享受其便捷之后，你有没有想过defer机制是如何实现的呢？\n\n首先编写简单的main函数\n\n```go\nfunc main() {\n\tdefer func() {\n\t\tfmt.Println(\"exit\")\n\t}()\n}\n```\n\n使用`go tool compile -N -S main.go > main.s`命令编译查看输出的汇编代码\n\n```assembly\n\"\".main STEXT size=96 args=0x0 locals=0x18\n\tTEXT\t\"\".main(SB), $24-0\n\t...\n\tMOVL\t$0, (SP)\t;deferproc第一个参数0\n\tLEAQ\t\"\".main.func1·f(SB), AX ;匿名函数被编译成main.func1，保存函数地址到AX\n\tMOVQ\tAX, 8(SP) ;deferproc第二个参数为匿名函数地址\n\tPCDATA\t$0, $0\n\tCALL\truntime.deferproc(SB) ;调用defer函数\n\t...\n\tCALL\truntime.deferreturn(SB) ;返回之前执行deferreturn函数\n\tMOVQ\t16(SP), BP\n\tADDQ\t$24, SP\n\tRET\n\t...\n```\n\n根据输出的汇编代码，可以看到defer语句被替换成了调用`runtime.deferproc`方法，查看具体的实现，而在函数返回时执行`runtime.deferreturn`方法\n\n首先分析`runtime.deferproc`方法\n\n```go\n// Create a new deferred function fn with siz bytes of arguments.\n// The compiler turns a defer statement into a call to this.\n//go:nosplit\n//siz表示fn函数的参数总大小\nfunc deferproc(siz int32, fn *funcval) { // arguments of fn follow fn\n    //deferproc不允许在系统栈执行\n\tif getg().m.curg != getg() {\n\t\t// go code on the system stack can't defer\n\t\tthrow(\"defer on system stack\")\n\t}\n\n\t// the arguments of fn are in a perilous state. The stack map\n\t// for deferproc does not describe them. So we can't let garbage\n\t// collection or stack copying trigger until we've copied them out\n\t// to somewhere safe. The memmove below does that.\n\t// Until the copy completes, we can only call nosplit routines.\n\tsp := getcallersp(unsafe.Pointer(&siz))\n    //fn的参数紧跟在fn之后,因此通过简单的指针运算可以获取fn的参数起始地址\n\targp := uintptr(unsafe.Pointer(&fn)) + unsafe.Sizeof(fn)\n    // 获取当前函数的调用者的PC\n\tcallerpc := getcallerpc()\n\t//获取一个_defer\n\td := newdefer(siz)\n\tif d._panic != nil {\n\t\tthrow(\"deferproc: d.panic != nil after newdefer\")\n\t}\n\td.fn = fn\n\td.pc = callerpc\n    // 保存当前的SP\n\td.sp = sp\n\tswitch siz {\n\tcase 0:\n\t\t// Do nothing.\n\tcase sys.PtrSize:\n\t\t*(*uintptr)(deferArgs(d)) = *(*uintptr)(unsafe.Pointer(argp))\n\tdefault:\n        //deferArgs:分配_defer时,连同参数存储空间一起分配,参数紧跟_defer之后存储,该函数进行指针运算,返回参数的起始地址：\n        //拷贝参数,因此在执行defer语句语义之前,需要先准备好接收者和参数\n\t\tmemmove(deferArgs(d), unsafe.Pointer(argp), uintptr(siz))\n\t}\n\n\t// deferproc returns 0 normally.\n\t// a deferred func that stops a panic\n\t// makes the deferproc return 1.\n\t// the code the compiler generates always\n\t// checks the return value and jumps to the\n\t// end of the function if deferproc returns != 0.\n\treturn0()\n\t// No code can go here - the C return register has\n\t// been set and must not be clobbered.\n}\n```\n\n具体的逻辑已经很清楚了，这里要说明的是：`runtime.deferproc`接受两个参数，需要延时执行的函数fn的地址以及fn的参数总大小，而fn的参数需要紧跟着分配在`&fn`后面。\n\n在函数中我们看到了`_defer`这个类型，该类型是实现`defer`机制的关键，其声明如下：\n\n```go\n// A _defer holds an entry on the list of deferred calls.\n// If you add a field here, add code to clear it in freedefer.\ntype _defer struct {\n\tsiz     int32\t//参数size\n\tstarted bool\t//是否执行过\n\tsp      uintptr // sp at time of defer\n\tpc      uintptr\n\tfn      *funcval //需要延时执行的函数地址\n\t_panic  *_panic // panic that is running defer\n\tlink    *_defer //每个goroutine中的_defer以链表组织\n}\n```\n\n在`runtime.newdefer`方法中，会获取一个_defer结构，**并将其加入当前goroutine的` _defer`队列头部**。\n\n接着看一下`runtime.deferreturn`方法实现\n\n```go\n// Run a deferred function if there is one.\n// The compiler inserts a call to this at the end of any\n// function which calls defer.\n// If there is a deferred function, this will call runtime·jmpdefer,\n// which will jump to the deferred function such that it appears\n// to have been called by the caller of deferreturn at the point\n// just before deferreturn was called. The effect is that deferreturn\n// is called again and again until there are no more deferred functions.\n// Cannot split the stack because we reuse the caller's frame to\n// call the deferred function.\n\n// The single argument isn't actually used - it just has its address\n// taken so it can be matched against pending defers.\n//go:nosplit\nfunc deferreturn(arg0 uintptr) { //这边的arg0只是为了获取当前的sp\n\tgp := getg()\n\td := gp._defer\t//获取_defer链表头部\n    //如果没有_defer,则返回,详见上面注释\n\tif d == nil {\n\t\treturn\n\t}\n    // 当前goroutine的所有的_defer通过链表连接\n    // 这里通过比较SP，确保只执行当前函数的_defer\n\tsp := getcallersp(unsafe.Pointer(&arg0))\n\tif d.sp != sp {\n\t\treturn\n\t}\n\n\t// Moving arguments around.\n\t//\n\t// Everything called after this point must be recursively\n\t// nosplit because the garbage collector won't know the form\n\t// of the arguments until the jmpdefer can flip the PC over to\n\t// fn.\n    //拷贝参数到sp中\n\tswitch d.siz {\n\tcase 0:\n\t\t// Do nothing.\n\tcase sys.PtrSize:\n\t\t*(*uintptr)(unsafe.Pointer(&arg0)) = *(*uintptr)(deferArgs(d))\n\tdefault:\n\t\tmemmove(unsafe.Pointer(&arg0), deferArgs(d), uintptr(d.siz))\n\t}\n\tfn := d.fn\n\td.fn = nil\n\tgp._defer = d.link //从链表中移除\n\tfreedefer(d) //释放当前_defer\n    //call runtime·jmpdefer,\n    // which will jump to the deferred function such that it appears\n    // to have been called by the caller of deferreturn at the point\n    // just before deferreturn was called. The effect is that deferreturn\n    // is called again and again until there are no more deferred fns.\n    //执行fn,并修改pc为 `CALL\truntime.deferreturn(SB)`,下一条指令再次进入该函数,如果gp.defer为nil或者sp不一致,则返回,否则继续执行defer\n    //每次添加defer时,总是添加到head,处理时则是从head开始处理,因此defer的处理顺序是FILO\n\tjmpdefer(fn, uintptr(unsafe.Pointer(&arg0)))\n}\n```\n\n至此，defer语句的运行机制分析完成了，主要理了大概的执行流程，其中还有一些细节由于篇幅有限并没有细说，可以自行分析。\n\ngo中还有一个比较独特的地方，如果程序发生异常，会保证先执行所有defer声明的延时函数，然后才退出程序；而我们可以在延时函数中获取到当前整个堆栈的信息，比如说：\n\n```\n函数A执行defer语句，调用函数B\n函数B函数B发生panic\n执行函数A的延时函数，这时候是可以获取到函数B的栈帧数据的\n```\n\n按照上面的执行流程，在执行函数A的延时函数时，实际上这时候函数B的栈帧还没有弹出，神奇吧？这是因为执行panic时，就会去遍历当前goroutine的`_defer`链表，并依次执行这些延时函数，而不是返回函数A之后再执行函数A的延时函数。\n\n实际的执行流程是这样的：\n\n```\n函数A执行defer语句，调用函数B\n函数B函数B发生panic\n在panic内部，遍历_defer链表，并依次执行延时函数\n如果有延时函数执行了recover，则在延时函数返回后，直接跳转到_defer.pc，而不会执行后续的延时函数\n```\n\n```go\n// 内置函数panic的实现\nfunc gopanic(e interface{}) {\n\tgp := getg()  // 当前panic的g\n\t\n    // 在系统栈panic\n    if gp.m.curg != gp {\n\t\tprint(\"panic: \")\n\t\tprintany(e)\n\t\tprint(\"\\n\")\n\t\tthrow(\"panic on system stack\") // throw是不可恢复的，直接终止进程\n\t}\n    \n    // 在内存分配过程中panic\n\tif gp.m.mallocing != 0 {\n\t\tprint(\"panic: \")\n\t\tprintany(e)\n\t\tprint(\"\\n\")\n\t\tthrow(\"panic during malloc\")\n\t}\n    \n\tif gp.m.preemptoff != \"\" {\n\t\tprint(\"panic: \")\n\t\tprintany(e)\n\t\tprint(\"\\n\")\n\t\tprint(\"preempt off reason: \")\n\t\tprint(gp.m.preemptoff)\n\t\tprint(\"\\n\")\n\t\tthrow(\"panic during preemptoff\")\n\t}\n    \n\tif gp.m.locks != 0 {\n\t\tprint(\"panic: \")\n\t\tprintany(e)\n\t\tprint(\"\\n\")\n\t\tthrow(\"panic holding locks\")\n\t}\n\n\tvar p _panic\n\tp.arg = e\n\tp.link = gp._panic\n    // 在defer中可以通过recover获取到该_panic\n\tgp._panic = (*_panic)(noescape(unsafe.Pointer(&p)))\n    // 统计\n\tatomic.Xadd(&runningPanicDefers, 1)\n\n    // 依次执行当前goroutine的_defer\n\tfor {\n\t\td := gp._defer\n\t\tif d == nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// If defer was started by earlier panic or Goexit (and, since we're back here, that triggered a new panic),\n\t\t// take defer off list. The earlier panic or Goexit will not continue running.\n        // defer已经开始执行了，执行defer的时候又触发了panic\n\t\tif d.started {\n            // 如果存在早期的panic\n\t\t\tif d._panic != nil {\n                // 终止原来的panic\n\t\t\t\td._panic.aborted = true\n\t\t\t}\n\t\t\td._panic = nil\n\t\t\td.fn = nil\n\t\t\tgp._defer = d.link\n\t\t\tfreedefer(d)\n            // 继续下一个defer\n\t\t\tcontinue\n\t\t}\n\n\t\t// Mark defer as started, but keep on list, so that traceback\n\t\t// can find and update the defer's argument frame if stack growth\n\t\t// or a garbage collection happens before reflectcall starts executing d.fn.\n\t\t// 标记开始执行\n        d.started = true\n\n\t\t// Record the panic that is running the defer.\n\t\t// If there is a new panic during the deferred call, that panic\n\t\t// will find d in the list and will mark d._panic (this panic) aborted.\n\t\t// 设置defer\n        d._panic = (*_panic)(noescape(unsafe.Pointer(&p)))\n\n\t\tp.argp = unsafe.Pointer(getargp(0))\n        // 调用defer延时的函数\n\t\treflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz))\n\t\tp.argp = nil\n\n\t\t// reflectcall did not panic. Remove d.\n\t\tif gp._defer != d {\n\t\t\tthrow(\"bad defer entry in panic\")\n\t\t}\n\t\td._panic = nil\n\t\td.fn = nil\n\t\tgp._defer = d.link\n\n\t\t// trigger shrinkage to test stack copy. See stack_test.go:TestStackPanic\n\t\t//GC()\n\n\t\tpc := d.pc\n\t\tsp := unsafe.Pointer(d.sp) // must be pointer so it gets adjusted during stack copy\n\t\tfreedefer(d)\n        \n        // 如果在defer中recover了\n\t\tif p.recovered {\n\t\t\tatomic.Xadd(&runningPanicDefers, -1)\n\n\t\t\tgp._panic = p.link\n\t\t\t// Aborted panics are marked but remain on the g.panic list.\n\t\t\t// Remove them from the list.\n            // 移除已经aborted的panic\n\t\t\tfor gp._panic != nil && gp._panic.aborted {\n\t\t\t\tgp._panic = gp._panic.link\n\t\t\t}\n\t\t\tif gp._panic == nil { // must be done with signal\n\t\t\t\tgp.sig = 0\n\t\t\t}\n\t\t\t// Pass information about recovering frame to recovery.\n\t\t\tgp.sigcode0 = uintptr(sp)\n\t\t\tgp.sigcode1 = pc\n\t\t\t// 调用recovery，恢复执行\n            mcall(recovery)\n\t\t\tthrow(\"recovery failed\") // mcall should not return\n\t\t}\n\t}\n\n\t// ran out of deferred calls - old-school panic now\n\t// Because it is unsafe to call arbitrary user code after freezing\n\t// the world, we call preprintpanics to invoke all necessary Error\n\t// and String methods to prepare the panic strings before startpanic.\n\tpreprintpanics(gp._panic)\n\n\tfatalpanic(gp._panic) // should not return\n\t*(*int)(nil) = 0      // not reached\n}\n```\n\n\n\n最后，`defer`函数虽然方便，但是需要有额外的运行开销，在使用时需要进行取舍，尤其是具有多个参数的时候，会发生多次内存拷贝：\n\n```\nruntime.deferproc执行之前：移动到栈中\nruntime.deferproc执行过程中，拷贝_defer之后\nruntime.deferreturn执行时，移动到栈中\n```\n\nupdate：go1.13对defer进行了优化，如果`_defer`没有发生逃逸，则将其分配在栈上，可以提高30%的性能。\n\n\n\n\n\n","tags":["go","defer"]},{"title":"go程序启动过程分析","url":"/2018/01/07/go程序启动过程分析/","content":"\n\n\n事实上，编译好的可执⾏⽂件真正执⾏时并⾮我们所写的 main.main 函数，因为编译器\n\n总是会插⼊⼀段引导代码，完成诸如命令⾏参数、运⾏时初始化等⼯作，然后才会进⼊⽤\n\n户逻辑。 \n\n程序的入口因平台而异：\n\n```sh\nrt0_android_arm.s rt0_dragonfly_amd64.s rt0_linux_amd64.s ...\nrt0_darwin_386.s rt0_freebsd_386.s rt0_linux_arm.s ...\nrt0_darwin_amd64.s rt0_freebsd_amd64.s rt0_linux_arm64.s ...\n```\n\nrt0_linux_amd64.s:\n\n```assembly\nTEXT _rt0_amd64_linux(SB),NOSPLIT,$-8\n   LEAQ   8(SP), SI ; argv\n   MOVQ   0(SP), DI ; argc\n   MOVQ   $main(SB), AX\t\t;move address of main to ax\n   JMP    AX\n   \n   TEXT main(SB),NOSPLIT,$-8\n   MOVQ   $runtime·rt0_go(SB), AX\t;跳转到runtime.rt0.go执行\n   JMP    AX\n```\n\nasm_amd64.s:\n\n```assembly\nTEXT runtime·rt0_go(SB),NOSPLIT,$0\n\t// copy arguments forward on an even stack\n\tMOVQ\tDI, AX\t\t// argc\n\tMOVQ\tSI, BX\t\t// argv\n\tSUBQ\t$(4*8+7), SP\t\t// 2args 2auto\n\tANDQ\t$~15, SP\n\tMOVQ\tAX, 16(SP)\n\tMOVQ\tBX, 24(SP)\n   ..\nok:\n\t; set the per-goroutine and per-mach \"registers\"\n\tget_tls(BX)\n\tLEAQ\truntime·g0(SB), CX\t;将g0的地址保存到CX\n\tMOVQ\tCX, g(BX)\t;设置 g(BX)为g0\n\tLEAQ\truntime·m0(SB), AX\t\n\n\t// save m->g0 = g0\n\tMOVQ\tCX, m_g0(AX)\t;设置m.g0\n\t// save m0 to g0->m\n\tMOVQ\tAX, g_m(CX)\t;设置g.m\n    ...\n\t;调用初始化函数\n\tMOVL\t16(SP), AX\t\t// copy argc\n\tMOVL\tAX, 0(SP)\n\tMOVQ\t24(SP), AX\t\t// copy argv\n\tMOVQ\tAX, 8(SP)\n\tCALL\truntime·args(SB)\t\t;\n\tCALL\truntime·osinit(SB)\t\t;\n\tCALL\truntime·schedinit(SB)\t;\n\n\t// create a new goroutine to start program\n\tMOVQ\t$runtime·mainPC(SB), AX\t\t// entry\n\tPUSHQ\tAX\n\tPUSHQ\t$0\t\t\t// arg size\n\t;创建一个新的goroutine并加入到等待队列，该goroutine执行runtime.mainPC所指向的函数\n\tCALL\truntime·newproc(SB)\n\tPOPQ\tAX\n\tPOPQ\tAX\n\n\t;该函数内部会调用调度程序，从而调度到刚刚创建的goroutine执行\n\tCALL\truntime·mstart(SB)\n\n\tMOVL\t$0xf1, 0xf1  // crash\n\tRET\n\n;声明全局的变量mainPC为runtime.main函数的地址，该变量为read only\nDATA\truntime·mainPC+0(SB)/8,$runtime·main(SB)\t\nGLOBL\truntime·mainPC(SB),RODATA,$8\n```\n\n\n\nruntime1.go:\n\n```go\nfunc args(c int32, v **byte) {\n\targc = c\n\targv = v\n\tsysargs(c, v)\n}\nfunc sysargs(argc int32, argv **byte) {\n}\n```\n\nos_windows.go:\n\n```go\nfunc osinit() {\n    ...\n\tncpu = getproccount()\t//获取cpu核数\n    ...\n}\n```\n\n\n\nproc.go:\n```go\n    // The bootstrap sequence is:\n    //\n    //\tcall osinit\n    //\tcall schedinit\n    //\tmake & queue new G\n    //\tcall runtime·mstart\n    //\n    // The new G calls runtime·main.\n    func schedinit() {\n    \t// raceinit must be the first call to race detector.\n    \t// In particular, it must be done before mallocinit below calls racemapshadow.\n    \t_g_ := getg()\t//获取的是g0\n    \tif raceenabled {\n    \t\t_g_.racectx, raceprocctx0 = raceinit()\n    \t}\n    \t//最大系统线程数量限制\n    \tsched.maxmcount = 10000\n    \n    \ttracebackinit()\n    \tmoduledataverify()\n      \t//栈、内存分配器和调度器的相关初始化\n    \tstackinit()\n    \tmallocinit()\n    \tmcommoninit(_g_.m)\n      \n    \talginit()       // maps must not be used before this call\n    \tmodulesinit()   // provides activeModules\n    \ttypelinksinit() // uses maps, activeModules\n    \titabsinit()     // uses activeModules\n    \n    \tmsigsave(_g_.m)\n    \tinitSigmask = _g_.m.sigmask\n    \n      \t//处理命令行参数和环境变量\n    \tgoargs()\n    \tgoenvs()\n      \t\n      \t//处理 GODEBUG、GOTRACEBACK 调试相关的环境变量设置\n    \tparsedebugvars()\n      \n      \t//垃圾回收器初始化\n    \tgcinit()\n    \n    \tsched.lastpoll = uint64(nanotime())\n      \t//通过 CPU核心数和GOMAXPROCS环境变量确定P的数量，P用于调度g到m上\n    \tprocs := ncpu\n    \tif n, ok := atoi32(gogetenv(\"GOMAXPROCS\")); ok && n > 0 {\n    \t\tprocs = n\n    \t}\n    \tif procs > _MaxGomaxprocs {\n    \t\tprocs = _MaxGomaxprocs\n    \t}\n    \tif procresize(procs) != nil {\n    \t\tthrow(\"unknown runnable goroutine during bootstrap\")\n    \t}\n    \n    \tif buildVersion == \"\" {\n    \t\t// Condition should never trigger. This code just serves\n    \t\t// to ensure runtime·buildVersion is kept in the resulting binary.\n    \t\tbuildVersion = \"unknown\"\n    \t}\n    }\n```\n\n```go\n    // Called to start an M.\n    //go:nosplit\n    func mstart() {\n    \t....\n    \tmstart1()\n    }\n```\n```go\n    func mstart1() {\n         ...\n      \t//调度goroutine\n    \tschedule()\n    }\n\n```\n```go\n// go程序编译时，会在main包生成init函数，该函数内调用所有依赖的包的init函数，如果同一个包被程序重复引入多次，他的init函数只会执行一次\n// 当编译时，链接器会将main.init链接到main_init\n//go:linkname main_init main.init\nfunc main_init()\n// 编译时，链接器会将用户的main.main函数链接到main_main\n//go:linkname main_main main.main\nfunc main_main()\n\n// The main goroutine.\nfunc main() {\n\tg := getg()\t//当前获取的g是刚刚在rt0_go内创建的goroutine\n\n\t// Racectx of m0->g0 is used only as the parent of the main goroutine.\n\t// It must not be used for anything else.\n\tg.m.g0.racectx = 0\n\n\t// Max stack size is 1 GB on 64-bit, 250 MB on 32-bit.\n\t// Using decimal instead of binary GB and MB because\n\t// they look nicer in the stack overflow failure message.\n  \t//执行栈最大限制：1GB on 64-bit，250MB on 32-bit\n\tif sys.PtrSize == 8 {\t//64-bit下指针长度是8个字节\n\t\tmaxstacksize = 1000000000\n\t} else {\n\t\tmaxstacksize = 250000000\n\t}\n\n\t// Allow newproc to start new Ms.\n\tmainStarted = true\n\n  \t//启动系统后台监控（定期垃圾回收以及并发任务的调度等）\n\tsystemstack(func() {\n\t\tnewm(sysmon, nil)\n\t})\n\n\t// Lock the main goroutine onto this, the main OS thread,\n\t// during initialization. Most programs won't care, but a few\n\t// do require certain calls to be made by the main thread.\n\t// Those can arrange for main.main to run in the main thread\n\t// by calling runtime.LockOSThread during initialization\n\t// to preserve the lock.\n\tlockOSThread()\n\n\tif g.m != &m0 {\n\t\tthrow(\"runtime.main not on m0\")\n\t}\n\n  \t//执行runtime包内的所有初始化函数 init\n\truntime_init() // must be before defer\n\tif nanotime() == 0 {\n\t\tthrow(\"nanotime returning zero\")\n\t}\n\n\t// Defer unlock so that runtime.Goexit during init does the unlock too.\n\tneedUnlock := true\n\tdefer func() {\n\t\tif needUnlock {\n\t\t\tunlockOSThread()\n\t\t}\n\t}()\n\n\t// Record when the world started. Must be after runtime_init\n\t// because nanotime on some platforms depends on startNano.\n\truntimeInitTime = nanotime()\n\n  \t//启动垃圾回收器的后台操作\n\tgcenable()\n\n\tmain_init_done = make(chan bool)\n\n  \t//执行用户包（包括标准库）的初始化函数 init，程序所有的包的init函数都会在这个函数内被全部执行\n    // 因为main_init是在编译时进行链接的，因此这里使用间接调用\n\tfn := main_init // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime\n\tfn()\n\tclose(main_init_done\n\tneedUnlock = false\n\tunlockOSThread()\n\n  \t//执行用户逻辑入口 main.main 函数\n\tfn = main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime\n\tfn()\n   ...\n  \t//执行结束，程序正常退出\n\texit(0)\n}\n```\n\n\n### 总结\n\n• 所有 init 函数都在同⼀个 goroutine 内执⾏\n\n• 所有 init 函数结束后才会执⾏ main.main 函数 \n\n### 参考\n\n- 雨痕的 Go 1.5源码剖析","tags":["go"]},{"title":"libtask分析","url":"/2017/12/29/libtask分析/","content":"\n\n\n`libtask` 是一个开源的 `C` 语言协程库。\n\n`C` 语言协程可以通过更改寄存器，切换协程上下文实现协程调度。\n\n更改寄存器可以通过 `C` 语言内联汇编实现，通过汇编代码直接更改寄存器的内容。\n\n也可以使用 `ucontext` 配合 `getContext` 、 `setContext` 、`makeContext` 、`swapContext` 函数来实现。\n\n`ucontext` 结构封装了寄存器信息和栈信息，是协程执行的上下文，而其他四个函数分别用于获取当前执行的上下文，设置当前上下文，创建上下文和交换上下文，这些函数已经封装了对寄存器内容的交换工作。\n\n##### Task\n\n一个Task可以看成是一个需要异步执行的任务，coroutine的抽象描述。\n\n```c\ntypedef struct Context Context;\t\nstruct Context\n{\n\tucontext_t\tuc;\t//ucontext封装了协程执行的上下文信息\n};\nstruct Task\n{\n\tchar\tname[256];\t// offset known to acid\n\tchar\tstate[256];\n\tTask\t*next;\n\tTask\t*prev;\n\tTask\t*allnext;\n\tTask\t*allprev;\n\tContext\tcontext;\t//协程上下文\n\tuvlong\talarmtime;\n\tuint\tid;\n\tuchar\t*stk;\t//协程栈指针\n\tuint\tstksize;\t//栈大小\n\tint\texiting;\n\tint\talltaskslot;\t//在全局task数组内的index\n\tint\tsystem;\n\tint\tready;\n\tvoid\t(*startfn)(void*);\t//Task需要执行的函数\n\tvoid\t*startarg;\t//startfn 的参数\n\tvoid\t*udata;\n};\n```\n\n##### Task创建\n\n```c\nint taskcreate(void (*fn)(void*), void *arg, uint stack){\n\tint id;\n\tTask *t;\n\n\tt = taskalloc(fn, arg, stack);\t//分配task和stack的空间\n\ttaskcount++;\t\n\tid = t->id;\n  //判断数组是否还有足够空间\n\tif(nalltask%64 == 0){\n      //扩展数组\n\t\talltask = realloc(alltask, (nalltask+64)*sizeof(alltask[0]));\n\t\tif(alltask == nil){\n\t\t\tfprint(2, \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t}\n  \t//保存Task在alltask数组内的index\n\tt->alltaskslot = nalltask;\n\talltask[nalltask++] = t;\t//保存task到alltask数组\n\ttaskready(t);\t//设置为ready，可以被调度执行\n\treturn id;\n}\n\n//taskalloc分配task和stack的空间\nstatic Task* taskalloc(void (*fn)(void*), void *arg, uint stack){\n\tTask *t;\n\tsigset_t zero;\n\tuint x, y;\n\tulong z;\n\n\t/* allocate the task and stack together */\n\tt = malloc(sizeof *t+stack);\t//分配内存，stack紧跟在task之后\n\tif(t == nil){\n\t\tfprint(2, \"taskalloc malloc: %r\\n\");\n\t\tabort();\n\t}\n  \t//清除task的内存\n\tmemset(t, 0, sizeof *t);\n\tt->stk = (uchar*)(t+1);\t//设置stack指针，stack紧跟task之后，t+1指针偏移一个Task大小\n\tt->stksize = stack;\t//设置stack大小\n\tt->id = ++taskidgen;\t//设置id\n\tt->startfn = fn;\t//设置任务需要执行的函数\n\tt->startarg = arg;\t//startfn的参数\n\n\t/* do a reasonable initialization */\n\tmemset(&t->context.uc, 0, sizeof t->context.uc);\n\tsigemptyset(&zero);\n\tsigprocmask(SIG_BLOCK, &zero, &t->context.uc.uc_sigmask);\n\n\t/* must initialize with current context */\n\tif(getcontext(&t->context.uc) < 0){\t//获取当前ucontext，并保存到t->context.uc\n\t\tfprint(2, \"getcontext: %r\\n\");\n\t\tabort();\n\t}\n\n\t/* call makecontext to do the real work. */\n\t/* leave a few words open on both ends */\n  \t//设置栈顶指针和栈大小，两端都保留一点空间\n\tt->context.uc.uc_stack.ss_sp = t->stk+8;\t\n\tt->context.uc.uc_stack.ss_size = t->stksize-64;\n#if defined(__sun__) && !defined(__MAKECONTEXT_V2_SOURCE)\t\t/* sigh */\n#warning \"doing sun thing\"\n\t/* can avoid this with __MAKECONTEXT_V2_SOURCE but only on SunOS 5.9 */\n\tt->context.uc.uc_stack.ss_sp = \n\t\t(char*)t->context.uc.uc_stack.ss_sp\n\t\t+t->context.uc.uc_stack.ss_size;\n#endif\n\t/*\n\t * All this magic is because you have to pass makecontext a\n\t * function that takes some number of word-sized variables,\n\t * and on 64-bit machines pointers are bigger than words.\n\t */\n//print(\"make %p\\n\", t);\n  //计算startfn的参数:y,x\n  /**\n  taskstart的参数是uint，即32位，而指针如果是64位，则需要将指针的高32位和低32位分离，分别传递\n  将指针分离为高32位(x)和低32位(y)，在taskstart内再通过两个参数合成task指针\n  该方法可以同时适用于32位和64位的编译器\n  **/\n\tz = (ulong)t;\n\ty = z;\n\tz >>= 16;\t/* hide undefined 32-bit shift from 32-bit compilers */\n\tx = z>>16;\n  //这里传入的是taskstart函数，在该函数内执调用t->startfn，并传入t->startarg\n\tmakecontext(&t->context.uc, (void(*)())taskstart, 2, y, x);\n\n\treturn t;\n}\n\n/**\n初始化uc_context，set the context of coroutine\n**/\n#ifdef NEEDAMD64MAKECONTEXT\nvoid\nmakecontext(ucontext_t *ucp, void (*func)(void), int argc, ...)\n{\n\tlong *sp;\n\tva_list va;\t//用于遍历可变长参数的指针\n\n\tmemset(&ucp->uc_mcontext, 0, sizeof ucp->uc_mcontext);\n\tif(argc != 2)\n\t\t*(int*)0 = 0;\t//报错\n\tva_start(va, argc);\t//遍历可变参数\n\t//前6个参数可以使用寄存器（%rdi，%rsi，%rdx，%rcx，%r8，%r9）保存，后面参数入栈\n\t//用于传递函数参数，rdi：第一个参数，rsi：第二个参数；调用func时传入rdi和rsi\n\tucp->uc_mcontext.mc_rdi = va_arg(va, int);\n\tucp->uc_mcontext.mc_rsi = va_arg(va, int);\n\tva_end(va);\n\t/**设置栈指针**/\n\tsp = (long*)ucp->uc_stack.ss_sp+ucp->uc_stack.ss_size/sizeof(long);\t//移动sp指针\n\tsp -= argc;\t\n\tsp = (void*)((uintptr_t)sp - (uintptr_t)sp%16);\t/* 16-align for OS X */ //地址对齐\n\t*--sp = 0;\t/* return address */\n\tucp->uc_mcontext.mc_rip = (long)func;\t//ip，rip存放下一条指令地址\n\tucp->uc_mcontext.mc_rsp = (long)sp;\t//栈顶指针\n}\n#endif\n\nstatic void\ntaskstart(uint y, uint x)\n{\n\tTask *t;\n\tulong z;\n\t// t = (x<<32)|y\n\tz = x<<16;\t/* hide undefined 32-bit shift from 32-bit compilers */\n\tz <<= 16;\n\tz |= y;\n\tt = (Task*)z;\t//获取task地址\n\n//print(\"taskstart %p\\n\", t);\n\tt->startfn(t->startarg);\t//调用startfn\n//print(\"taskexits %p\\n\", t);\n\ttaskexit(0);\t//startfn结束，设置退出标志位\n//print(\"not reacehd\\n\");\n}\n```\n\n### Task调度\n\n```c\ntypedef struct Tasklist Tasklist;\nstruct Tasklist\t/* used internally */\n{\n\tTask\t*head;\n\tTask\t*tail;\n};\n```\n\n```c\nContext\ttaskschedcontext;\t\nTasklist\ttaskrunqueue;\nTask\t*taskrunning;\n\nvoid\nneedstack(int n)\n{\n\tTask *t;\n\n\tt = taskrunning;\n\n\tif((char*)&t <= (char*)t->stk\n\t|| (char*)&t - (char*)t->stk < 256+n){\n\t\tfprint(2, \"task stack overflow: &t=%p tstk=%p n=%d\\n\", &t, t->stk, 256+n);\n\t\tabort();\n\t}\n}\n\nvoid\ntaskswitch(void)\n{\n\tneedstack(0);\n\tcontextswitch(&taskrunning->context, &taskschedcontext);\n}\n\nint\nswapcontext(ucontext_t *oucp, const ucontext_t *ucp)\n{\n\tif(getcontext(oucp) == 0)\t//get the context into *oucp\n\t\tsetcontext(ucp);\t//set the context as *ucp\n\treturn 0;\n}\n\nvoid\ntaskready(Task *t)\n{\n\tt->ready = 1;\n\taddtask(&taskrunqueue, t);\t//加入调度队列\n}\n\nint\ntaskyield(void)\n{\n\tint n;\n\t\n\tn = tasknswitch;\n\ttaskready(taskrunning);\t//将当前task加入等待队列\n\ttaskstate(\"yield\");\n\ttaskswitch();\t//切换\n\treturn tasknswitch - n - 1;\n}\n\nint\nanyready(void)\n{\n\treturn taskrunqueue.head != nil;\t//判断等待队列队首是否为空\n}\n\nvoid\ntaskexit(int val)\n{\n\ttaskexitval = val;\n\ttaskrunning->exiting = 1;\n\ttaskswitch();\t//切换上下文，执行调度程序\n}\n\n\nstatic void\ntaskscheduler(void)\n{\n\tint i;\n\tTask *t;\n\n\ttaskdebug(\"scheduler enter\");\n\tfor(;;){\n\t\tif(taskcount == 0)\n\t\t\texit(taskexitval);\n\t\tt = taskrunqueue.head;\n\t\tif(t == nil){\n\t\t\tfprint(2, \"no runnable tasks! %d tasks stalled\\n\", taskcount);\n\t\t\texit(1);\n\t\t}\n\t\tdeltask(&taskrunqueue, t);\n\t\tt->ready = 0;\n\t\ttaskrunning = t;\t//设置taskrunning\n\t\ttasknswitch++;\n\t\ttaskdebug(\"run %d (%s)\", t->id, t->name);\n      //当前context保存到taskschedcontext，即taskschedcontext为调度上下文\n\t\tcontextswitch(&taskschedcontext, &t->context);\t//交换 task context\n//print(\"back in scheduler\\n\");\n\t\ttaskrunning = nil;\n\t\tif(t->exiting){\t//如果退出\n\t\t\tif(!t->system)\n\t\t\t\ttaskcount--;\n\t\t\ti = t->alltaskslot;\n\t\t\talltask[i] = alltask[--nalltask];\t//替换为全局数组的最后一个task\n\t\t\talltask[i]->alltaskslot = i;\n\t\t\tfree(t);\t//释放task\n\t\t}\n\t}\n}\n\n/*\n * startup\n */\n\nstatic int taskargc;\nstatic char **taskargv;\nint mainstacksize;\n\nstatic void\ntaskmainstart(void *v)\n{\n\ttaskname(\"taskmain\");\n\ttaskmain(taskargc, taskargv);\n}\n\nint\nmain(int argc, char **argv)\n{\n\tstruct sigaction sa, osa;\n\n\tmemset(&sa, 0, sizeof sa);\n\tsa.sa_handler = taskinfo;\n\tsa.sa_flags = SA_RESTART;\n\tsigaction(SIGQUIT, &sa, &osa);\n\n#ifdef SIGINFO\n\tsigaction(SIGINFO, &sa, &osa);\n#endif\n\n\targv0 = argv[0];\n\ttaskargc = argc;\n\ttaskargv = argv;\n\n\tif(mainstacksize == 0)\n\t\tmainstacksize = 256*1024;\n\ttaskcreate(taskmainstart, nil, mainstacksize);\n\ttaskscheduler();\n\tfprint(2, \"taskscheduler returned in main!\\n\");\n\tabort();\n\treturn 0;\n}\n```\n","tags":["libtask"]},{"title":"kotlin函数初级入门","url":"/2017/12/11/kotlin函数初级入门/","content":"\n\n\n### 普通函数声明\n使用 `func` 关键字声明一个函数，像这样\n\n```kotlin\nfun add(a:Int,b:Int):Int{\n    return a+b\n}\n```\n**在`kotlin`中，所有函数的参数都是`val`的，即不可变参数**\n如果函数体只有一行代码，可以简洁点：\n```kotlin\nfun add(a: Int, b: Int): Int = a + b\n```\n更简单点，返回值自动推断：\n```kotlin\nfun add(a: Int, b: Int) = a + b\n```\n###带默认值的函数\n可以在声明函数参数的时候，直接指定默认值，如果调用时没有传入，将使用默认值，带有默认值的参数可以在任何位置\n```kotlin\nfun add(a: Int=1, b: Int) = a + b\n```\n调用的时候，可以使用`参数名=值`的形式给出参数\n```kotlin\nadd(b=1)\n```\n换个位置声明默认值：\n```kotlin\nfun add(a: Int , b: Int=1) = a + b\nfun adds (a:Int,b:Int=1,c:Int)= a+b+c\nfun main(vararg args:String){\n    add(1)    //自动匹配第一个参数\n    adds (1,c=2)    //默认值之后的参数需要显示指出参数名\n}\n```\n函数匹配优先级：\n```kotlin\nfun add(a:Int) = a*a\nfun add(a: Int , b: Int=1) = a + b\nfun main(vararg args:String){\n   println(add(3))  //输出 9 \n}\n```\n当有多个函数匹配时，带默认值参数个数少的优先级越高，不带默认值的优先级最高\n\n### 可变参数\n在`kotlin`中，使用 `vararg` 关键字来标识可变参数\n和`java`一样，多个参数会被封装成数组赋值给a\n\n```kotlin\nfun add(vararg a: Int, b: Int):Int{\n    var sum :Int = 0\n    a.forEach { \n        sum+=it\n    }\n    return sum+b\n}\n```\n和`java`不同的是，在java中我们可以直接将一个数组赋值给可变参数，这有时候会引起混淆，因此在kotlin中，我们需要显示使用运算符`*`将数组解构成元素\n比如我们可以这样调用上面的方法：\n```kotlin\nfun main(vararg args: String) {\n    var arr = IntArray(10) { it }\n    add(*arr, b = 1)\n}\n```\n注意，第二个参数我们需要明确指出他的参数名，否则他会被当作可变参数中的一个值\n\n### 使用lambda\n`lambda`是一种特殊的函数。和传统函数不同的是，他可以被存储，传递，并且可以捕获外部变量形成闭包。\n#####`lambda`的类型\n因为`lambda`本质上还是对象，因此他是有类型的。\n `lambda`的类型格式为：\n\n```kotlin\n(参数列表)->返回值类型\n```\n`lambda`的`body`结构为：\n```kotlin\n{形参列表->\n  语句\n  ...\n  最后一个语句的值为返回值（如果需要返回值）\n}\n```\n比如：\n```kotlin\nval max:(Int,Int)->Int = {a,b -> if (a>b) a else b}\n```\n`max`用于比较两个整数，并返回他们中的最大者。因为这边接收了两个参数，因此我们需要在`lambda`的`body`中显示的为这两个参数指定一个名字，就像我们需要为函数指定形参名。\n上面的例子也可以这样写：\n```kotlin\nval max =  { a:Int,b:Int-> if (a>b) a else b}  as (Int,Int)->Unit\n```\n不过这是比较2b的写法了，使用`as`对`lambda`进行类型转换，然后`max`的类型自动推出，这里我只是想说明`lambda`本质上还是个对象，因此一样可以进行类型转换。\n\n### 高阶函数\n`lambda`可以作为函数的参数或者返回值，举个例子：\n\n```kotlin\nfun <T> forEach(list:List<T>,block:(t:T)->Unit){\n    for (t in list){\n        block(t)\n    }\n}\n\nfun main(vararg args:String){\n    var list = listOf(1,2,3,4,5)\n    forEach(list){\n        println(it)\n    }\n}\n```\n在上面的例子中，我们声明了一个`forEach`的函数，他的功能是遍历一个列表，并对列表中的每个元素调用指定的`lambda`，然后我们在`main`函数中调用它。\n值得注意的是，这里当我们的`lambda`是函数的最后一个参数时，我们可以将其写在`()`外面，当函数参数只有一个`lambda`时，可以直接省略`()`；\n还有第二个要注意的是，我们使用了匿名参数，当`lambda`只有一个参数时，我们可以不用显示的指定一个参数名，而是使用默认的`it`来引用。\n\n### 扩展函数\n在`kotlin`中，我们可以很方便的扩展某个类，为其增加方法：\n\n```kotlin\nfun Int.compareWith(a: Int): Int {\n    return when {\n        this > a -> 1\n        this < a -> -1\n        else -> 0\n    }\n}\n\nfun main(vararg args: String) {\n    println(10.compareWith(4))\n}\n```\n如上所示，声明一个扩展函数的语法很简单，只需要在方法名前面加上`类名.`，在方法中我们可以使用`this`来引用他，但是只能访问`public`的成员。这个`类名`我们使用`receiver`来描述它。\n扩展函数只是`kotlin`中众多语法糖中的一个，他并没有真正的扩展这个类，只是将方法的一个参数提到了方法名前面作为`receiver`：\n```kotlin\nfun Int.compareWith(a:Int):Int\n===>\nfun compareWith(this:Int,a:Int):Int  \n```\n所以调用扩展函数和普通的函数调用没有区别，函数的`receiver`本质上还是这个函数的参数，而不是这个方法的所有者，因此在调用时使用的是静态分派，并不支持多态。\n而且当扩展函数与类的方法冲突时，默认使用的是类的方法。\n\n##### 结合泛型的扩展函数\n\n```kotlin\nfun <T:Any> T?.toStr(){\n    println(this?.toString()?:\"this is a null ref\")\n}\nfun main(vararg args:String){\n    null.toStr()\n}\n```\n正如上面的例子中所看到的，我们可以使用泛型参数作为函数的`receiver`，而且我们使用了`T?`，说明支持`null`\n\n##### 函数参数使用扩展函数\n对刚刚的`forEach`函数稍加改造：\n\n```kotlin\nfun <T> forEach(list:List<T>,block:T.()->Unit){\n    for (t in list){\n       t.block()\n    }\n}\n\nfun main(vararg args:String){\n    var list = listOf(1,2,3,4,5)\n    forEach(list){\n        println(this)\n    }\n}\n```\n注意在声明函数参数时，我们使用了`T.()->Unit`，也就是声明了一个具有`receiver`的`lambda`\n\n ","tags":["kotlin"]},{"title":"使用kotlin自定义生成器","url":"/2017/12/02/使用kotlin自定义生成器/","content":"\n使用kotlin的coroutine机制，可以很容易实现一个generator\n\n```kotlin\nimport java.util.concurrent.atomic.AtomicReference\nimport kotlin.coroutines.experimental.*\n\n\nclass Generater<T : Any> private constructor() {\n    private var mContinuation: AtomicReference<Continuation<Unit>?> = AtomicReference(null)\n    private val values: ThreadLocal<T?> = ThreadLocal()\n    /**\n     * -1:结束\n     *  0:未开始\n     *  1:开始\n     */\n    @Volatile\n    private var status: Int = 0\n\n    companion object {\n        fun <T : Any> build(block: suspend Generater<T>.() -> Unit): Generater<T> {\n            val g = Generater<T>()\n            var c = object : Continuation<Unit> {\n                override val context: CoroutineContext\n                    get() = EmptyCoroutineContext\n\n                override fun resume(value: Unit) {\n                    g.status = -1\n                }\n\n                override fun resumeWithException(exception: Throwable) {\n                    g.status = -1\n                    throw exception\n                }\n            }\n            g.mContinuation.compareAndSet(null, block.createCoroutine(g, c))\n            g.status = 1\n            return g\n        }\n    }\n\n    suspend fun yield(t: T?) {\n        suspendCoroutine<Unit> {\n            values.set(t)\n            mContinuation.compareAndSet(null, it)\n            //Thread.sleep(100)\n        }\n    }\n\n    fun next(): T? {\n        while (true) {\n            if (status == -1) {\n                values.set(null)\n                break\n            }\n          \t//可以提到循环外面\n            if (status == 0) {\n                throw IllegalStateException(\"生成器未启动\")\n            }\n            val c = mContinuation.getAndSet(null)\n            c ?: continue\n\n            synchronized(this) {\n                c.resume(Unit)\n            }\n            break\n        }\n        return values.get()\n    }\n\n}\n```\n\n使用\n\n```kotlin\n\nfun main(vararg args: String) {\n\t//声明生成器\n    var g = Generater.build {\n        yield(0L)\n        var i = 0L\n        var j = 1L\n        while (true) {\n            yield(j)\n            var next = i + j\n            i = j\n            j = next\n        }\n    }\n\t//多线程访问\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n\n    Thread {\n        for (i in 0..2)\n            println(Thread.currentThread().name + \":\" + g.next())\n    }.start()\n}\n```\n\n","tags":["kotlin","coroutine","generator"]},{"title":"使用kotlin协程机制撸一个简易的异步执行库","url":"/2017/11/10/使用kotlin协程机制撸一个简易的异步执行库/","content":"\n\n\n> 由于android限制了只能在UI线程更新视图，而在UI线程中做耗时任务又会导致ANR，因此在平时的开发中，需要将耗时的数据请求工作放到子线程中执行，而视图更新工作放到UI线程中，使用传统的handler或者asyncTask，需要将逻辑分到多个函数内\n\n**使用`kotlin`的协程机制，可以用同步的方式实现异步**\nkotlin的协程机制是基于状态机模型和`C-P-S`风格实现的。\n一个协程通过`resume`启动，当协程内部调用`supended`函数时，协程会被暂停，通过调用 `resume`可以再次启动协程。每次暂停都会修改协程的状态，再次启动协程时，会从新的状态处开始执行。\n\n现在通过kotlin的基础api实现一个简单的异步调用接口，最后的效果如下：\n\n ```\n btn.setOnClickListener {\n            runOnUI {  \n                //执行在主线程，可以做一些初始化操作                         \n                Log.e(\"log\", Thread.currentThread().name)\n                var used = async {               //从工作线程直接返回数据到主线程\n                   //切换到工作线程执行，而且lambda可以直接访问外部变量，构成闭包\n                    Log.e(\"log\", Thread.currentThread().name)\n                    var start = System.currentTimeMillis()\n                    Thread.sleep(3000)\n                    System.currentTimeMillis() - start\n                }\n                //继续执行在主线程\n                Log.e(\"log\", Thread.currentThread().name)\n                Toast.makeText(this@MainActivity, \"后台线程用时${used}ms\", Toast.LENGTH_SHORT).show()\n            }\n        }\n ```\n>在后续的内容中，我将在实现的过程中逐步分析kotlin协程机制的基本原理\n\n首先声明一个创建协程的函数：\n```\n//该函数接收一个 suspend类型的lambda\ninline fun runOnUI(noinline block: suspend () -> Unit) {\n    var continuation = object : Continuation<Unit> {\n      //ThreadSwitcher是ContinuationInterceptor的子类，用于在协程resume时切换到主线程执行\n        override val context: CoroutineContext\n            get() = ThreadSwitcher()  \n\n        override inline fun resume(value: Unit) = Unit\n\n        override inline fun resumeWithException(exception: Throwable) = Unit\n    }\n        //使用suspend类型的lambda创建一个协程并启动\n        block.createCoroutine(continuation).resume(Unit)\n}\n```\n`createCoroutine`是官方提供的一个基础api，该函数如下：\n```\npublic fun <T> (suspend () -> T).createCoroutine(\n        completion: Continuation<T>\n): Continuation<Unit> = SafeContinuation(createCoroutineUnchecked(completion), COROUTINE_SUSPENDED)\n```\n可以看到调用了`createCoroutineUnchecked`创建一个`Coroutine`，继续查看该方法：\n```\n@SinceKotlin(\"1.1\")\n@kotlin.jvm.JvmVersion\npublic fun <T> (suspend () -> T).createCoroutineUnchecked(\n        completion: Continuation<T>\n): Continuation<Unit> =\n//这里的this是执行createCoroutine函数的block\n        if (this !is kotlin.coroutines.experimental.jvm.internal.CoroutineImpl)\n            buildContinuationByInvokeCall(completion) {\n                @Suppress(\"UNCHECKED_CAST\")\n                (this as Function1<Continuation<T>, Any?>).invoke(completion)\n            }\n        else\n//编译时，block会被编译成一个CoroutineImpl的子类，所以走这个分支\n            (this.create(completion) as kotlin.coroutines.experimental.jvm.internal.CoroutineImpl).facade\n```\n查看编译之后生成的`block`：\n```\n//查看在Activity#onCreate调用runOnUI处传入的lambda的编译类\nfinal class ymc/demo/com/asyncframe/MainActivity$onCreate$1$1 \n          extends kotlin/coroutines/experimental/jvm/internal/CoroutineImpl   \n          implements kotlin/jvm/functions/Function1  {      //lambda编译类都实现FunctionN函数\n  ...\n}\n```\n可以看到传入`runOnUI`的`lambda`确实被编译成了一个`CoroutineImpl`，这是因为编译器推断出了这个`lambda`是`suspend`类型的。\n\n继续上面的分析，创建协程所涉及到的两个方法中都出现了 `Continuation`这个类，那么这个类是干嘛的呢？\n首先，先看看`completion`，这个是我们调用`createCoroutine`手动传入的，当协程结束时，他的`resume`会被调用，当协程异常结束时，他的`resumeWithException`会被调用。\n再看看`createCoroutineUnchecked`，这个函数也返回了一个`Continuation`，那么这个又是什么呢？\n```\n (this.create(completion) as kotlin.coroutines.experimental.jvm.internal.CoroutineImpl).facade\n```\n可以看到，返回的是`CoroutineImpl`的`facade`，那这个又是什么呢？\n我们进入`CoroutineImpl`，可以看到\n```\nabstract class CoroutineImpl(\n        arity: Int,\n        @JvmField\n        protected var completion: Continuation<Any?>?\n) : Lambda(arity), Continuation<Any?> {     //Coroutine本身是一个Continuation\n\n  override val context: CoroutineContext\n          get() = _context!!\n\n  private var _facade: Continuation<Any?>? = null\n \n  val facade: Continuation<Any?> get() {\n          if (_facade == null) _facade = interceptContinuationIfNeeded(_context!!, this)\n          return _facade!!\n      }\n  ...\n}\n```\n原来这是一个代理属性，接着查看`interceptContinuationIfNeeded`，\n```\ninternal fun <T> interceptContinuationIfNeeded(\n        context: CoroutineContext,\n        continuation: Continuation<T>\n) = context[ContinuationInterceptor]?.interceptContinuation(continuation) ?: continuation\n```\n这个函数从`Coroutine`的上下文中查找`ContinuationInterceptor`，如果有就调用他的`interceptContinuation`对传入的`continuation`进行包装，否则直接返回传入的`continuation`\n\n**`Continuation`**是一个可**继续执行**体的抽象，每个`Coroutine`都是一个可继续执行体，`Continuation`是一个协程对外的接口，启动/恢复协程的`resume`就是在该接口中定义的。\n协程可以是链式连接的，一个协程可以有子协程，子协程持有父协程的引用，当子协程执行时，父协程暂停，子协程结束时，内部通过调用父协程的`resume`返回父协程。\n\n还记得我们前面用到的`ThreadSwitcher`吗，他就是一个`ContinuationInterceptor`\n我们来看看来看`ThreadSwitcher`的实现：\n```\n/**\nInterceptor用于用于拦截并包装Continuation，让我们有机会在协程resume前做一些额外的操作，比如线程切换\n**/\nclass ThreadSwitcher : ContinuationInterceptor, AbstractCoroutineContextElement(ContinuationInterceptor.Key) {\n\n    override fun <T> interceptContinuation(continuation: Continuation<T>): Continuation<T>\n            = object : Continuation<T> by continuation {\n\n        override fun resume(value: T) {\n          //如果在主线程，直接执行\n            if (Looper.getMainLooper() === Looper.myLooper()) {\n                continuation.resume(value)\n            } else {\n            //否则，使用handler机制post到主线程执行\n                postman.post {\n                    resume(value)\n                }\n            }\n        }\n\n        override fun resumeWithException(exception: Throwable) {\n            if (Looper.getMainLooper() === Looper.myLooper()) {\n                continuation.resumeWithException(exception)\n            } else {\n                postman.post {\n                    resumeWithException(exception)\n                }\n            }\n        }\n    }\n}\n```\n从上面的分析中，我们可以想象，我们创建的协程会被`ThreadSwitcher`包装，\n```\nblock.createCoroutine(continuation).resume(Unit)\n```\n`createCoroutine`返回的实际是`ThreadSwitcher`返回的`Continuation`，所以当我们执行`resume`启动协程时，会先切换到主线程执行。\n\n紧接着，我们来实现`async`：\n```\nsuspend inline fun <T> async(crossinline block: () -> T): T\n        = suspendCoroutine {\n//dispatcher是一个对线程池的封装，将任务分发到子线程中\n    dispatcher.dispatch {\n        it.resume(block())\n    }\n}\n```\n使用`suspend`修饰的方法只可以在协程内部调用，而`suspendCoroutine`方法是`kotlin`提供的一个基础api,用于实现暂停协程。\n我们接着来分析`suspendCoroutine`，查看他的实现：\n```\npublic inline suspend fun <T> suspendCoroutine(crossinline block: (Continuation<T>) -> Unit): T =\n        suspendCoroutineOrReturn { c: Continuation<T> ->\n            val safe = SafeContinuation(c)\n            block(safe)\n            safe.getResult()\n        }\n```\n可以看到这个方法接收的`block`是带`Continuation`参数的\n真正实现功能的是`suspendCoroutineOrReturn`，当我们继续跟进时，发现：\n```\npublic inline suspend fun <T> suspendCoroutineOrReturn(crossinline block: (Continuation<T>) -> Any?): T =\n        throw NotImplementedError(\"Implementation is intrinsic\")\n```\nwhat!直接抛出异常了???\n这是因为这是一个特殊的函数，需要编译器特殊处理，他需要将当前协程内的`_facade`属性，包装成`SafeContinuation`，再作为我们传入的`block`的参数，而且这个`_facade`是经过`ContinuationInterceptor`处理过的，也就是说当我们调用`resume`恢复线程时，会先切换到主线程。\n为了验证上面的分析，我们查看`async`编译之后的字节码：\n```\n//可以看到编译之后，我们的async多了一个Continuation类型的参数\n private final static async(Lkotlin/jvm/functions/Function0;Lkotlin/coroutines/experimental/Continuation;)Ljava/lang/Object;\n   L0\n    LINENUMBER 70 L0\n    NOP\n   L1\n    LINENUMBER 77 L1\n    ICONST_0\n    INVOKESTATIC kotlin/jvm/internal/InlineMarker.mark (I)V\n    ALOAD 1  //将第二个参数，也就是Continuation入栈\n//调用CoroutineIntrinsics.normalizeContinuation \n    INVOKESTATIC kotlin/coroutines/experimental/jvm/internal/CoroutineIntrinsics.normalizeContinuation (Lkotlin/coroutines/experimental/Continuation;)Lkotlin/coroutines/experimental/Continuation;  \n//将返回值存到slot3\n    ASTORE 3\n   L2\n    LINENUMBER 78 L2\n//new 一个SafeContinuation\n    NEW kotlin/coroutines/experimental/SafeContinuation\n    DUP  \n  //将刚刚normalizeContinuation返回的continuation传入SafeContinuation的构造函数\n    ALOAD 3\n    INVOKESPECIAL kotlin/coroutines/experimental/SafeContinuation.<init> (Lkotlin/coroutines/experimental/Continuation;)V\n    ASTORE 4\n   L3\n  ...\n```\n我们可以看到，编译之后的字节码已经没有了`suspendCoroutine`和`suspendCoroutineOrReturn`的身影，因为这两个函数都是`inline`函数。\n我们接着来看`CoroutineIntrinsics.normalizeContinuation`的实现：\n```\nfun <T> normalizeContinuation(continuation: Continuation<T>): Continuation<T> =\n        (continuation as? CoroutineImpl)?.facade ?: continuation\n```\n 还记得我们刚刚分析过`facade`这个属性吗？他是对`_facade`的代理，这个函数返回的是经过拦截器处理过的`Continuation`\n根据刚刚的字节码，我们可以发现`suspend`类型的函数，都会**隐式**额外接受一个当前协程的引用，但是又不能在函数中直接访问。\n\n最后，还有两个上文出现过的线程切换处理类，`postman`和`dispatcher`，使用的是单例模式：\n```\nobject postman : Handler(Looper.getMainLooper()) {\n    override fun handleMessage(msg: Message?) {\n        msg?.callback?.run()\n    }\n}\n\nobject dispatcher {\n    val mCachedThreads = Executors.newCachedThreadPool()\n    inline fun dispatch(noinline block: () -> Unit) {\n        mCachedThreads.execute(block)\n    }\n}\n```\n到此，我们实现了一个简易的异步调用库！","tags":["kotlin","coroutine"]},{"title":"自定义classLoader","url":"/2017/10/19/自定义classLoader/","content":"\n\n\n在`java`中，加载一个类到`jvm`虚拟机并为其实例化一个对象是由`ClassLoader`实现的\n有时候，我们需要实现特殊的类加载方式，就需要自己实现一个`ClassLoader `\n`ClassLoader`中有几个比较重要的方法：\n\n- `loadClass`：\n  该方法实现了双亲委托机制，一般不会重写该方法，他的执行步骤是先委托父加载器加载，如果加载不到，在执行自己的`findClass `加载\n- `findClass`：\n  一般实现自己的`ClassLoader`都是重写该方法，如果方法找不着，则抛出一个`ClassNotFound`异常\n- `defineClass`：\n  该方法是`jvm`提供的一个接口，验证一个`class`字节码数组，并为其创建一个`Class`对象\n>下面简单实现一个`ClassLoader`，用于加载指定目录下的`jar`包内的类\n```java\npublic class JarsClassLoader extends ClassLoader {\n    private String basePath;  //jar包存放的目录\n\n    public JarsClassLoader(String path, ClassLoader parentClasss) {\n        super(parentClasss); //指定父加载器\n        basePath = path;\n    }\n\n    @Override\n    protected Class<?> findClass(String name) throws ClassNotFoundException {\n        byte[] classByte = getClassByte(name); //根据类的全路径名去获取一个字节码数组\n        try {\n            return defineClass(name, classByte, 0, classByte.length);//生成一个class\n        } catch (Exception e) {\n            throw new ClassNotFoundException(\"找不到类：\"+name);\n        }\n    }\n    /**\n     * 该方法遍历basePath下的jar包，查找是否存在指定的类文件\n    **/\n    private byte[] getClassByte(String className) {\n        try {\n            File baseDir = new File(basePath); \n            File[] childrens = baseDir.listFiles();\n            for (File child : childrens) {\n                if (!child.getName().endsWith(\".jar\"))\n                    continue;\n                //jar包的全路径名\n                String jarName = basePath + File.separator + child.getName();\n                //创建一个Zip文件对象\n                ZipFile zip = new ZipFile(jarName);\n                //遍历Zip内的所有实体项\n                Enumeration<ZipEntry> zipEntries = (Enumeration<ZipEntry>) zip.entries();\n                ZipEntry zn;\n                while (zipEntries.hasMoreElements()) {\n                    zn = zipEntries.nextElement();\n                    if (!zn.getName().endsWith(\".class\"))\n                        continue;\n                    //处理实体名，将`/`替换成`.`，并去除`.class`后缀\n                    String znName =\n                            zn.getName().replace(\"/\", \".\").replace(\".class\", \"\");\n                    //如果找到指定的类文件\n                    if (znName.equals(className)) {\n                        BufferedInputStream bis = new BufferedInputStream(zip.getInputStream(zn));\n                        ByteArrayOutputStream bos = new ByteArrayOutputStream((int) zn.getSize());\n                        byte[] bytes = new byte[1024];\n                        int cnt;\n                        while ((cnt = (bis.read(bytes, 0, bytes.length))) > 0) {\n                             bos.write(bytes, 0, cnt);\n                        }\n                        return bos.toByteArray();\n                    }\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        } \n        return null;\n    }\n}\n```","tags":["java","classloader"]},{"title":"kotlin中的object更像是语法糖","url":"/2017/06/10/kotlin中的object更像是语法糖/","content":"\n\n\n### 单例声明\n\n\n\nkotlin中，声明一个单例的语法很简单：\n```\nobject obj\n```\n我们使用`object`关键字替代`class`关键字就可以声明一个单例对象\n`object`一样可以继承其他类，或者实现其他接口：\n```kotlin\ninterface IObj\nabstract class AbstractObj\nobject obj : AbstractObj(),IObj  \n```\n在这里，我们让`obj`这个单例继承了`AbstractObj`，并且实现了`IObj`接口\n声明一个单例对象，和声明一个`class`很类似\n但是，`object`声明的单例对象**不能声明构造函数**，因为单例对象只有一个实例，无需我们手动将它创建出来，因此自然不需要构造函数。\n> 如果需要对单例对象做初始化操作，可以在`init`初始化块内进行\n\n那么`object`是什么时候被创建的呢？\n>官方文档的解释是，`object`是`lazy-init`，即在第一次使用时被创造出来的\n\n`object`单例基本的使用就像上面这样了，基本的用法参照官方的文档说明就好了\n\n### 实现\n\n在java中，我们要使用一个单例模式时，一般使用双重检查锁：\n\n```java\npublic class Obj {\n      private Obj(){}\n      private static volatile Obj INSTANCE;\t\t//volatile防止指令重排\n      public static Obj getObj(){\n          if(INSTANCE==null){\n              synchronized(Obj.class){\n                  if (INSTANCE==null){\n                      INSTANCE=new Obj();\n                  }\n              }\n          }\n          return INSTANCE;\n      }\n}\n```\n而相同的功能，在kotlin只要`object Obj`就搞定了，这样的黑魔法是怎么实现的呢？\n为了探究一二，我们先来看看编译之后的字节码:\n\n```kotlin\n//源代码:\nobject Obj{\n    init{\n        println(\"object init...\")\n    }\n}\n```\n```java\n//对应的字节码：\npublic final class Obj {   //可以看到生成了一个class，而类名就是object name\n  // access flags 0x2\n  private <init>()V     //注意看，<init>的可见性是`private`\n   L0\n    LINENUMBER 8 L0\n    ALOAD 0  //将局部变量表slot 0处的引用入栈，即this引用\n    INVOKESPECIAL java/lang/Object.<init> ()V   //调用父类的<init>\n    ALOAD 0  //和上面一样，将局部变量表slot 0处的引用入栈，即this引用\n    CHECKCAST Obj    //类型检查\n    PUTSTATIC Obj.INSTANCE : LObj;     //保存this引用到`INSTANCE`这个静态域\n   L1\n    LINENUMBER 10 L1\n    LDC \"object init...\"    //从常量池将字符串引用送至栈顶\n    ASTORE 1    //将栈顶的引用保存到局部遍历表第一个slot处\n   L2\n    GETSTATIC java/lang/System.out : Ljava/io/PrintStream;    //获取out实例\n    ALOAD 1    //从局部变量表第一个slot处加载引用到栈顶\n    INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/Object;)V   //输出\n   L3\n   L4\n   L5\n    RETURN  //返回\n   L6\n    LOCALVARIABLE this LObj; L0 L6 0\n    MAXSTACK = 2    //操作数栈深度为2\n    MAXLOCALS = 2    //局部变量表为2个slot\n  // access flags 0x19\n  public final static LObj; INSTANCE    //**静态域**，类型为Obj\n  // access flags 0x8\n  static <clinit>()V    //静态初始化块，类初始化时执行\n   L0\n    LINENUMBER 8 L0\n    NEW Obj     //创建一个Obj实例，引用保存在栈顶\n    INVOKESPECIAL Obj.<init> ()V   //调用其<init>，初始化对象，此时会把栈顶引用作为this引用传入\n    RETURN\n    MAXSTACK = 1\n    MAXLOCALS = 0\n}\n```\n从上面的字节码中，我们可以看到，声明一个`object`，实际上就是创建了一个`class`，在类静态初始化时，会创建一个该类的实例，保存到其静态域`INSTANCE`中\n进而可以猜想，源码中对单例的引用会被编译器替换为对`INSTANCE`这个静态域的引用\n为了验证我们的分析，现在来看看使用单例时，对应的字节码\n```kotlin\n源码：\nfun main(args:Array<String>){\n    Obj is Any\n}\n```\n```\n对应的字节码：\npublic final static main([Ljava/lang/String;)V\n    @Lorg/jetbrains/annotations/NotNull;() // invisible, parameter 0\n   L0\n    ALOAD 0\n    LDC \"args\"\n    INVOKESTATIC kotlin/jvm/internal/Intrinsics.checkParameterIsNotNull (Ljava/lang/Object;Ljava/lang/String;)V\n   L1\n    LINENUMBER 5 L1\n    GETSTATIC Obj.INSTANCE : LObj;    //获取Obj的静态域`INSTANCE`\n    INSTANCEOF java/lang/Object        //判断是否是`Object`类型\n    POP\n   L2\n    LINENUMBER 6 L2\n    RETURN\n   L3\n    LOCALVARIABLE args [Ljava/lang/String; L0 L3 0\n    MAXSTACK = 2\n    MAXLOCALS = 1\n```\n\n可以看到，我们在源码中直接使用`object name`访问单例对象，而编译器帮我们做了翻译，实际使用的是内部的静态域`INSTANCE`\n而且，从对上面`Obj`这个生成的类的分析，我们可以发现，`object`在java中的对应的实现是类型这样的：\n\n```java\npublic class  Obj {\n    private Obj(){}\n    private static Obj INSTANCE=null;\n    static {\n        INSTANCE=new Obj();\n    }\n}\n```\n这是最简单的单例实现方法，在类加载器加载class后执行静态初始化时就创建单例对象。\n\n而`object`单例初始化的时机，准确来说，应该是这个类被加载时，静态初始化的时候。\n做个小实验：\n\n```kotlin\nobject Obj{\n    init{\n        println(\"object init...\")\n    }\n}\n```\n```kotlin\nfun main(args:Array<String>){\n         Class.forName(\"Obj\")\n    }\n```\n>控制台输出：object init...\n\n可见，当我们加载这个类的时候，单例就被创建了。\n而单例名就是类名。\n\n那么，`object`真的就是单例吗？\n一般情况下是的，因为字节码中`<init>`方法被声明为`private`，虽然不太准确但是我们可以认为对应了类的一个`private`的无参构造函数，这就使得我们无法创建出一个新的对象出来。\n但是，我们完全可以使用反射机制，从一个`private`的构造函数中创建一个对象出来：\n```kotlin\nfun main(args:Array<String>){\n    println(Obj)\n    var clazz=Class.forName(\"Obj\")\n    var constrouctor=clazz.getDeclaredConstructor()\n    constrouctor.setAccessible(true)\n    var instance=constrouctor.newInstance()\n    constrouctor.setAccessible(false)\n    println(instance)\n}\n输出：\nobject init...\nObj@511d50c0\nobject init...\nObj@60e53b93\n```\n可见，两次输出的对象引用是不一样的。\n\n那么，这就说明kotlin的单例是不安全的吗？这到未必\n我们在原先的基础上，加上几个属性声明：\n```\nobject Obj{\n    var name=\"name\"\n    var age=\"10\"\n    init{\n        println(\"object init...\")\n    }\n}\n```\n观察对应的字节码：\n```\npublic final class Obj {\n  private static Ljava/lang/String; name\n   ...\n  private static Ljava/lang/String; age\n  ....\n  public final static LObj; INSTANCE\n  ...  \n}\n```\n可以看到，这些属性的`field`都被声明为`static`了，尽管可以通过反射手段创建多个`object`的实例，但是它们的状态都是共享的\n总结：\n\n- `object`实际上还是生成一个`class`，但是这个`class`在`kotlin`中是透明的，无法直接访问，比如`Obj.INSTANCE`在kotlin中是不允许的，只能通过`Obj`来引用这个单例\n- `object name`本质上是类名，只是编译器在编译时自动将`object name`换成了 `object`的`INSTANCE`\n- `object`更像是语法糖","tags":["kotlin"]},{"title":"kotlin之代理属性","url":"/2017/05/16/kotlin之代理属性/","content":"\n### 属性代理\n\n##### 委托类\n\n委托类可以自己定义，必须提供`getValue`，如果用于代理var属性，还必须提供`setValue` \n\n```kotlin\nclass Delegate {\n  \t/**\n     * @param thisRef 被代理类实例\n     * @param property 被代理属性\n     */\n    operator fun getValue(thisRef: Any?, property: KProperty<*>): String {\n        return \"$thisRef, thank you for delegating '${property.name}' to me!\"\n    } \n    operator fun setValue(thisRef: Any?, property: KProperty<*>, value: String) {\n        println(\"$value has been assigned to '${property.name} in $thisRef.'\")\n    }\n}\n```\n\n##### 使用\n\n具体的使用方法：\n\n```\nvar 变量名:type by 委托对象\nval 变量名:type by 委托对象\n```\n\n使用代理属性禁止自定义setter和getter，代理的本质就是将setter和getter委托给其他对象\n\nkotlin提供了几个标准的代理工厂方法：\n\n- 懒加载： **the value gets computed only upon first access** \n\n```kotlin\n//通过使用工厂方法lazy()获得Lazy<T>实例\nfun main(args:Array<String>){\n\t//lazy没有提供setter，所以使用lazy代理的属性必须为val\n    val str:String by lazy{  //Lazy<T>懒加载，只会在第一次时执行\n        println(\"lazy\")\n        \"hello\"\n    }\n    println(str)\n    println(str)\n}\n```\n\n- observable properties: **listeners get notified about changes to this property** \n\n```kotlin\nfun main(args:Array<String>){\n    var p=Person()\n    println(p.name)\n    p.name=\"Jim\"\n    println(p.name)\n}\n\nclass Person{\n    var name by Delegates.observable(\"no-name\"){  //set时被调用\n        prop,old,new->\n        println(\"$prop($old->$new)\")\n    }\n}\n```\n\n- **storing properties in a map, not in separate field each **\n\n```kotlin\nfun main(args:Array<String>){\n    var p=Person(mutableMapOf(\"name\" to \"Tim\",\"age\" to 10))\n    println(p.name)\n    println(p.age)\n\n}\n\nclass Person(map:MutableMap<String,Any?>){\n    var name:String by map\n    var age:Int by map\n}\n```\n\n##### 工作原理\n\n```kotlin\nclass C {\n    var prop: Type by MyDelegate()\n} \n// this code is generated by the compiler\n// when the 'provideDelegate' function is available:\nclass C {\n    // calling \"provideDelegate\" to create the additional \"delegate\" property\n    private val prop$delegate = MyDelegate().provideDelegate(this, this::prop)\n    val prop: Type\n        get() = prop$delegate.getValue(this, this::prop)\n}\n```\n\nkotlin 1.1之后，代理属性可以用于local-properties\n\n\n\n### 方法代理\n\n```kotlin\nfun main(args:Array<String>){\n    var b=B(AImpl())\n    b.echo()\n}\n\ninterface A{\n    fun echo()\n}\n\nclass AImpl:A{\n    override fun echo(){\n        println(\"a implemention of A\")\n    }\n}\nclass B(impl:AImpl):A by impl  //impl提供B的接口方法\n```\n方法代理的实现原理就是编译时，自动为B生成echo方法的实现，并在该方法中调用impl的echo方法，因此B实例会持有impl的引用","tags":["kotlin"]},{"title":"使用kotlin写自己的dsl","url":"/2017/04/25/使用kotlin写自己的dsl/","content":"\n\n\n相比于java，kotlin对**FP**更加友好，支持扩展函数和操作符重载，这就为定义dsl提供了支持。\n什么是dsl呢？就是一种面向特定问题的语言。gradle就是是一种用groovy定义的dsl。而kotlin一样对定义dsl提供了友好的支持。\n本篇文章就来定义一个简单用于配置hibernate框架的dsl，写起来就像：\n\n```kotlin\nvar conf= buildConfiguration{\n                connection {\n                    username = \"***\"\n                    password = \"******\"\n                    url = \"jdbc:mysql://localhost:3306/******\"\n                    driver = Driver::class.java\n                }\n                c3p0 {\n                    max_size = 30\n                    min_size = 10\n                    timeout=5000\n                    max_statements=100\n                    idle_test_period=300\n                    acquire_increment=2\n                    validate=true\n                }\n                entity {\n                    mapping = Client::class.java\n                    mapping = Financial_account::class.java\n                    mapping = FundHolding::class.java\n                    mapping=Fund::class.java\n                }\n                dialect=\"org.hibernate.dialect.MySQL5InnoDBDialect\"\n            }\n```\n\n上面是一个对hibernate的简单配置，最后获取一个configuration实例。通过使用dsl，可以避免在运行时解析xml文件，同时又比使用java代码配置简洁，兼具xml的结构化和java的高效。\n那么，这样一个dsl是如何实现的呢？\n\n> 先介绍一下预备知识：\n> 扩展函数：\n```kotlin\nfun Type.foo():Unit{\n  ...\n}\n```\n这样就为`Type`对象创建了一个扩展函数，假如`t`是`Type`的一个实例，就可以：\n```t.foo()```\n`Type`称作`reciver`，而在`foo`函数体内，可以使用`this`访问其`public`成员，甚至可以省略`this`，仿佛`foo`函数是定义在`class Type`内\n而声明一个函数的参数是扩展函数，一般的语法：\n`fun funName(block:Type.(params...)->ReturnType):ReturnType{...}`\n关于扩展函数更多的用法，可以参考官方的文档\n\n首先先声明一个方法：\n```kotlin\nfun buildConfiguration(block:ConfigurationBuilder.()->Unit):Configuration{\n    var cfg=ConfigurationBuilder()\n    cfg.block()\n    return cfg.cfg\n} \n```\n这个方法接收一个有receiver的lambda表达式，因为这样在block的内部就可以直接访问receiver的公共成员了，这一点很重要\n紧接着对这个Configuration这个类进行定义\n```kotlin\nclass ConfigurationBuilder{\n     val TAG=\"hibernate\"\n     val cfg=Configuration()\n     var dialect:String? get() = null\n        set(value){\n            cfg.setProperty(\"$TAG.dialect\",value!!)\n        }\n     inline fun connection(block:ConnectionBuilder.()->Unit)=ConnectionBuilder(cfg).block()\n     inline fun c3p0(block:C3p0Builder.()->Unit)=C3p0Builder(cfg).block()\n     inline fun entity(block:Entity.()->Unit)=Entity(cfg).block()\n}\n```\n\n在里面我定义了三个成员函数，分别对应前面示例中的\n```kotlin\nvar conf= buildConfiguration{\n                connection {\n                    ...\n                }\n                c3p0 {\n                    ...\n                }\n                entity {\n                    ...\n                }\n               ...\n            }\n```\n\n在这个lambda里面，我就直接调用了buildConfiguration的成员函数，那么对象引用呢？还记得我前面说过的吗？buildConfiguration这个方法的参数是一个有receiver的lambda，而在buildConfiguration中声明了一个ConfigurationBuilder对象并通过这个对象调用了这个lambda。那么这个lambda就会在这个对象的上下文中，我们可以直接访问它的公共成员，甚至可以使用this引用这个对象。\n后续的步骤都差不多,我这里为了省事直接就声明了一个Configuration对象，并传到了其他对象里面\n后面的源码\n```kotlin\nclass ConnectionBuilder(val cfg:Configuration){//直接接受了一个configuration对象\n     val TAG=\"hibernate.connection\"\n     var username:String? get() = null  //重写了setter和getter，防止属性有field\n        set(name){\n            cfg.setProperty(\"$TAG.username\",name!!)  //直接硬编码设置属性\n        }\n     var password:String?  get() = null\n        set(password) {\n            cfg.setProperty(\"$TAG.password\", password!!)\n        }\n      var url:String? get() = null\n        set(url){\n            cfg.setProperty(\"$TAG.url\",url!!)\n        }\n      var driver:Class<*>? get() = null\n        set(driver){\n            cfg.setProperty(\"$TAG.driver_class\",driver!!.name)\n        }\n      var pool_size:Int? get() = null\n        set(size){\n            cfg.setProperty(\"$TAG.pool_size\",size!!.toString())\n        }\n}\n//后面的都差不多。。。\n class C3p0Builder(val cfg:Configuration){\n     val TAG=\"hibernate.c3p0\"\n     var max_size:Int? get() = null\n        set(max_size){\n            cfg.setProperty(\"$TAG.max_size\",max_size!!.toString())\n        }\n     var min_size:Int? get() = null\n        set(min_size){\n            cfg.setProperty(\"$TAG.min_size\",min_size!!.toString())\n        }\n     var timeout:Int? get() = null\n        set(timeout){\n            cfg.setProperty(\"$TAG.timeout\",timeout!!.toString())\n        }\n     var max_statements:Int? get() = null\n        set(max_stmt){\n            cfg.setProperty(\"$TAG.max_statements\",max_stmt!!.toString())\n        }\n     var idle_test_period:Int? get() = null\n        set(idle_test_period){\n            cfg.setProperty(\"$TAG.idle_test_period\",idle_test_period!!.toString())\n        }\n     var acquire_increment:Int? get() = null\n        set(acquire){\n            cfg.setProperty(\"$TAG.acquire_increment\",acquire!!.toString())\n        }\n     var validate:Boolean? get() = null\n        set(validate){\n            cfg.setProperty(\"$TAG.validate\",validate!!.toString())\n        }\n}\n class Entity(val cfg:Configuration){\n     var mapping:Class<*>?\n        get()=null\n        set(clazz){\n            cfg.addAnnotatedClass(clazz!!)\n        }\n}\n```\n\n至此，一个简单的dsl就完成了\n总体来说，定义一个dsl的过程基本是一个递归下去的过程，每个步骤都很类似","tags":["kotlin","dsl"]}]